{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5046834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yfinance\n",
    "!pip install transformers\n",
    "!pip install beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b27869",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import mpl, plt\n",
    "import math, time\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from operator import itemgetter\n",
    "from tqdm import tqdm\n",
    "from math import sqrt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038b76c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e89f966",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "wandb.init(project=\"stock_prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f74160",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8398dc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "no_of_days_to_lookforward = 1\n",
    "no_of_days_to_lookback = 5\n",
    "up_threshold = 0.015\n",
    "down_threshold = -0.015\n",
    "max_text_per_iter = 20\n",
    "batch_size = 16\n",
    "MAX_LEN = 10\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf67837",
   "metadata": {},
   "source": [
    "### Get stocks data for last N days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433931d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64983989",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stock_symbols = [ 'XOM']\n",
    "no_of_days = 4*365\n",
    "\n",
    "EXPORT_DATA_FOLDER = './data/'\n",
    "\n",
    "# Set the start and end dates for the data \n",
    "# here matching it with dates of news text available\n",
    "train_start = datetime.strptime('2020/01/04', '%Y/%m/%d')\n",
    "train_end = datetime.strptime('2022/09/30', '%Y/%m/%d')\n",
    "test_start = datetime.strptime('2022/10/01', '%Y/%m/%d')\n",
    "test_end = datetime.strptime('2023/01/04', '%Y/%m/%d')\n",
    "\n",
    "\n",
    "# start = datetime.datetime.now() - datetime.timedelta(days=no_of_days)\n",
    "# end = datetime.datetime.now()\n",
    "\n",
    "# Get training data\n",
    "for symbol in stock_symbols:\n",
    "    # Download the historical price and volume data using yfinance\n",
    "    train_data_raw = yf.download(symbol, start=train_start, end=train_end)\n",
    "\n",
    "    # Normalize features by percent of changes between today and yesterday\n",
    "    pct_change_open = train_data_raw['Open'].pct_change().fillna(0)\n",
    "    pct_change_high = train_data_raw['High'].pct_change().fillna(0)\n",
    "    pct_change_high_over_open = (train_data_raw['High']-train_data_raw['Open'])/train_data_raw['Open']\n",
    "    pct_change_low = train_data_raw['Low'].pct_change().fillna(0)\n",
    "    pct_change_low_over_open = (train_data_raw['Low']-train_data_raw['Open'])/train_data_raw['Open']\n",
    "    pct_change_close = train_data_raw['Close'].pct_change().fillna(0)\n",
    "    pct_change_close_over_open = (train_data_raw['Close']-train_data_raw['Open'])/train_data_raw['Open']\n",
    "    pct_change_adjclose = train_data_raw['Adj Close'].pct_change().fillna(0)\n",
    "    pct_change_adjclose_over_open = (train_data_raw['Adj Close']-train_data_raw['Open'])/train_data_raw['Open']\n",
    "    pct_change_volume = train_data_raw['Volume'].pct_change().fillna(0)\n",
    "\n",
    "    # Prepare labels: 2 means the close price of tomorow is higher than today's close price; 1 is down; 0 means the movement is between up_threshold and down_threshold\n",
    "    label = np.where(pct_change_close > up_threshold, 2, np.where(pct_change_close < down_threshold, 1, 0))[1:]\n",
    "    label = np.append(label, 0)\n",
    "\n",
    "    # Construct a train_data_norm data frame\n",
    "    train_data_norm = pd.DataFrame({'Open_norm':pct_change_open,\n",
    "                              'High_norm':pct_change_high,\n",
    "                              'Low_norm': pct_change_low,\n",
    "                              'Close_norm':pct_change_close,\n",
    "                              'Volume_norm':pct_change_volume,\n",
    "                              'High-Open_norm':pct_change_high_over_open,\n",
    "                              'Low-Open_norm':pct_change_low_over_open,\n",
    "                              'Close-Open_norm':pct_change_close_over_open,\n",
    "                              'Label_2up1down':label})\n",
    "\n",
    "    # Normalize by min-max normalization after the pct normalization\n",
    "    train_data_norm['Open_norm'] = train_data_norm['Open_norm'].apply(lambda x: (x - train_data_norm['Open_norm'].min()) / (train_data_norm['Open_norm'].max() - train_data_norm['Open_norm'].min()))\n",
    "    train_data_norm['High_norm'] = train_data_norm['High_norm'].apply(lambda x: (x - train_data_norm['High_norm'].min()) / (train_data_norm['High_norm'].max() - train_data_norm['High_norm'].min()))\n",
    "    train_data_norm['Low_norm'] = train_data_norm['Low_norm'].apply(lambda x: (x - train_data_norm['Low_norm'].min()) / (train_data_norm['Low_norm'].max() - train_data_norm['Low_norm'].min()))\n",
    "    train_data_norm['Close_norm'] = train_data_norm['Close_norm'].apply(lambda x: (x - train_data_norm['Close_norm'].min()) / (train_data_norm['Close_norm'].max() - train_data_norm['Close_norm'].min()))\n",
    "    train_data_norm['Volume_norm'] = train_data_norm['Volume_norm'].apply(lambda x: (x - train_data_norm['Volume_norm'].min()) / (train_data_norm['Volume_norm'].max() - train_data_norm['Volume_norm'].min()))\n",
    "    train_data_norm['High-Open_norm'] = train_data_norm['High-Open_norm'].apply(lambda x: (x - train_data_norm['High-Open_norm'].min()) / (train_data_norm['High-Open_norm'].max() - train_data_norm['High-Open_norm'].min()))\n",
    "    train_data_norm['Low-Open_norm'] = train_data_norm['Low-Open_norm'].apply(lambda x: (x - train_data_norm['Low-Open_norm'].min()) / (train_data_norm['Low-Open_norm'].max() - train_data_norm['Low-Open_norm'].min()))\n",
    "    train_data_norm['Close-Open_norm'] = train_data_norm['Close-Open_norm'].apply(lambda x: (x - train_data_norm['Close-Open_norm'].min()) / (train_data_norm['Close-Open_norm'].max() - train_data_norm['Close-Open_norm'].min()))\n",
    "\n",
    "    # Remove the first and the last row, becuase of NAN values\n",
    "    train_data_raw = train_data_raw.iloc[1:-1]\n",
    "    train_data_norm = train_data_norm.iloc[1:-1]\n",
    "\n",
    "    train_data_raw.to_csv(EXPORT_DATA_FOLDER+symbol+'train_raw_data.csv', index=True)\n",
    "    train_data_norm.to_csv(EXPORT_DATA_FOLDER+symbol+'train_norm_data.csv', index=True)\n",
    "    \n",
    "    \n",
    "# Get test data\n",
    "for symbol in stock_symbols:\n",
    "    # Download the historical price and volume data using yfinance\n",
    "    test_data_raw = yf.download(symbol, start=test_start, end=test_end)\n",
    "\n",
    "    # Normalize features by percent of changes between today and yesterday\n",
    "    pct_change_open = test_data_raw['Open'].pct_change().fillna(0)\n",
    "    pct_change_high = test_data_raw['High'].pct_change().fillna(0)\n",
    "    pct_change_high_over_open = (test_data_raw['High']-test_data_raw['Open'])/test_data_raw['Open']\n",
    "    pct_change_low = test_data_raw['Low'].pct_change().fillna(0)\n",
    "    pct_change_low_over_open = (test_data_raw['Low']-test_data_raw['Open'])/test_data_raw['Open']\n",
    "    pct_change_close = test_data_raw['Close'].pct_change().fillna(0)\n",
    "    pct_change_close_over_open = (test_data_raw['Close']-test_data_raw['Open'])/test_data_raw['Open']\n",
    "    pct_change_adjclose = test_data_raw['Adj Close'].pct_change().fillna(0)\n",
    "    pct_change_adjclose_over_open = (test_data_raw['Adj Close']-test_data_raw['Open'])/test_data_raw['Open']\n",
    "    pct_change_volume = test_data_raw['Volume'].pct_change().fillna(0)\n",
    "\n",
    "    # Prepare labels: 2 means the close price of tomorow is higher than today's close price; 1 is down; 0 means the movement is between up_threshold and down_threshold\n",
    "    label = np.where(pct_change_close > up_threshold, 2, np.where(pct_change_close < down_threshold, 1, 0))[1:]\n",
    "    label = np.append(label, 0)\n",
    "\n",
    "    # Construct a test_data_norm data frame\n",
    "    test_data_norm = pd.DataFrame({'Open_norm':pct_change_open,\n",
    "                              'High_norm':pct_change_high,\n",
    "                              'Low_norm': pct_change_low,\n",
    "                              'Close_norm':pct_change_close,\n",
    "                              'Volume_norm':pct_change_volume,\n",
    "                              'High-Open_norm':pct_change_high_over_open,\n",
    "                              'Low-Open_norm':pct_change_low_over_open,\n",
    "                              'Close-Open_norm':pct_change_close_over_open,\n",
    "                              'Label_2up1down':label})\n",
    "\n",
    "    # Normalize by min-max normalization after the pct normalization\n",
    "    test_data_norm['Open_norm'] = test_data_norm['Open_norm'].apply(lambda x: (x - test_data_norm['Open_norm'].min()) / (test_data_norm['Open_norm'].max() - test_data_norm['Open_norm'].min()))\n",
    "    test_data_norm['High_norm'] = test_data_norm['High_norm'].apply(lambda x: (x - test_data_norm['High_norm'].min()) / (test_data_norm['High_norm'].max() - test_data_norm['High_norm'].min()))\n",
    "    test_data_norm['Low_norm'] = test_data_norm['Low_norm'].apply(lambda x: (x - test_data_norm['Low_norm'].min()) / (test_data_norm['Low_norm'].max() - test_data_norm['Low_norm'].min()))\n",
    "    test_data_norm['Close_norm'] = test_data_norm['Close_norm'].apply(lambda x: (x - test_data_norm['Close_norm'].min()) / (test_data_norm['Close_norm'].max() - test_data_norm['Close_norm'].min()))\n",
    "    test_data_norm['Volume_norm'] = test_data_norm['Volume_norm'].apply(lambda x: (x - test_data_norm['Volume_norm'].min()) / (test_data_norm['Volume_norm'].max() - test_data_norm['Volume_norm'].min()))\n",
    "    test_data_norm['High-Open_norm'] = test_data_norm['High-Open_norm'].apply(lambda x: (x - test_data_norm['High-Open_norm'].min()) / (test_data_norm['High-Open_norm'].max() - test_data_norm['High-Open_norm'].min()))\n",
    "    test_data_norm['Low-Open_norm'] = test_data_norm['Low-Open_norm'].apply(lambda x: (x - test_data_norm['Low-Open_norm'].min()) / (test_data_norm['Low-Open_norm'].max() - test_data_norm['Low-Open_norm'].min()))\n",
    "    test_data_norm['Close-Open_norm'] = test_data_norm['Close-Open_norm'].apply(lambda x: (x - test_data_norm['Close-Open_norm'].min()) / (test_data_norm['Close-Open_norm'].max() - test_data_norm['Close-Open_norm'].min()))\n",
    "\n",
    "    # Remove the first and the last row, becuase of NAN values\n",
    "    test_data_raw = test_data_raw.iloc[1:-1]\n",
    "    test_data_norm = test_data_norm.iloc[1:-1]\n",
    "\n",
    "    test_data_raw.to_csv(EXPORT_DATA_FOLDER+symbol+'test_raw_data.csv', index=True)\n",
    "    test_data_norm.to_csv(EXPORT_DATA_FOLDER+symbol+'test_norm_data.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c946df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4974ced",
   "metadata": {},
   "source": [
    "## TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07da5ca5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "(2023-06-05)\n",
    "cuda support check\n",
    "//read textual data into correct shape\n",
    "hyperparam tuning: number of neurons: tune to right number of neurons in FC in model\n",
    "//max_text_per_iter -> code in dataloader to maintain the size \n",
    "\n",
    "(2023-06-07)\n",
    "cuda check\n",
    "roberta encoder fix\n",
    "multi label - how to create target label?\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632dfdf7",
   "metadata": {},
   "source": [
    "## Prep textual data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda1eda3",
   "metadata": {},
   "source": [
    "### Crawl textual news data from internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a5d4a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import Request\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Parameters \n",
    "n = 3 #the # of article headlines displayed per ticker\n",
    "tickers = ['AAPL', 'TSLA', 'AMZN']\n",
    "\n",
    "\n",
    "\n",
    "# Get Data\n",
    "finviz_url = 'https://finviz.com/quote.ashx?t='\n",
    "news_tables = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    url = finviz_url + ticker\n",
    "    req = Request(url=url,\n",
    "                  headers={'user-agent': 'Mozilla/5.0',\n",
    "                                   'referer': 'https://...'}) \n",
    "    resp = urlopen(req)    \n",
    "    html = BeautifulSoup(resp, features=\"lxml\")\n",
    "    news_table = html.find(id='news-table')\n",
    "    news_tables[ticker] = news_table\n",
    "\n",
    "try:\n",
    "    for ticker in tickers:\n",
    "        df = news_tables[ticker]\n",
    "        df_tr = df.findAll('tr')\n",
    "    \n",
    "        print ('\\n')\n",
    "        print ('Recent News Headlines for {}: '.format(ticker))\n",
    "        \n",
    "        for i, table_row in enumerate(df_tr):\n",
    "            a_text = table_row.a.text\n",
    "            td_text = table_row.td.text\n",
    "            td_text = td_text.strip()\n",
    "            print(a_text,'(',td_text,')')\n",
    "            if i == n-1:\n",
    "                break\n",
    "except KeyError:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Iterate through the news\n",
    "parsed_news = []\n",
    "for file_name, news_table in news_tables.items():\n",
    "    for x in news_table.findAll('tr'):\n",
    "        text = x.a.get_text() \n",
    "        date_scrape = x.td.text.split()\n",
    "\n",
    "        if len(date_scrape) == 1:\n",
    "            time = date_scrape[0]\n",
    "            \n",
    "        else:\n",
    "            date = date_scrape[0]\n",
    "            time = date_scrape[1]\n",
    "\n",
    "        ticker = file_name.split('_')[0]\n",
    "        \n",
    "        parsed_news.append([ticker, date, time, text])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d832b747",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parsed_news[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeff4e8a",
   "metadata": {},
   "source": [
    "### Read downloaded data from saved files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4f812c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text_data_df = pd.read_csv('./data/XOM_20200401_20230401_medium.csv', sep= ',', header= 0)\n",
    "text_data_df = text_data_df[['Date', 'News']]\n",
    "\n",
    "\n",
    "text_data_df = text_data_df.groupby('Date')['News'].apply('$$$###'.join)\n",
    "\n",
    "text_data_df.index = pd.to_datetime(text_data_df.index, dayfirst=True)\n",
    "# text_data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eddacc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_train_df = train_data_norm.join(text_data_df, how = 'inner')\n",
    "all_test_df = test_data_norm.join(text_data_df, how = 'inner')\n",
    "\n",
    "print(all_train_df.index.min())\n",
    "print(all_train_df.index.max())\n",
    "print(all_test_df.index.min())\n",
    "print(all_test_df.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad722f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca779813",
   "metadata": {},
   "source": [
    "### Merge textual and numerical data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e91800",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_train = all_train_df.values\n",
    "\n",
    "window_size = no_of_days_to_lookback\n",
    "\n",
    "X_numerical_train = []\n",
    "y_train = []\n",
    "X_text_train = []\n",
    "X_text_train_curr = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(window_size, len(all_train) - no_of_days_to_lookforward + 1):\n",
    "    X_numerical_train.append(all_train[i-window_size: i, :-2])\n",
    "    \n",
    "    # split and append sequence of text\n",
    "    curr_seq = all_train[i-window_size: i, -1]\n",
    "    for j in range(window_size):\n",
    "        split_curr_seq = curr_seq[window_size - 1 -j].split('$$$###')\n",
    "        X_text_train_curr = X_text_train_curr + split_curr_seq\n",
    "    \n",
    "    if len(X_text_train_curr) > max_text_per_iter:\n",
    "        X_text_train_curr = X_text_train_curr[:max_text_per_iter]\n",
    "    \n",
    "    X_text_train.append(X_text_train_curr)\n",
    "        \n",
    "    # target labels\n",
    "    y_train.append(all_train[i:i+no_of_days_to_lookforward, -2])\n",
    "\n",
    "X_numerical_train, y_train = np.array(X_numerical_train).astype(np.float16), np.array(y_train).astype(np.int32)\n",
    "print(type(X_numerical_train))\n",
    "print(type(y_train))\n",
    "\n",
    "X_numerical_train = torch.from_numpy(X_numerical_train).type(torch.Tensor)\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "\n",
    "print(len(X_numerical_train))\n",
    "print(len(X_text_train))\n",
    "print(len(y_train))\n",
    "print(X_numerical_train.shape)\n",
    "\n",
    "print(len(X_text_train))\n",
    "print(len(X_text_train[2]))\n",
    "# print(X_text_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b543f0c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_test = all_test_df.values\n",
    "\n",
    "\n",
    "X_numerical_test = []\n",
    "y_test = []\n",
    "X_text_test = []\n",
    "X_text_test_curr = []\n",
    "\n",
    "for i in range(window_size, len(all_test) - no_of_days_to_lookforward + 1):\n",
    "    X_numerical_test.append(all_test[i-window_size: i, :-2])\n",
    "    \n",
    "    # split and append sequence of text (in reverse order to add the latest news first)\n",
    "    curr_seq = all_test[i-window_size: i, -1]\n",
    "    for j in range(window_size):\n",
    "        split_curr_seq = curr_seq[window_size - 1 -j].split('$$$###')\n",
    "        X_text_test_curr = X_text_test_curr + split_curr_seq\n",
    "    \n",
    "    if len(X_text_test_curr) > max_text_per_iter:\n",
    "        X_text_test_curr = X_text_test_curr[:max_text_per_iter]\n",
    "    \n",
    "    X_text_test.append(X_text_test_curr)\n",
    "        \n",
    "    # target labels\n",
    "    y_test.append(all_test[i:i+no_of_days_to_lookforward, -2])\n",
    "\n",
    "X_numerical_test, y_test = np.array(X_numerical_test).astype(np.float16), np.array(y_test).astype(np.int32)\n",
    "print(type(X_numerical_test))\n",
    "print(type(y_test))\n",
    "\n",
    "X_numerical_test = torch.from_numpy(X_numerical_test).type(torch.Tensor)\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "print(len(X_numerical_test))\n",
    "print(len(X_text_test))\n",
    "print(len(y_test))\n",
    "print(X_numerical_test.shape)\n",
    "\n",
    "print(len(X_text_test))\n",
    "print(len(X_text_test[2]))\n",
    "# print(X_text_test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8968cd52",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "041596ba",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Check sample train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425dea0a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_numerical_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a432985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_text_train[0])\n",
    "X_text_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8567ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f65275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77440f62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1e81060",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec05f5a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large', truncation=True, do_lower_case=True)\n",
    "\n",
    "class SiameseDataloader(Dataset):\n",
    "    \n",
    "    def __init__(self, X_numerical_train, y_train, X_text_train, MAX_LEN, tokenizer):\n",
    "        self.X_numerical_train = X_numerical_train\n",
    "        self.X_text_train = X_text_train\n",
    "        self.MAX_LEN = MAX_LEN\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        \n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        token_type_ids = []\n",
    "        input_seq = []\n",
    "\n",
    "        for sent in X_text_train[index]:\n",
    "            encoded_sent = self.tokenizer.encode_plus(\n",
    "                text=sent,\n",
    "                add_special_tokens=True,        # Add `[CLS]` and `[SEP]` special tokens\n",
    "                max_length=self.MAX_LEN,             # Choose max length to truncate/pad\n",
    "                pad_to_max_length=True,         # Pad sentence to max length \n",
    "                #return_attention_mask=True      # Return attention mask\n",
    "                return_token_type_ids=True\n",
    "                )\n",
    "            input_ids.append(encoded_sent.get('input_ids'))\n",
    "            attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "            token_type_ids.append(encoded_sent.get('token_type_ids'))\n",
    "\n",
    "        # Convert lists to tensors\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        attention_masks = torch.tensor(attention_masks)\n",
    "        token_type_ids = torch.tensor(token_type_ids)\n",
    "\n",
    "\n",
    "        return {\n",
    "            'x_numerical': X_numerical_train[index],\n",
    "            'ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(attention_masks, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(y_train[index], dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_numerical_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6b94ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_set = SiameseDataloader(X_numerical_train, y_train, X_text_train, MAX_LEN, tokenizer)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "test_set = SiameseDataloader(X_numerical_test, y_test, X_text_test, MAX_LEN, tokenizer)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b3c3a5",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Check the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47651e58",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx, data in tqdm(enumerate(train_loader, 0)):\n",
    "    if idx > 1:\n",
    "        break\n",
    "        \n",
    "    \n",
    "    x_numerical = data['x_numerical'].to(device, dtype = torch.float)\n",
    "    ids = data['ids'].to(device, dtype = torch.long)\n",
    "    masks = data['mask'].to(device, dtype = torch.long)\n",
    "    token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "    targets = data['targets'].to(device, dtype = torch.long)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7f4d1a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(x_numerical.shape)\n",
    "x_numerical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb6885e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(ids.shape)\n",
    "ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a83eb4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(targets.shape)\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dddd83",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb91ea5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d64e94",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79f13b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9eed8a87",
   "metadata": {},
   "source": [
    "## Build model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fb01fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class SiameseModel(nn.Module):\n",
    "    def __init__(self, input_dim1, input_dim2, \n",
    "                 hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4,\n",
    "                 num_layers1, num_layers2, output_dim1, output_dim2):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.input_dim1 = input_dim1\n",
    "        self.input_dim2 = input_dim2\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.hidden_dim3 = hidden_dim3\n",
    "        self.hidden_dim4 = hidden_dim4\n",
    "        self.num_layers1 = num_layers1\n",
    "        self.num_layers2 = num_layers2\n",
    "        self.output_dim1 = output_dim1\n",
    "        self.output_dim2 = output_dim2\n",
    "        \n",
    "        \n",
    "\n",
    "#         self.roberta = RobertaModel.from_pretrained(\"roberta-large\").to(device)\n",
    "        \n",
    "        \n",
    "#         self.lstm1 = nn.LSTM(input_dim1, hidden_dim1, num_layers1, batch_first=True)\n",
    "#         self.lstm2 = nn.LSTM(input_dim2, hidden_dim2, num_layers2, batch_first=True)\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim1, output_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim2, output_dim2)\n",
    "        self.fc3 = nn.Linear(input_dim1, hidden_dim3)\n",
    "#         self.fc3 = nn.Linear(output_dim1+output_dim2, hidden_dim3)\n",
    "        self.fc4 = nn.Linear(hidden_dim3, hidden_dim4)\n",
    "        self.fc5 = nn.Linear(hidden_dim4, 3)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x1, ids, masks, token_type_ids):\n",
    "        #left tower with numerical features\n",
    "#         h_10 = Variable(torch.zeros(self.num_layers1, x1.size(0), self.hidden_dim1)).to(device)\n",
    "#         c_10 = Variable(torch.zeros(self.num_layers1, x1.size(0), self.hidden_dim1)).to(device)\n",
    "#         ula1, (h_out1, _) = self.lstm1(x1, (h_10, c_10))\n",
    "#         h_out1 = h_out1.view(-1, self.hidden_dim1)\n",
    "#         out1 = self.fc1(h_out1)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # right tower with roberta on textual features  \n",
    "        #TODO\n",
    "#         batch_size_here = ids.shape[0]\n",
    "#         e2 = torch.zeros(batch_size_here, max_text_per_iter,1024).to(device)\n",
    "        \n",
    "#         for k in range(ids.shape[1]):\n",
    "#             seq_ids = ids[:,k,:]\n",
    "#             seq_masks = masks[:,k,:]\n",
    "#             seq_token_type_ids = token_type_ids[:,k,:]\n",
    "\n",
    "\n",
    "#             e2k = self.roberta(input_ids= seq_ids, attention_mask=seq_masks, token_type_ids=seq_token_type_ids)\n",
    "#             # print(e2.shape)\n",
    "#             # print(e2k[1].shape)\n",
    "#             #first 0 is for last_hidden_state: https://huggingface.co/docs/transformers/main/en/model_doc/roberta#transformers.RobertaModel.forward.returns:~:text=transformers.modeling_outputs.-,BaseModelOutputWithPoolingAndCrossAttentions%20or%20tuple(torch.FloatTensor),-A%20transformers.modeling_outputs\n",
    "#             # the shape of e2k[0] is (batch_size, sequence_length (<=MAX_LEN), hidden_size (=1024))\n",
    "#             e2k1 = e2k[0][:, 0, :]  \n",
    "#             e2[:,k,:] = e2k1\n",
    "    \n",
    "    \n",
    "#         print('e2 shape: ', e2.shape)        \n",
    "#         h_20 = Variable(torch.zeros(self.num_layers2, e2.size(0), self.hidden_dim2)).to(device)\n",
    "#         c_20 = Variable(torch.zeros(self.num_layers2, e2.size(0), self.hidden_dim2)).to(device)\n",
    "#         ula2, (h_out2, _) = self.lstm2(e2, (h_20, c_20))\n",
    "#         h_out2 = h_out2.view(-1, self.hidden_dim2)\n",
    "#         out2 = self.fc2(h_out2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # siamese merging layers\n",
    "        x1 = torch.squeeze(x1[:, 0, :], 1) #x1[:, 0, :].squeeze()\n",
    "        print('shape of x1 after squeeze:', x1.shape)\n",
    "#         output = torch.cat((out1, out2),1)\n",
    "#         output = F.relu(self.fc3(output))\n",
    "        output = F.relu(self.fc3(x1))\n",
    "        output = F.relu(self.fc4(output))\n",
    "        output = self.fc5(output)\n",
    "        return output\n",
    "    \n",
    "#TODO : correct these values\n",
    "model = SiameseModel(input_dim1 = 8, input_dim2 = 1024, \n",
    "                 hidden_dim1 = 20, hidden_dim2 = 768, hidden_dim3 = 128, hidden_dim4 = 64,\n",
    "                 num_layers1 = 1, num_layers2 = 1, output_dim1 = 10, output_dim2 = 256).to(device)\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96c4800",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model)\n",
    "print(len(list(model.parameters())))\n",
    "# for i in range(len(list(model.parameters()))):\n",
    "#     print(list(model.parameters())[i].size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64d1362",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3306d04b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.008)\n",
    "loss_arr = np.zeros(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba9919",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"stock_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6a968c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# roberta = RobertaModel.from_pretrained(\"roberta-large\").to(device)\n",
    "train_loss_record = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    train_loss_sum = []\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for idx, data in tqdm(enumerate(train_loader, 0)):\n",
    "        x_numerical = data['x_numerical'].to(device, dtype = torch.float)\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        masks = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        # debugging roberta encoder and second lstm\n",
    "        '''\n",
    "        debug starts here\n",
    "        '''\n",
    "        # if idx > 1:\n",
    "        #     break\n",
    "        # batch_size_here = data['ids'].shape[0]\n",
    "        # print('batch_size_here:', batch_size_here)\n",
    "        # e2 = torch.zeros(batch_size_here, max_text_per_iter, 1024)\n",
    "        # print('ids shape:', ids.shape)\n",
    "        \n",
    "        # for k in range(ids.shape[1]):  #number of sentences in sequence = max_text_per_iter\n",
    "        #     print('k:', k)\n",
    "        #     seq_ids = ids[:,k,:].to(device)\n",
    "        #     seq_masks = masks[:,k,:].to(device)\n",
    "        #     seq_token_type_ids = token_type_ids[:,k,:].to(device)\n",
    "\n",
    "\n",
    "        #     e2k = roberta(input_ids= seq_ids, attention_mask=seq_masks, token_type_ids=seq_token_type_ids)\n",
    "        #     print(e2.shape)\n",
    "        #     print(e2k[1].shape)\n",
    "        #     #first 0 is for last_hidden_state: https://huggingface.co/docs/transformers/main/en/model_doc/roberta#transformers.RobertaModel.forward.returns:~:text=transformers.modeling_outputs.-,BaseModelOutputWithPoolingAndCrossAttentions%20or%20tuple(torch.FloatTensor),-A%20transformers.modeling_outputs\n",
    "        #     # the shape of e2k[0] is (batch_size, sequence_length (<=MAX_LEN), hidden_size (=1024))\n",
    "        #     e2k1 = e2k[0][:, 0, :]  \n",
    "        #     e2[:,k,:] = e2k1\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "        # lstm2 = nn.LSTM(1024, 768, 1, batch_first=True)\n",
    "        # fc2 = nn.Linear(768, 256)\n",
    "\n",
    "        # h_20 = Variable(torch.zeros(1, e2.size(0), 768))\n",
    "        # c_20 = Variable(torch.zeros(1, e2.size(0), 768))\n",
    "        # ula2, (h_out2, _) = lstm2(e2, (h_20, c_20))\n",
    "        # h_out2 = h_out2.view(-1, 768)\n",
    "        # out2 = fc2(h_out2)\n",
    "        \n",
    "\n",
    "    #     print(ids.shape)\n",
    "    #     print(masks.shape)\n",
    "    #     print(token_type_ids.shape)\n",
    "    \n",
    "        # print(out2)\n",
    "\n",
    "\n",
    "        \n",
    "        '''\n",
    "        debug ends here\n",
    "        '''\n",
    "        print('shape of x1 = x_numerical : ', x_numerical.shape)    \n",
    "        y_pred = model(x_numerical, ids, masks, token_type_ids)\n",
    "        _, pred_label = torch.max(y_pred.data, 1)\n",
    "\n",
    "        print('y_pred:', y_pred)\n",
    "        print('target:', targets)\n",
    "        loss = criterion(y_pred, targets.reshape(-1))\n",
    "        \n",
    "         # Zero out gradient, else they will accumulate between epochs\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimiser.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        train_loss.append(loss.data.cpu())\n",
    "        train_loss_sum.append(loss.data.cpu())\n",
    "         \n",
    "\n",
    "        wandb.log({'avg train loss in this batch': loss.item(), 'epoch': epoch, 'batch_id': idx})\n",
    "        \n",
    "        # Get accuracy\n",
    "        train_total += targets.reshape(-1).size(0)\n",
    "        train_correct += (pred_label == targets.reshape(-1)).sum()\n",
    "        print('in this epoch, total, correct:', train_total, train_correct)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "    # Record at every epoch\n",
    "    print('Train Loss at epoch {}: {}\\n'.format(epoch, np.mean(train_loss_sum)))\n",
    "    train_loss_record.append(np.mean(train_loss_sum))\n",
    "    wandb.log({'avg train loss in this epoch': np.mean(train_loss_sum), 'epoch': epoch})\n",
    "    wandb.log({'train accuracy in this epoch': train_accuracy, 'epoch': epoch})\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # evaluate on test set every epoch\n",
    "    print('starting testing..')\n",
    "    tloss = []\n",
    "    test_loss_sum = []\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for idx, data in tqdm(enumerate(test_loader, 0)):\n",
    "        test_x_numerical = data['x_numerical'].to(device, dtype = torch.float)\n",
    "        test_ids = data['ids'].to(device, dtype = torch.long)\n",
    "        test_masks = data['mask'].to(device, dtype = torch.long)\n",
    "        test_token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        test_targets = data['targets'].to(device, dtype = torch.long)\n",
    "        \n",
    "        print('shape of x1 = test_x_numerical : ', test_x_numerical.shape)    \n",
    "        y_pred = model(test_x_numerical, test_ids, test_masks, test_token_type_ids)\n",
    "        _, pred_label = torch.max(y_pred.data, 1)\n",
    "\n",
    "#         print('y_pred:', y_pred)\n",
    "        test_loss = criterion(y_pred, test_targets.reshape(-1))\n",
    "    \n",
    "        tloss.append(test_loss.data.cpu())\n",
    "        test_loss_sum.append(test_loss.data.cpu()) \n",
    "\n",
    "        wandb.log({'avg test loss in this batch': test_loss.item(), 'epoch': epoch, 'batch_id': idx})\n",
    "        \n",
    "        # Get accuracy\n",
    "        total += test_targets.reshape(-1).size(0)\n",
    "        correct += (pred_label == test_targets.reshape(-1)).sum()\n",
    "        print('in this epoch, total, correct:', total, correct)\n",
    "    test_accuracy = 100 * correct / total\n",
    "\n",
    "    # Record at every epoch\n",
    "    print('test Loss at epoch {}: {}\\n'.format(epoch, np.mean(test_loss_sum)))\n",
    "    wandb.log({'avg test loss in this epoch': np.mean(test_loss_sum), 'epoch': epoch})\n",
    "    wandb.log({'test accuracy in this epoch': test_accuracy, 'epoch': epoch})\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52034a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc98314",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(test_targets.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a4db0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_, pred_label = torch.max(y_pred.data, 1)\n",
    "pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becf70d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(pred_label == test_targets.reshape(-1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0040dec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_id = str(1)\n",
    "!mkdir output/$run_id\n",
    "output_model_file = 'roberta_stock_pred.bin'\n",
    "output_vocab_file = './output' + str(run_id)\n",
    "\n",
    "model_to_save = model\n",
    "torch.save(model_to_save, output_model_file)\n",
    "tokenizer.save_vocabulary(output_vocab_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f95298",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3105ea1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a49d19",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e1748b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_arr, label=\"Training loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26f68b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
