{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12204acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yfinance\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5bc9685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/visriv/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import mpl, plt\n",
    "import math, time\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from operator import itemgetter\n",
    "from tqdm import tqdm\n",
    "from math import sqrt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from transformers import RobertaTokenizer, RobertaModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de0b2fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a786825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "wandb.init(project=\"stock_prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83438f8",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c775342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_days_to_lookforward = 1\n",
    "no_of_days_to_lookback = 5\n",
    "up_threshold = 0.015\n",
    "down_threshold = -0.015\n",
    "max_text_per_iter = 100\n",
    "batch_size = 1\n",
    "MAX_LEN = 1000\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93965310",
   "metadata": {},
   "source": [
    "### Get stocks data for last N days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c19af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64983989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "stock_symbols = [ 'XOM']\n",
    "no_of_days = 4*365\n",
    "\n",
    "EXPORT_DATA_FOLDER = './data/'\n",
    "\n",
    "# Set the start and end dates for the data \n",
    "# here matching it with dates of news text available\n",
    "start = datetime.strptime('2019/01/04', '%Y/%m/%d')\n",
    "end = datetime.strptime('2023/01/04', '%Y/%m/%d')\n",
    "\n",
    "\n",
    "# start = datetime.datetime.now() - datetime.timedelta(days=no_of_days)\n",
    "# end = datetime.datetime.now()\n",
    "\n",
    "for symbol in stock_symbols:\n",
    "    # Download the historical price and volume data using yfinance\n",
    "    data_raw = yf.download(symbol, start=start, end=end)\n",
    "\n",
    "    # Normalize features by percent of changes between today and yesterday\n",
    "    pct_change_open = data_raw['Open'].pct_change().fillna(0)\n",
    "    pct_change_high = data_raw['High'].pct_change().fillna(0)\n",
    "    pct_change_high_over_open = (data_raw['High']-data_raw['Open'])/data_raw['Open']\n",
    "    pct_change_low = data_raw['Low'].pct_change().fillna(0)\n",
    "    pct_change_low_over_open = (data_raw['Low']-data_raw['Open'])/data_raw['Open']\n",
    "    pct_change_close = data_raw['Close'].pct_change().fillna(0)\n",
    "    pct_change_close_over_open = (data_raw['Close']-data_raw['Open'])/data_raw['Open']\n",
    "    pct_change_adjclose = data_raw['Adj Close'].pct_change().fillna(0)\n",
    "    pct_change_adjclose_over_open = (data_raw['Adj Close']-data_raw['Open'])/data_raw['Open']\n",
    "    pct_change_volume = data_raw['Volume'].pct_change().fillna(0)\n",
    "\n",
    "    # Prepare labels: 2 means the close price of tomorow is higher than today's close price; 1 is down; 0 means the movement is between up_threshold and down_threshold\n",
    "    label = np.where(pct_change_close > up_threshold, 2, np.where(pct_change_close < down_threshold, 1, 0))[1:]\n",
    "    label = np.append(label, 0)\n",
    "\n",
    "    # Construct a data_norm data frame\n",
    "    data_norm = pd.DataFrame({'Open_norm':pct_change_open,\n",
    "                              'High_norm':pct_change_high,\n",
    "                              'Low_norm': pct_change_low,\n",
    "                              'Close_norm':pct_change_close,\n",
    "                              'Volume_norm':pct_change_volume,\n",
    "                              'High-Open_norm':pct_change_high_over_open,\n",
    "                              'Low-Open_norm':pct_change_low_over_open,\n",
    "                              'Close-Open_norm':pct_change_close_over_open,\n",
    "                              'Label_2up1down':label})\n",
    "\n",
    "    # Normalize by min-max normalization after the pct normalization\n",
    "    data_norm['Open_norm'] = data_norm['Open_norm'].apply(lambda x: (x - data_norm['Open_norm'].min()) / (data_norm['Open_norm'].max() - data_norm['Open_norm'].min()))\n",
    "    data_norm['High_norm'] = data_norm['High_norm'].apply(lambda x: (x - data_norm['High_norm'].min()) / (data_norm['High_norm'].max() - data_norm['High_norm'].min()))\n",
    "    data_norm['Low_norm'] = data_norm['Low_norm'].apply(lambda x: (x - data_norm['Low_norm'].min()) / (data_norm['Low_norm'].max() - data_norm['Low_norm'].min()))\n",
    "    data_norm['Close_norm'] = data_norm['Close_norm'].apply(lambda x: (x - data_norm['Close_norm'].min()) / (data_norm['Close_norm'].max() - data_norm['Close_norm'].min()))\n",
    "    data_norm['Volume_norm'] = data_norm['Volume_norm'].apply(lambda x: (x - data_norm['Volume_norm'].min()) / (data_norm['Volume_norm'].max() - data_norm['Volume_norm'].min()))\n",
    "    data_norm['High-Open_norm'] = data_norm['High-Open_norm'].apply(lambda x: (x - data_norm['High-Open_norm'].min()) / (data_norm['High-Open_norm'].max() - data_norm['High-Open_norm'].min()))\n",
    "    data_norm['Low-Open_norm'] = data_norm['Low-Open_norm'].apply(lambda x: (x - data_norm['Low-Open_norm'].min()) / (data_norm['Low-Open_norm'].max() - data_norm['Low-Open_norm'].min()))\n",
    "    data_norm['Close-Open_norm'] = data_norm['Close-Open_norm'].apply(lambda x: (x - data_norm['Close-Open_norm'].min()) / (data_norm['Close-Open_norm'].max() - data_norm['Close-Open_norm'].min()))\n",
    "\n",
    "    # Remove the first and the last row, becuase of NAN values\n",
    "    data_raw = data_raw.iloc[1:-1]\n",
    "    data_norm = data_norm.iloc[1:-1]\n",
    "\n",
    "    data_raw.to_csv(EXPORT_DATA_FOLDER+symbol+'_raw_data.csv', index=True)\n",
    "    data_norm.to_csv(EXPORT_DATA_FOLDER+symbol+'_norm_data.csv', index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b75198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "682b0091",
   "metadata": {},
   "source": [
    "## TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a31b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "(2023-06-05)\n",
    "cuda support check\n",
    "//read textual data into correct shape\n",
    "hyperparam tuning: number of neurons: tune to right number of neurons in FC in model\n",
    "//max_text_per_iter -> code in dataloader to maintain the size \n",
    "\n",
    "(2023-06-07)\n",
    "cuda check\n",
    "roberta encoder fix\n",
    "multi label - how to create target label?\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935eca96",
   "metadata": {},
   "source": [
    "## Prep textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "823a5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data_df = pd.read_csv('./data/XOM_20200401_20230401_medium.csv', sep= ',', header= 0)\n",
    "text_data_df = text_data_df[['Date', 'News']]\n",
    "\n",
    "\n",
    "text_data_df = text_data_df.groupby('Date')['News'].apply('$$$###'.join)\n",
    "\n",
    "text_data_df.index = pd.to_datetime(text_data_df.index, dayfirst=True)\n",
    "# text_data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b6857f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_norm</th>\n",
       "      <th>High_norm</th>\n",
       "      <th>Low_norm</th>\n",
       "      <th>Close_norm</th>\n",
       "      <th>Volume_norm</th>\n",
       "      <th>High-Open_norm</th>\n",
       "      <th>Low-Open_norm</th>\n",
       "      <th>Close-Open_norm</th>\n",
       "      <th>Label_2up1down</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-04-01</th>\n",
       "      <td>0.424588</td>\n",
       "      <td>0.325630</td>\n",
       "      <td>0.445426</td>\n",
       "      <td>0.444210</td>\n",
       "      <td>0.187485</td>\n",
       "      <td>0.493448</td>\n",
       "      <td>0.887338</td>\n",
       "      <td>0.568367</td>\n",
       "      <td>2</td>\n",
       "      <td>Global Polymers Market, By Type (Thermoplastic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-02</th>\n",
       "      <td>0.778879</td>\n",
       "      <td>0.795313</td>\n",
       "      <td>0.736065</td>\n",
       "      <td>0.797702</td>\n",
       "      <td>0.445139</td>\n",
       "      <td>0.821628</td>\n",
       "      <td>0.826839</td>\n",
       "      <td>0.730773</td>\n",
       "      <td>1</td>\n",
       "      <td>European Morning Briefing: U.S. Jobs Report Ey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-03</th>\n",
       "      <td>0.853804</td>\n",
       "      <td>0.435964</td>\n",
       "      <td>0.626517</td>\n",
       "      <td>0.372487</td>\n",
       "      <td>0.160943</td>\n",
       "      <td>0.174091</td>\n",
       "      <td>0.435487</td>\n",
       "      <td>0.093004</td>\n",
       "      <td>2</td>\n",
       "      <td>Nordic Morning Briefing: Services PMI Data in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-06</th>\n",
       "      <td>0.427455</td>\n",
       "      <td>0.266519</td>\n",
       "      <td>0.635065</td>\n",
       "      <td>0.619722</td>\n",
       "      <td>0.182685</td>\n",
       "      <td>0.235477</td>\n",
       "      <td>0.869836</td>\n",
       "      <td>0.546103</td>\n",
       "      <td>2</td>\n",
       "      <td>圖表 Texas Takes Two Punches -- Oil Shock and O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-07</th>\n",
       "      <td>0.876632</td>\n",
       "      <td>0.689534</td>\n",
       "      <td>0.761913</td>\n",
       "      <td>0.567103</td>\n",
       "      <td>0.283049</td>\n",
       "      <td>0.096776</td>\n",
       "      <td>0.676091</td>\n",
       "      <td>0.194020</td>\n",
       "      <td>2</td>\n",
       "      <td>Exxon Cuts Capital Spending by 30% in Response...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-23</th>\n",
       "      <td>0.542767</td>\n",
       "      <td>0.435239</td>\n",
       "      <td>0.669969</td>\n",
       "      <td>0.596883</td>\n",
       "      <td>0.188873</td>\n",
       "      <td>0.165444</td>\n",
       "      <td>0.996267</td>\n",
       "      <td>0.555196</td>\n",
       "      <td>0</td>\n",
       "      <td>Energy Transfer's Gulf Run gas pipeline gets U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-27</th>\n",
       "      <td>0.662985</td>\n",
       "      <td>0.484486</td>\n",
       "      <td>0.633508</td>\n",
       "      <td>0.546500</td>\n",
       "      <td>0.238297</td>\n",
       "      <td>0.108558</td>\n",
       "      <td>0.967843</td>\n",
       "      <td>0.504141</td>\n",
       "      <td>1</td>\n",
       "      <td>Butadiene Market, By Application (Polybutadien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-28</th>\n",
       "      <td>0.600374</td>\n",
       "      <td>0.392759</td>\n",
       "      <td>0.523834</td>\n",
       "      <td>0.424789</td>\n",
       "      <td>0.195329</td>\n",
       "      <td>0.008094</td>\n",
       "      <td>0.853954</td>\n",
       "      <td>0.357091</td>\n",
       "      <td>0</td>\n",
       "      <td>Exxon sues EU in move to block new windfall ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-29</th>\n",
       "      <td>0.508146</td>\n",
       "      <td>0.400534</td>\n",
       "      <td>0.562507</td>\n",
       "      <td>0.521098</td>\n",
       "      <td>0.222456</td>\n",
       "      <td>0.158359</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.522474</td>\n",
       "      <td>0</td>\n",
       "      <td>Global Car Care Products Market 2023-2027 Publ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-30</th>\n",
       "      <td>0.599916</td>\n",
       "      <td>0.444452</td>\n",
       "      <td>0.582740</td>\n",
       "      <td>0.531163</td>\n",
       "      <td>0.263573</td>\n",
       "      <td>0.172017</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.551144</td>\n",
       "      <td>1</td>\n",
       "      <td>Global Polypropylene Nonwoven Fabric Market 20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>694 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Open_norm  High_norm  Low_norm  Close_norm  Volume_norm  \\\n",
       "Date                                                                  \n",
       "2020-04-01   0.424588   0.325630  0.445426    0.444210     0.187485   \n",
       "2020-04-02   0.778879   0.795313  0.736065    0.797702     0.445139   \n",
       "2020-04-03   0.853804   0.435964  0.626517    0.372487     0.160943   \n",
       "2020-04-06   0.427455   0.266519  0.635065    0.619722     0.182685   \n",
       "2020-04-07   0.876632   0.689534  0.761913    0.567103     0.283049   \n",
       "...               ...        ...       ...         ...          ...   \n",
       "2022-12-23   0.542767   0.435239  0.669969    0.596883     0.188873   \n",
       "2022-12-27   0.662985   0.484486  0.633508    0.546500     0.238297   \n",
       "2022-12-28   0.600374   0.392759  0.523834    0.424789     0.195329   \n",
       "2022-12-29   0.508146   0.400534  0.562507    0.521098     0.222456   \n",
       "2022-12-30   0.599916   0.444452  0.582740    0.531163     0.263573   \n",
       "\n",
       "            High-Open_norm  Low-Open_norm  Close-Open_norm  Label_2up1down  \\\n",
       "Date                                                                         \n",
       "2020-04-01        0.493448       0.887338         0.568367               2   \n",
       "2020-04-02        0.821628       0.826839         0.730773               1   \n",
       "2020-04-03        0.174091       0.435487         0.093004               2   \n",
       "2020-04-06        0.235477       0.869836         0.546103               2   \n",
       "2020-04-07        0.096776       0.676091         0.194020               2   \n",
       "...                    ...            ...              ...             ...   \n",
       "2022-12-23        0.165444       0.996267         0.555196               0   \n",
       "2022-12-27        0.108558       0.967843         0.504141               1   \n",
       "2022-12-28        0.008094       0.853954         0.357091               0   \n",
       "2022-12-29        0.158359       1.000000         0.522474               0   \n",
       "2022-12-30        0.172017       1.000000         0.551144               1   \n",
       "\n",
       "                                                         News  \n",
       "Date                                                           \n",
       "2020-04-01  Global Polymers Market, By Type (Thermoplastic...  \n",
       "2020-04-02  European Morning Briefing: U.S. Jobs Report Ey...  \n",
       "2020-04-03  Nordic Morning Briefing: Services PMI Data in ...  \n",
       "2020-04-06   圖表 Texas Takes Two Punches -- Oil Shock and O...  \n",
       "2020-04-07  Exxon Cuts Capital Spending by 30% in Response...  \n",
       "...                                                       ...  \n",
       "2022-12-23  Energy Transfer's Gulf Run gas pipeline gets U...  \n",
       "2022-12-27  Butadiene Market, By Application (Polybutadien...  \n",
       "2022-12-28  Exxon sues EU in move to block new windfall ta...  \n",
       "2022-12-29  Global Car Care Products Market 2023-2027 Publ...  \n",
       "2022-12-30  Global Polypropylene Nonwoven Fabric Market 20...  \n",
       "\n",
       "[694 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_df = data_norm.join(text_data_df, how = 'inner')\n",
    "all_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56e91800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "all_train = all_train_df.values\n",
    "\n",
    "window_size = no_of_days_to_lookback\n",
    "\n",
    "X_numerical_train = []\n",
    "y_train = []\n",
    "X_text_train = []\n",
    "X_text_train_curr = []\n",
    "\n",
    "\n",
    "for i in range(window_size, len(all_train) - no_of_days_to_lookforward + 1):\n",
    "    X_numerical_train.append(all_train[i-window_size: i, :-2])\n",
    "    \n",
    "    # split and append sequence of text\n",
    "    curr_seq = all_train[i-window_size: i, -1]\n",
    "    for j in range(window_size):\n",
    "        split_curr_seq = curr_seq[window_size - 1 -j].split('$$$###')\n",
    "        X_text_train_curr = X_text_train_curr + split_curr_seq\n",
    "    \n",
    "    if len(X_text_train_curr) > max_text_per_iter:\n",
    "        X_text_train_curr = X_text_train_curr[:100]\n",
    "    \n",
    "    X_text_train.append(X_text_train_curr)\n",
    "        \n",
    "    # target labels\n",
    "    y_train.append(all_train[i:i+no_of_days_to_lookforward, -2])\n",
    "\n",
    "X_numerical_train, y_train = np.array(X_numerical_train).astype(np.float16), np.array(y_train).astype(np.int32)\n",
    "print(type(X_numerical_train))\n",
    "print(type(y_train))\n",
    "\n",
    "X_numerical_train = torch.from_numpy(X_numerical_train).type(torch.Tensor)\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3f1c11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "689\n",
      "689\n",
      "689\n",
      "torch.Size([689, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "print(len(X_numerical_train))\n",
    "print(len(X_text_train))\n",
    "print(len(y_train))\n",
    "print(X_numerical_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c2be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_text_train))\n",
    "print(len(X_text_train[2]))\n",
    "print(X_text_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04818e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc17828c",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d46960ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large', truncation=True, do_lower_case=True)\n",
    "\n",
    "class SiameseDataloader(Dataset):\n",
    "    \n",
    "    def __init__(self, X_numerical_train, y_train, X_text_train, MAX_LEN, tokenizer):\n",
    "        self.X_numerical_train = X_numerical_train\n",
    "        self.X_text_train = X_text_train\n",
    "        self.MAX_LEN = MAX_LEN\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        \n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        token_type_ids = []\n",
    "        input_seq = []\n",
    "\n",
    "        for sent in X_text_train[index]:\n",
    "            encoded_sent = self.tokenizer.encode_plus(\n",
    "                text=sent,\n",
    "                add_special_tokens=True,        # Add `[CLS]` and `[SEP]` special tokens\n",
    "                max_length=self.MAX_LEN,             # Choose max length to truncate/pad\n",
    "                pad_to_max_length=True,         # Pad sentence to max length \n",
    "                #return_attention_mask=True      # Return attention mask\n",
    "                return_token_type_ids=True\n",
    "                )\n",
    "            input_ids.append(encoded_sent.get('input_ids'))\n",
    "            attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "            token_type_ids.append(encoded_sent.get('token_type_ids'))\n",
    "\n",
    "        # Convert lists to tensors\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        attention_masks = torch.tensor(attention_masks)\n",
    "        token_type_ids = torch.tensor(token_type_ids)\n",
    "\n",
    "\n",
    "        return {\n",
    "            'x_numerical': X_numerical_train[index],\n",
    "            'ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(attention_masks, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(y_train[index], dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_numerical_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dec86a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = SiameseDataloader(X_numerical_train, y_train, X_text_train, MAX_LEN, tokenizer)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b676dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be0eaa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f3a3fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a000d370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbff448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f789fcd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018b1026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c48d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0119f008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bddb2686",
   "metadata": {},
   "source": [
    "## Build model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b43cda15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class SiameseModel(nn.Module):\n",
    "    def __init__(self, input_dim1, input_dim2, \n",
    "                 hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4,\n",
    "                 num_layers1, num_layers2, output_dim1, output_dim2):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.input_dim1 = input_dim1\n",
    "        self.input_dim2 = input_dim2\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.hidden_dim3 = hidden_dim3\n",
    "        self.hidden_dim4 = hidden_dim4\n",
    "        self.num_layers1 = num_layers1\n",
    "        self.num_layers2 = num_layers2\n",
    "        self.output_dim1 = output_dim1\n",
    "        self.output_dim2 = output_dim2\n",
    "        \n",
    "        \n",
    "\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta-large\")\n",
    "        \n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_dim1, hidden_dim1, num_layers1, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_dim2, hidden_dim2, num_layers2, batch_first=True)\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim1, output_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim2, output_dim2)\n",
    "        self.fc3 = nn.Linear(output_dim1+output_dim2, hidden_dim3)\n",
    "        self.fc4 = nn.Linear(hidden_dim3, hidden_dim4)\n",
    "        self.fc5 = nn.Linear(hidden_dim4, 3)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x1, ids, masks, token_type_ids):\n",
    "        #left tower with numerical features\n",
    "        \n",
    "        h_10 = Variable(torch.zeros(self.num_layers1, x1.size(0), self.hidden_dim1))\n",
    "        c_10 = Variable(torch.zeros(self.num_layers1, x1.size(0), self.hidden_dim1))\n",
    "        ula1, (h_out1, _) = self.lstm1(x1, (h_10, c_10))\n",
    "        h_out1 = h_out1.view(-1, self.hidden_dim1)\n",
    "        out1 = self.fc1(h_out1)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # right tower with roberta on textual features  \n",
    "        #TODO\n",
    "        e2 = torch.zeros(1,100,1024)\n",
    "        \n",
    "        for k in range(ids.shape[1]):\n",
    "            seq_ids = ids[:,k,:]\n",
    "            seq_masks = masks[:,k,:]\n",
    "            seq_token_type_ids = token_type_ids[:,k,:]\n",
    "\n",
    "\n",
    "            e2k = roberta(input_ids= seq_ids, attention_mask=seq_masks, token_type_ids=seq_token_type_ids)\n",
    "\n",
    "            e2k1 = e2k[0][:, 0, :]  #first 0 is for 'CLS' token\n",
    "            e2[:,k,:] = e2k1\n",
    "    \n",
    "    \n",
    "        print(e2.shape)        \n",
    "        h_20 = Variable(torch.zeros(self.num_layers2, e2.size(0), self.hidden_dim2))\n",
    "        c_20 = Variable(torch.zeros(self.num_layers2, e2.size(0), self.hidden_dim2))\n",
    "        ula2, (h_out2, _) = self.lstm2(e2, (h_20, c_20))\n",
    "        h_out2 = h_out2.view(-1, self.hidden_dim2)\n",
    "        out2 = self.fc2(h_out2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # siamese merging layers\n",
    "        \n",
    "        output = torch.cat((out1, out2),1)\n",
    "        output = F.relu(self.fc3(output))\n",
    "        output = F.relu(self.fc4(output))\n",
    "        output = self.fc5(output)\n",
    "        return output\n",
    "    \n",
    "#TODO : correct these values\n",
    "model = SiameseModel(input_dim1 = 8, input_dim2 = 1024, \n",
    "                 hidden_dim1 = 20, hidden_dim2 = 768, hidden_dim3 = 128, hidden_dim4 = 64,\n",
    "                 num_layers1 = 1, num_layers2 = 1, output_dim1 = 10, output_dim2 = 256)\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d95556",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "print(len(list(model.parameters())))\n",
    "# for i in range(len(list(model.parameters()))):\n",
    "#     print(list(model.parameters())[i].size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372471f7",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38c21d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_arr = np.zeros(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42c6ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/visriv/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/var/folders/tw/t5br5nkd2k3f065pr50zxvn80000gn/T/ipykernel_30832/3665585654.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/var/folders/tw/t5br5nkd2k3f065pr50zxvn80000gn/T/ipykernel_30832/3665585654.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'mask': torch.tensor(attention_masks, dtype=torch.long),\n",
      "/var/folders/tw/t5br5nkd2k3f065pr50zxvn80000gn/T/ipykernel_30832/3665585654.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
      "/var/folders/tw/t5br5nkd2k3f065pr50zxvn80000gn/T/ipykernel_30832/3665585654.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'targets': torch.tensor(y_train[index], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 0\n",
      "k: 1\n",
      "k: 2\n",
      "k: 3\n",
      "k: 4\n",
      "k: 5\n",
      "k: 6\n",
      "k: 7\n",
      "k: 8\n",
      "k: 9\n",
      "k: 10\n",
      "k: 11\n",
      "k: 12\n",
      "k: 13\n",
      "k: 14\n",
      "k: 15\n",
      "k: 16\n",
      "k: 17\n",
      "k: 18\n",
      "k: 19\n",
      "k: 20\n",
      "k: 21\n",
      "k: 22\n",
      "k: 23\n",
      "k: 24\n",
      "k: 25\n",
      "k: 26\n",
      "k: 27\n",
      "k: 28\n",
      "k: 29\n",
      "k: 30\n",
      "k: 31\n",
      "k: 32\n",
      "k: 33\n",
      "k: 34\n"
     ]
    }
   ],
   "source": [
    "roberta = RobertaModel.from_pretrained(\"roberta-large\").to(device)\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for idx, data in tqdm(enumerate(train_loader, 0)):\n",
    "        x_numerical = data['x_numerical'].to(device, dtype = torch.float)\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        masks = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        # debugging roberta encoder and second lstm\n",
    "        '''\n",
    "        debug starts here\n",
    "        '''\n",
    "        if idx > 1:\n",
    "            break\n",
    "        e2 = torch.zeros(1,100,1024)\n",
    "        \n",
    "        for k in range(ids.shape[1]):  #number of sentences in sequence\n",
    "            print('k:', k)\n",
    "            seq_ids = ids[:,k,:].to(device)\n",
    "            seq_masks = masks[:,k,:].to(device)\n",
    "            seq_token_type_ids = token_type_ids[:,k,:].to(device)\n",
    "\n",
    "\n",
    "            e2k = roberta(input_ids= seq_ids, attention_mask=seq_masks, token_type_ids=seq_token_type_ids)\n",
    "\n",
    "            e2k1 = e2k[0][:, 0, :]  #first 0 is for 'CLS' token\n",
    "            e2[:,k,:] = e2k1\n",
    "    \n",
    "    \n",
    "        print(e2.shape) \n",
    "        lstm2 = nn.LSTM(1024, 768, 1, batch_first=True)\n",
    "        fc2 = nn.Linear(768, 256)\n",
    "\n",
    "        h_20 = Variable(torch.zeros(1, e2.size(0), 768))\n",
    "        c_20 = Variable(torch.zeros(1, e2.size(0), 768))\n",
    "        ula2, (h_out2, _) = lstm2(e2, (h_20, c_20))\n",
    "        h_out2 = h_out2.view(-1, 768)\n",
    "        out2 = fc2(h_out2)\n",
    "        \n",
    "        print(out)\n",
    "\n",
    "    #     print(ids.shape)\n",
    "    #     print(masks.shape)\n",
    "    #     print(token_type_ids.shape)\n",
    "\n",
    "        \n",
    "        '''\n",
    "        debug ends here\n",
    "        '''\n",
    "        \n",
    "\n",
    "    \n",
    "        y_pred = model(x_numerical, ids, masks, token_type_ids)\n",
    "        \n",
    "\n",
    "    \n",
    "        loss = criterion(y_pred, y_train)\n",
    "        if epoch % 10 == 0 and epoch !=0:\n",
    "            print(\"Epoch \", t, \"CELoss: \", loss.item())\n",
    "        loss_arr[epoch] = loss.item()\n",
    "        wandb.log({'celoss': loss.item().avg, 'epoch': epoch, 'batch_id': batch_id})\n",
    "\n",
    "        # Zero out gradient, else they will accumulate between epochs\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimiser.step()\n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94410220",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b36391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c83612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f0a9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_arr, label=\"Training loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de40bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
