{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e199799",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yfinance\n",
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64983989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tw/t5br5nkd2k3f065pr50zxvn80000gn/T/ipykernel_22948/1880521505.py:6: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn')\n",
      "/var/folders/tw/t5br5nkd2k3f065pr50zxvn80000gn/T/ipykernel_22948/1880521505.py:10: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  from pandas import datetime\n",
      "/Users/visriv/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import mpl, plt\n",
    "plt.style.use('seaborn')\n",
    "# mpl.rcParams['font.family'] = 'serif'\n",
    "%matplotlib inline\n",
    "\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import itertools\n",
    "import datetime\n",
    "from operator import itemgetter\n",
    "\n",
    "from math import sqrt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Set hyperparameters\n",
    "stock_symbols = [ 'XOM']\n",
    "no_of_days = 4*365\n",
    "up_threshold = 0.015\n",
    "down_threshold = -0.015\n",
    "EXPORT_DATA_FOLDER = './data/'\n",
    "\n",
    "# Set the start and end dates for the data\n",
    "start = datetime.strptime('04-01-2019', '%m/%d/%y ')\n",
    "end = datetime.strptime('04-01-2023', '%m/%d/%y ')\n",
    "\n",
    "\n",
    "# start = datetime.datetime.now() - datetime.timedelta(days=no_of_days)\n",
    "# end = datetime.datetime.now()\n",
    "\n",
    "for symbol in stock_symbols:\n",
    "    # Download the historical price and volume data using yfinance\n",
    "    data_raw = yf.download(symbol, start=start, end=end)\n",
    "\n",
    "    # Normalize features by percent of changes between today and yesterday\n",
    "    pct_change_open = data_raw['Open'].pct_change().fillna(0)\n",
    "    pct_change_high = data_raw['High'].pct_change().fillna(0)\n",
    "    pct_change_high_over_open = (data_raw['High']-data_raw['Open'])/data_raw['Open']\n",
    "    pct_change_low = data_raw['Low'].pct_change().fillna(0)\n",
    "    pct_change_low_over_open = (data_raw['Low']-data_raw['Open'])/data_raw['Open']\n",
    "    pct_change_close = data_raw['Close'].pct_change().fillna(0)\n",
    "    pct_change_close_over_open = (data_raw['Close']-data_raw['Open'])/data_raw['Open']\n",
    "    pct_change_adjclose = data_raw['Adj Close'].pct_change().fillna(0)\n",
    "    pct_change_adjclose_over_open = (data_raw['Adj Close']-data_raw['Open'])/data_raw['Open']\n",
    "    pct_change_volume = data_raw['Volume'].pct_change().fillna(0)\n",
    "\n",
    "    # Prepare labels: 2 means the close price of tomorow is higher than today's close price; 1 is down; 0 means the movement is between up_threshold and down_threshold\n",
    "    label = np.where(pct_change_close > up_threshold, 2, np.where(pct_change_close < down_threshold, 1, 0))[1:]\n",
    "    label = np.append(label, 0)\n",
    "\n",
    "    # Construct a data_norm data frame\n",
    "    data_norm = pd.DataFrame({'Open_norm':pct_change_open,\n",
    "                              'High_norm':pct_change_high,\n",
    "                              'Low_norm': pct_change_low,\n",
    "                              'Close_norm':pct_change_close,\n",
    "                              'Volume_norm':pct_change_volume,\n",
    "                              'High-Open_norm':pct_change_high_over_open,\n",
    "                              'Low-Open_norm':pct_change_low_over_open,\n",
    "                              'Close-Open_norm':pct_change_close_over_open,\n",
    "                              'Label_2up1down':label})\n",
    "\n",
    "    # Normalize by min-max normalization after the pct normalization\n",
    "    data_norm['Open_norm'] = data_norm['Open_norm'].apply(lambda x: (x - data_norm['Open_norm'].min()) / (data_norm['Open_norm'].max() - data_norm['Open_norm'].min()))\n",
    "    data_norm['High_norm'] = data_norm['High_norm'].apply(lambda x: (x - data_norm['High_norm'].min()) / (data_norm['High_norm'].max() - data_norm['High_norm'].min()))\n",
    "    data_norm['Low_norm'] = data_norm['Low_norm'].apply(lambda x: (x - data_norm['Low_norm'].min()) / (data_norm['Low_norm'].max() - data_norm['Low_norm'].min()))\n",
    "    data_norm['Close_norm'] = data_norm['Close_norm'].apply(lambda x: (x - data_norm['Close_norm'].min()) / (data_norm['Close_norm'].max() - data_norm['Close_norm'].min()))\n",
    "    data_norm['Volume_norm'] = data_norm['Volume_norm'].apply(lambda x: (x - data_norm['Volume_norm'].min()) / (data_norm['Volume_norm'].max() - data_norm['Volume_norm'].min()))\n",
    "    data_norm['High-Open_norm'] = data_norm['High-Open_norm'].apply(lambda x: (x - data_norm['High-Open_norm'].min()) / (data_norm['High-Open_norm'].max() - data_norm['High-Open_norm'].min()))\n",
    "    data_norm['Low-Open_norm'] = data_norm['Low-Open_norm'].apply(lambda x: (x - data_norm['Low-Open_norm'].min()) / (data_norm['Low-Open_norm'].max() - data_norm['Low-Open_norm'].min()))\n",
    "    data_norm['Close-Open_norm'] = data_norm['Close-Open_norm'].apply(lambda x: (x - data_norm['Close-Open_norm'].min()) / (data_norm['Close-Open_norm'].max() - data_norm['Close-Open_norm'].min()))\n",
    "\n",
    "    # Remove the first and the last row, becuase of NAN values\n",
    "    data_raw = data_raw.iloc[1:-1]\n",
    "    data_norm = data_norm.iloc[1:-1]\n",
    "\n",
    "    data_raw.to_csv(EXPORT_DATA_FOLDER+symbol+'_raw_data.csv', index=True)\n",
    "    data_norm.to_csv(EXPORT_DATA_FOLDER+symbol+'_norm_data.csv', index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5e5afbd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2021-06-08', '2021-06-09', '2021-06-10', '2021-06-11',\n",
       "               '2021-06-14', '2021-06-15', '2021-06-16', '2021-06-17',\n",
       "               '2021-06-18', '2021-06-21',\n",
       "               ...\n",
       "               '2023-05-18', '2023-05-19', '2023-05-22', '2023-05-23',\n",
       "               '2023-05-24', '2023-05-25', '2023-05-26', '2023-05-30',\n",
       "               '2023-05-31', '2023-06-01'],\n",
       "              dtype='datetime64[ns]', name='Date', length=500, freq=None)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d24a5c0",
   "metadata": {},
   "source": [
    "## TODO (2023-06-05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e256f1f6",
   "metadata": {},
   "source": [
    "\n",
    "cuda support check\n",
    "read textual data into correct shape\n",
    "hyperparam tuning: number of neurons: tune to right number of neurons in FC in model\n",
    "max_text_per_iter -> code in dataloader to maintain the size \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab6d647",
   "metadata": {},
   "source": [
    "## Prep textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "eb401d16",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2021-01-01    Tomato processor's accrued production costs fa...\n",
       "2022-01-01    Industrial Alcohol Market Forecasts to 2028 – ...\n",
       "2023-01-01    Global Cumene Market Research Report 2022-2032...\n",
       "2020-10-01    Press Release: SBM Offshore awarded contracts ...\n",
       "2021-10-01    JPMorgan 's Own Employee Travel Numbers Now He...\n",
       "                                    ...                        \n",
       "2021-08-09    Climate Change Is a ‘Hammer Hitting Us on the ...\n",
       "2022-08-09    KASE - Trading in common shares US30231G1022 (...\n",
       "2020-09-09    TAP Clouds Italy’s LNG Import Plans The immine...\n",
       "2021-09-09    Storm's Fallout Cripples U.S. Oil Output --- S...\n",
       "2022-09-09    Thermoplastic Elastomer Market Forecasts to 20...\n",
       "Name: News, Length: 1065, dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data_df = pd.read_csv('./data/XOM_20200401_20230401_medium.csv', sep= ',', header= 0)\n",
    "text_data_df = text_data_df[['Date', 'News']]\n",
    "\n",
    "\n",
    "text_data_df = text_data_df.groupby('Date')['News'].apply('$$$###'.join)\n",
    "\n",
    "text_data_df.index = pd.to_datetime(text_data_df.index, dayfirst=True)\n",
    "text_data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "981660a4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_norm</th>\n",
       "      <th>High_norm</th>\n",
       "      <th>Low_norm</th>\n",
       "      <th>Close_norm</th>\n",
       "      <th>Volume_norm</th>\n",
       "      <th>High-Open_norm</th>\n",
       "      <th>Low-Open_norm</th>\n",
       "      <th>Close-Open_norm</th>\n",
       "      <th>Label_2up1down</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-06-08</th>\n",
       "      <td>0.624690</td>\n",
       "      <td>0.576438</td>\n",
       "      <td>0.482223</td>\n",
       "      <td>0.675290</td>\n",
       "      <td>0.490718</td>\n",
       "      <td>0.240756</td>\n",
       "      <td>0.785477</td>\n",
       "      <td>0.583539</td>\n",
       "      <td>0</td>\n",
       "      <td>Blowing Agent Market by Type (HC, HFC, HCFC), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-09</th>\n",
       "      <td>0.757691</td>\n",
       "      <td>0.598956</td>\n",
       "      <td>0.716982</td>\n",
       "      <td>0.610093</td>\n",
       "      <td>0.229087</td>\n",
       "      <td>0.180812</td>\n",
       "      <td>0.878255</td>\n",
       "      <td>0.490038</td>\n",
       "      <td>0</td>\n",
       "      <td>Global Asphalt Market 2021-2025 Published By: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-10</th>\n",
       "      <td>0.724752</td>\n",
       "      <td>0.550617</td>\n",
       "      <td>0.533313</td>\n",
       "      <td>0.562715</td>\n",
       "      <td>0.210127</td>\n",
       "      <td>0.087882</td>\n",
       "      <td>0.660152</td>\n",
       "      <td>0.373188</td>\n",
       "      <td>0</td>\n",
       "      <td>Global Benzene Market 2021-2025 Published By: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-11</th>\n",
       "      <td>0.524817</td>\n",
       "      <td>0.385896</td>\n",
       "      <td>0.512483</td>\n",
       "      <td>0.486898</td>\n",
       "      <td>0.118679</td>\n",
       "      <td>0.043161</td>\n",
       "      <td>0.780528</td>\n",
       "      <td>0.374858</td>\n",
       "      <td>0</td>\n",
       "      <td>Keystone Illustrates Pipelines' Hurdles The fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-14</th>\n",
       "      <td>0.509796</td>\n",
       "      <td>0.431470</td>\n",
       "      <td>0.456017</td>\n",
       "      <td>0.540300</td>\n",
       "      <td>0.216318</td>\n",
       "      <td>0.116407</td>\n",
       "      <td>0.813746</td>\n",
       "      <td>0.460184</td>\n",
       "      <td>2</td>\n",
       "      <td>C-Suite Strategies (A Special Report): Managem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-27</th>\n",
       "      <td>0.865728</td>\n",
       "      <td>0.678916</td>\n",
       "      <td>0.752635</td>\n",
       "      <td>0.704917</td>\n",
       "      <td>0.258925</td>\n",
       "      <td>0.250995</td>\n",
       "      <td>0.874040</td>\n",
       "      <td>0.582445</td>\n",
       "      <td>0</td>\n",
       "      <td>Exxon Eyes Staggered, But Larger, Rovuma LNG S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-28</th>\n",
       "      <td>0.656699</td>\n",
       "      <td>0.567402</td>\n",
       "      <td>0.619880</td>\n",
       "      <td>0.638819</td>\n",
       "      <td>0.150382</td>\n",
       "      <td>0.316675</td>\n",
       "      <td>0.962312</td>\n",
       "      <td>0.633900</td>\n",
       "      <td>2</td>\n",
       "      <td>PDF China National Chemical Corporation Ltd. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-29</th>\n",
       "      <td>0.805350</td>\n",
       "      <td>0.591039</td>\n",
       "      <td>0.692672</td>\n",
       "      <td>0.671698</td>\n",
       "      <td>0.314097</td>\n",
       "      <td>0.153800</td>\n",
       "      <td>0.916154</td>\n",
       "      <td>0.566586</td>\n",
       "      <td>0</td>\n",
       "      <td>Global Polyolefin Market 2023-2027 Published B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-30</th>\n",
       "      <td>0.714846</td>\n",
       "      <td>0.511150</td>\n",
       "      <td>0.599019</td>\n",
       "      <td>0.585574</td>\n",
       "      <td>0.170996</td>\n",
       "      <td>0.002758</td>\n",
       "      <td>0.850549</td>\n",
       "      <td>0.488014</td>\n",
       "      <td>0</td>\n",
       "      <td>30 March The high-yielding shares powering thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-31</th>\n",
       "      <td>0.611718</td>\n",
       "      <td>0.524529</td>\n",
       "      <td>0.563195</td>\n",
       "      <td>0.562412</td>\n",
       "      <td>0.301445</td>\n",
       "      <td>0.067498</td>\n",
       "      <td>0.908698</td>\n",
       "      <td>0.491249</td>\n",
       "      <td>2</td>\n",
       "      <td>Global Commodity Plastics Market 2023-2027 Pub...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>458 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Open_norm  High_norm  Low_norm  Close_norm  Volume_norm  \\\n",
       "Date                                                                  \n",
       "2021-06-08   0.624690   0.576438  0.482223    0.675290     0.490718   \n",
       "2021-06-09   0.757691   0.598956  0.716982    0.610093     0.229087   \n",
       "2021-06-10   0.724752   0.550617  0.533313    0.562715     0.210127   \n",
       "2021-06-11   0.524817   0.385896  0.512483    0.486898     0.118679   \n",
       "2021-06-14   0.509796   0.431470  0.456017    0.540300     0.216318   \n",
       "...               ...        ...       ...         ...          ...   \n",
       "2023-03-27   0.865728   0.678916  0.752635    0.704917     0.258925   \n",
       "2023-03-28   0.656699   0.567402  0.619880    0.638819     0.150382   \n",
       "2023-03-29   0.805350   0.591039  0.692672    0.671698     0.314097   \n",
       "2023-03-30   0.714846   0.511150  0.599019    0.585574     0.170996   \n",
       "2023-03-31   0.611718   0.524529  0.563195    0.562412     0.301445   \n",
       "\n",
       "            High-Open_norm  Low-Open_norm  Close-Open_norm  Label_2up1down  \\\n",
       "Date                                                                         \n",
       "2021-06-08        0.240756       0.785477         0.583539               0   \n",
       "2021-06-09        0.180812       0.878255         0.490038               0   \n",
       "2021-06-10        0.087882       0.660152         0.373188               0   \n",
       "2021-06-11        0.043161       0.780528         0.374858               0   \n",
       "2021-06-14        0.116407       0.813746         0.460184               2   \n",
       "...                    ...            ...              ...             ...   \n",
       "2023-03-27        0.250995       0.874040         0.582445               0   \n",
       "2023-03-28        0.316675       0.962312         0.633900               2   \n",
       "2023-03-29        0.153800       0.916154         0.566586               0   \n",
       "2023-03-30        0.002758       0.850549         0.488014               0   \n",
       "2023-03-31        0.067498       0.908698         0.491249               2   \n",
       "\n",
       "                                                         News  \n",
       "Date                                                           \n",
       "2021-06-08  Blowing Agent Market by Type (HC, HFC, HCFC), ...  \n",
       "2021-06-09  Global Asphalt Market 2021-2025 Published By: ...  \n",
       "2021-06-10  Global Benzene Market 2021-2025 Published By: ...  \n",
       "2021-06-11  Keystone Illustrates Pipelines' Hurdles The fa...  \n",
       "2021-06-14  C-Suite Strategies (A Special Report): Managem...  \n",
       "...                                                       ...  \n",
       "2023-03-27  Exxon Eyes Staggered, But Larger, Rovuma LNG S...  \n",
       "2023-03-28   PDF China National Chemical Corporation Ltd. ...  \n",
       "2023-03-29  Global Polyolefin Market 2023-2027 Published B...  \n",
       "2023-03-30  30 March The high-yielding shares powering thi...  \n",
       "2023-03-31  Global Commodity Plastics Market 2023-2027 Pub...  \n",
       "\n",
       "[458 rows x 10 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_df = data_norm.join(text_data_df, how = 'inner')\n",
    "all_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ae41a7b1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: yfinance in /Users/visriv/Library/Python/3.9/lib/python/site-packages (0.2.18)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from yfinance) (4.11.2)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from yfinance) (1.24.1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from yfinance) (2.3.8)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from yfinance) (1.4.4)\n",
      "Requirement already satisfied: html5lib>=1.1 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from yfinance) (1.1)\n",
      "Requirement already satisfied: requests>=2.26 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from yfinance) (2.28.2)\n",
      "Requirement already satisfied: pytz>=2022.5 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from yfinance) (2022.7.1)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from yfinance) (1.5.3)\n",
      "Requirement already satisfied: cryptography>=3.3.2 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from yfinance) (41.0.1)\n",
      "Requirement already satisfied: lxml>=4.9.1 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from yfinance) (4.9.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.4)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from cryptography>=3.3.2->yfinance) (1.15.1)\n",
      "Requirement already satisfied: six>=1.9 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from html5lib>=1.1->yfinance) (1.15.0)\n",
      "Requirement already satisfied: webencodings in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from requests>=2.26->yfinance) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from requests>=2.26->yfinance) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from requests>=2.26->yfinance) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from requests>=2.26->yfinance) (1.26.14)\n",
      "Requirement already satisfied: pycparser in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from cffi>=1.12->cryptography>=3.3.2->yfinance) (2.21)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from transformers) (4.64.1)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2023.6.3-cp39-cp39-macosx_10_9_x86_64.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from transformers) (1.24.1)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-macosx_10_11_x86_64.whl (4.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.4.0)\n",
      "Requirement already satisfied: fsspec in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (1.26.14)\n",
      "Installing collected packages: tokenizers, regex, filelock, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.12.0 huggingface-hub-0.15.1 regex-2023.6.3 tokenizers-0.13.3 transformers-4.29.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "56e91800",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m     y_train\u001b[38;5;241m.\u001b[39mappend(all_train[i, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;66;03m#TODO\u001b[39;00m\n\u001b[1;32m     15\u001b[0m X_numerical_train, y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(X_numerical_train), np\u001b[38;5;241m.\u001b[39marray(y_train)\n\u001b[0;32m---> 17\u001b[0m X_numerical_train \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_numerical_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     18\u001b[0m y_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(y_train)\u001b[38;5;241m.\u001b[39mlong()\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "all_train = all_train_df.values\n",
    "\n",
    "window_size = 5\n",
    "\n",
    "X_numerical_train = []\n",
    "y_train = []\n",
    "X_text_train = []\n",
    "\n",
    "for i in range(window_size, len(all_train)):\n",
    "    X_numerical_train.append(all_train[i-window_size: i, :-2])\n",
    "    X_text_train.append(all_train[i-window_size: i, -1])\n",
    "    \n",
    "    y_train.append(all_train[i, -2]) #TODO\n",
    "    \n",
    "X_numerical_train, y_train = np.array(X_numerical_train), np.array(y_train)\n",
    "\n",
    "X_numerical_train = torch.from_numpy(X_numerical_train).type(torch.Tensor)\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "328efba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(all_train[0: 5, :-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "34153024",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d42d800",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a3817c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class SiameseDataloader(Dataset):\n",
    "    \n",
    "    def __init__(self, X_numerical_train, y_train, X_text_train):\n",
    "        \n",
    "        pass\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return (X_train[index], text_train[index]), y_train[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_numerical_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708fe068",
   "metadata": {},
   "source": [
    "## Build model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "998f50c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)olve/main/vocab.json: 100%|███| 899k/899k [00:00<00:00, 1.06MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|████| 456k/456k [00:00<00:00, 621kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████| 482/482 [00:00<00:00, 193kB/s]\n",
      "Downloading pytorch_model.bin: 100%|███████| 1.43G/1.43G [06:27<00:00, 3.68MB/s]\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "class SiameseModel(nn.Module):\n",
    "    def __init__(self, input_dim1, input_dim2, \n",
    "                 hidden_dim1, hidden_dim2, hidden_dim3, \n",
    "                 num_layers1, num_layers2, output_dim1, output_dim2):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "        self.text_encoder = RobertaModel.from_pretrained('roberta-large')\n",
    "        \n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_dim1, hidden_dim1, num_layers1, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_dim2, hidden_dim2, num_layers2, batch_first=True)\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim1, output_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim2, output_dim2)\n",
    "        self.fc3 = nn.Linear(output_dim1+output_dim2, hidden_dim3)\n",
    "        self.fc4 = nn.Linear(hidden_dim3, 3)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        #left tower with numerical features\n",
    "        h10 = torch.zeros(self.num_layers1, x1.size(0), self.hidden_dim1).requires_grad_()\n",
    "        c10 = torch.zeros(self.num_layers1, x1.size(0), self.hidden_dim1).requires_grad_()\n",
    "        out1, (h1n, c1n) = self.lstm1(x1, (h10.detach(), c10.detach()))\n",
    "        out1 = self.fc1(out1[:, -1, :]) \n",
    "        \n",
    "        \n",
    "        # right tower with roberta on textual features\n",
    "        encoded_input = self.tokenizer(x2, return_tensors='pt')\n",
    "        e2 = self.text_encoder(**encoded_input)\n",
    "        \n",
    "        h20 = torch.zeros(self.num_layers2, e2.size(0), self.hidden_dim2).requires_grad_()\n",
    "        c20 = torch.zeros(self.num_layers2, e2.size(0), self.hidden_dim2).requires_grad_()\n",
    "        out2, (hn, cn) = self.lstm2(x2, (h20.detach(), c20.detach()))\n",
    "        out2 = self.fc2(out2[:, -1, :]) \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        output = torch.cat((out1, out2),1)\n",
    "        output = F.relu(self.fc3(output))\n",
    "        output = self.fc4(output)\n",
    "        return output\n",
    "    \n",
    "#TODO : correct these values\n",
    "model = SiameseModel(input_dim1 = 8, input_dim2 = 1024, \n",
    "                 hidden_dim1 = 10, hidden_dim2 = 800, hidden_dim3 = 500, \n",
    "                 num_layers1 = 1, num_layers2 = 1, output_dim1 = 50, output_dim2 = 50)\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "83623987",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SiameseModel(\n",
      "  (text_encoder): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (12): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (13): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (14): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (15): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (16): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (17): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (18): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (19): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (20): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (21): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (22): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (23): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (lstm1): LSTM(8, 10, batch_first=True)\n",
      "  (lstm2): LSTM(768, 800, batch_first=True)\n",
      "  (fc1): Linear(in_features=10, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=800, out_features=50, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=500, bias=True)\n",
      "  (fc4): Linear(in_features=500, out_features=3, bias=True)\n",
      ")\n",
      "407\n",
      "torch.Size([50265, 1024])\n",
      "torch.Size([514, 1024])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([4096, 1024])\n",
      "torch.Size([4096])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([4096, 1024])\n",
      "torch.Size([4096])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([4096, 1024])\n",
      "torch.Size([4096])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([4096, 1024])\n",
      "torch.Size([4096])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([4096, 1024])\n",
      "torch.Size([4096])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([4096, 1024])\n",
      "torch.Size([4096])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([4096, 1024])\n",
      "torch.Size([4096])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([4096, 1024])\n",
      "torch.Size([4096])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([4096, 1024])\n",
      "torch.Size([4096])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([4096, 1024])\n",
      "torch.Size([4096])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([4096, 1024])\n",
      "torch.Size([4096])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([4096, 1024])\n",
      "torch.Size([4096])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([4096, 1024])\n",
      "torch.Size([4096])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([4096, 1024])\n",
      "torch.Size([4096])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([4096, 1024])\n",
      "torch.Size([4096])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([4096, 1024])\n",
      "torch.Size([4096])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([4096, 1024])\n",
      "torch.Size([4096])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([4096, 1024])\n",
      "torch.Size([4096])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([4096, 1024])\n",
      "torch.Size([4096])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([4096, 1024])\n",
      "torch.Size([4096])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([1024])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([4096, 1024])\n",
      "torch.Size([4096])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([4096, 1024])\n",
      "torch.Size([4096])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([4096, 1024])\n",
      "torch.Size([4096])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([4096, 1024])\n",
      "torch.Size([4096])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([40, 8])\n",
      "torch.Size([40, 10])\n",
      "torch.Size([40])\n",
      "torch.Size([40])\n",
      "torch.Size([3200, 768])\n",
      "torch.Size([3200, 800])\n",
      "torch.Size([3200])\n",
      "torch.Size([3200])\n",
      "torch.Size([50, 10])\n",
      "torch.Size([50])\n",
      "torch.Size([50, 800])\n",
      "torch.Size([50])\n",
      "torch.Size([500, 100])\n",
      "torch.Size([500])\n",
      "torch.Size([3, 500])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(len(list(model.parameters())))\n",
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c860820d",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6a4cd598",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10 MSE:  0.8407482504844666\n",
      "Epoch  20 MSE:  0.8068487048149109\n",
      "Epoch  30 MSE:  0.7411683797836304\n",
      "Epoch  40 MSE:  0.7207826375961304\n",
      "Epoch  50 MSE:  0.47958889603614807\n",
      "Epoch  60 MSE:  0.29257601499557495\n",
      "Epoch  70 MSE:  0.14471979439258575\n",
      "Epoch  80 MSE:  0.045132964849472046\n",
      "Epoch  90 MSE:  0.01147476676851511\n",
      "Epoch  100 MSE:  0.0039354609325528145\n",
      "Epoch  110 MSE:  0.001934159197844565\n",
      "Epoch  120 MSE:  0.0012813452631235123\n",
      "Epoch  130 MSE:  0.0009837034158408642\n",
      "Epoch  140 MSE:  0.0008080229163169861\n",
      "Epoch  150 MSE:  0.0006962246261537075\n",
      "Epoch  160 MSE:  0.0006137943710200489\n",
      "Epoch  170 MSE:  0.0005495513323694468\n",
      "Epoch  180 MSE:  0.0004968825378455222\n",
      "Epoch  190 MSE:  0.00045257911551743746\n",
      "Epoch  200 MSE:  0.0004146856372244656\n",
      "Epoch  210 MSE:  0.00038184275035746396\n",
      "Epoch  220 MSE:  0.0003531062975525856\n",
      "Epoch  230 MSE:  0.0003277502255514264\n",
      "Epoch  240 MSE:  0.0003052429819945246\n",
      "Epoch  250 MSE:  0.0002851351164281368\n",
      "Epoch  260 MSE:  0.000267079594777897\n",
      "Epoch  270 MSE:  0.00025078782346099615\n",
      "Epoch  280 MSE:  0.00023602470173500478\n",
      "Epoch  290 MSE:  0.00022259485558606684\n",
      "Epoch  300 MSE:  0.0002103321603499353\n",
      "Epoch  310 MSE:  0.00019910575065296143\n",
      "Epoch  320 MSE:  0.00018879210983868688\n",
      "Epoch  330 MSE:  0.00017928759916685522\n",
      "Epoch  340 MSE:  0.0001705099130049348\n",
      "Epoch  350 MSE:  0.00016238645184785128\n",
      "Epoch  360 MSE:  0.0001548424334032461\n",
      "Epoch  370 MSE:  0.0001478299527661875\n",
      "Epoch  380 MSE:  0.00014129634655546397\n",
      "Epoch  390 MSE:  0.0001352005056105554\n",
      "Epoch  400 MSE:  0.00012949401570949703\n",
      "Epoch  410 MSE:  0.0001241543359356001\n",
      "Epoch  420 MSE:  0.00011914050992345437\n",
      "Epoch  430 MSE:  0.00011443185940152034\n",
      "Epoch  440 MSE:  0.00010999880032613873\n",
      "Epoch  450 MSE:  0.00010582833056105301\n",
      "Epoch  460 MSE:  0.00010189037857344374\n",
      "Epoch  470 MSE:  9.817193495109677e-05\n",
      "Epoch  480 MSE:  9.465834591537714e-05\n",
      "Epoch  490 MSE:  9.132863488048315e-05\n",
      "Epoch  500 MSE:  8.817756315693259e-05\n",
      "Epoch  510 MSE:  8.518871618434787e-05\n",
      "Epoch  520 MSE:  8.234671986429021e-05\n",
      "Epoch  530 MSE:  7.964819815242663e-05\n",
      "Epoch  540 MSE:  7.708567136432976e-05\n",
      "Epoch  550 MSE:  7.464136433554813e-05\n",
      "Epoch  560 MSE:  7.231043127831072e-05\n",
      "Epoch  570 MSE:  7.009095861576498e-05\n",
      "Epoch  580 MSE:  6.797187961637974e-05\n",
      "Epoch  590 MSE:  6.594765727641061e-05\n",
      "Epoch  600 MSE:  6.401153950719163e-05\n",
      "Epoch  610 MSE:  6.216402107384056e-05\n",
      "Epoch  620 MSE:  6.039403160684742e-05\n",
      "Epoch  630 MSE:  5.8696503401733935e-05\n",
      "Epoch  640 MSE:  5.707578020519577e-05\n",
      "Epoch  650 MSE:  5.5515476560685784e-05\n",
      "Epoch  660 MSE:  5.4020911193219945e-05\n",
      "Epoch  670 MSE:  5.2586528909159824e-05\n",
      "Epoch  680 MSE:  5.1207523938501254e-05\n",
      "Epoch  690 MSE:  4.988486034562811e-05\n",
      "Epoch  700 MSE:  4.8607224016450346e-05\n",
      "Epoch  710 MSE:  4.7382065531564876e-05\n",
      "Epoch  720 MSE:  4.619977335096337e-05\n",
      "Epoch  730 MSE:  4.506563345785253e-05\n",
      "Epoch  740 MSE:  4.397338489070535e-05\n",
      "Epoch  750 MSE:  4.2914838559227064e-05\n",
      "Epoch  760 MSE:  4.189506944385357e-05\n",
      "Epoch  770 MSE:  4.091285518370569e-05\n",
      "Epoch  780 MSE:  3.996507075498812e-05\n",
      "Epoch  790 MSE:  3.9044738514348865e-05\n",
      "Epoch  800 MSE:  3.8160764233907685e-05\n",
      "Epoch  810 MSE:  3.730784737854265e-05\n",
      "Epoch  820 MSE:  3.647780977189541e-05\n",
      "Epoch  830 MSE:  3.567449311958626e-05\n",
      "Epoch  840 MSE:  3.489935625111684e-05\n",
      "Epoch  850 MSE:  3.4149736166000366e-05\n",
      "Epoch  860 MSE:  3.34188953274861e-05\n",
      "Epoch  870 MSE:  3.271406239946373e-05\n",
      "Epoch  880 MSE:  3.20357212331146e-05\n",
      "Epoch  890 MSE:  3.137230305583216e-05\n",
      "Epoch  900 MSE:  3.072886829613708e-05\n",
      "Epoch  910 MSE:  3.0105424229986966e-05\n",
      "Epoch  920 MSE:  2.950196540041361e-05\n",
      "Epoch  930 MSE:  2.891439726226963e-05\n",
      "Epoch  940 MSE:  2.8345370083115995e-05\n",
      "Epoch  950 MSE:  2.7794885681942105e-05\n",
      "Epoch  960 MSE:  2.725427293626126e-05\n",
      "Epoch  970 MSE:  2.6731717298389412e-05\n",
      "Epoch  980 MSE:  2.62262619799003e-05\n",
      "Epoch  990 MSE:  2.572947778389789e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 20\n",
    "loss_arr = np.zeros(num_epochs)\n",
    "\n",
    "for t in range(num_epochs):\n",
    "    \n",
    "    for batch_idx, (x1, x2, y_train) in enumerate(train_loader):\n",
    "        \n",
    "        # Forward pass\n",
    "        y_train_pred = model(x1, x2)\n",
    "\n",
    "\n",
    "        loss = criterion(y_train_pred, y_train)\n",
    "        if t % 10 == 0 and t !=0:\n",
    "            print(\"Epoch \", t, \"CELoss: \", loss.item())\n",
    "        loss_arr[t] = loss.item()\n",
    "\n",
    "        # Zero out gradient, else they will accumulate between epochs\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimiser.step()\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "        \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d42ebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_arr, label=\"Training loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bb01b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
