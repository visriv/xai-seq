{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ff5a093",
   "metadata": {
    "id": "7ff5a093"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: yfinance in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.2.25)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from yfinance) (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from yfinance) (1.24.3)\n",
      "Requirement already satisfied: requests>=2.26 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from yfinance) (2.29.0)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: lxml>=4.9.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from yfinance) (4.9.3)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from yfinance) (1.4.4)\n",
      "Requirement already satisfied: pytz>=2022.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from yfinance) (2023.3)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from yfinance) (2.3.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from yfinance) (4.12.2)\n",
      "Requirement already satisfied: html5lib>=1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from yfinance) (1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.3.2.post1)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.26->yfinance) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.26->yfinance) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.26->yfinance) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.26->yfinance) (2023.5.7)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.31.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.29.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: wandb in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.15.5)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (3.1.32)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (2.29.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (1.28.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: pathtools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (67.7.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: beautifulsoup4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from beautifulsoup4) (2.3.2.post1)\n"
     ]
    }
   ],
   "source": [
    "!pip install yfinance\n",
    "!pip install transformers\n",
    "!pip install wandb\n",
    "!pip install beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a3524a9",
   "metadata": {
    "id": "1a3524a9"
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import mpl, plt\n",
    "import math, time\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from operator import itemgetter\n",
    "from tqdm import tqdm\n",
    "from math import sqrt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24afc749",
   "metadata": {
    "id": "24afc749"
   },
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ef9b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e128245b",
   "metadata": {
    "id": "e128245b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvisriv\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ec2-user/SageMaker/wandb/run-20230720_040021-2u6boszt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/visriv/stock_prediction/runs/2u6boszt' target=\"_blank\">solar-vortex-51</a></strong> to <a href='https://wandb.ai/visriv/stock_prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/visriv/stock_prediction' target=\"_blank\">https://wandb.ai/visriv/stock_prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/visriv/stock_prediction/runs/2u6boszt' target=\"_blank\">https://wandb.ai/visriv/stock_prediction/runs/2u6boszt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/visriv/stock_prediction/runs/2u6boszt?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f96e8d1bb50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "wandb.init(project=\"stock_prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ea47cc",
   "metadata": {
    "id": "76ea47cc"
   },
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a8dec6d",
   "metadata": {
    "id": "9a8dec6d"
   },
   "outputs": [],
   "source": [
    "no_of_days_to_lookforward = 1\n",
    "no_of_days_to_lookback = 10\n",
    "up_threshold = 0.015\n",
    "down_threshold = -0.015\n",
    "max_text_per_iter = 20\n",
    "batch_size = 16\n",
    "MAX_LEN = 10\n",
    "num_epochs = 200\n",
    "save_freq_epoch = 10\n",
    "run_id = str(51)\n",
    "output_path = '/content/drive/MyDrive/machine_learning/projects/xai-seq/output/run_' + run_id\n",
    "# !mkdir $output_path\n",
    "# !mkdir /content/drive/MyDrive/machine_learning/projects/xai-seq/output/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af59387d",
   "metadata": {
    "id": "af59387d"
   },
   "source": [
    "### Get stocks data for last N days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ebb836a",
   "metadata": {
    "id": "6ebb836a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64983989",
   "metadata": {
    "id": "64983989"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "stock_symbols = [ 'XOM']#, 'CVX', 'COP', 'SHEL']\n",
    "no_of_days = 4*365\n",
    "\n",
    "EXPORT_DATA_FOLDER = './data/'\n",
    "# !mkdir data\n",
    "# Set the start and end dates for the data\n",
    "# here matching it with dates of news text available\n",
    "train_start = datetime.strptime('2020/01/04', '%Y/%m/%d')\n",
    "train_end = datetime.strptime('2022/09/30', '%Y/%m/%d')\n",
    "test_start = datetime.strptime('2022/10/01', '%Y/%m/%d')\n",
    "test_end = datetime.strptime('2023/01/04', '%Y/%m/%d')\n",
    "\n",
    "\n",
    "# start = datetime.datetime.now() - datetime.timedelta(days=no_of_days)\n",
    "# end = datetime.datetime.now()\n",
    "\n",
    "train_data_final = pd.DataFrame()\n",
    "test_data_final = pd.DataFrame()\n",
    "\n",
    "# Get training data\n",
    "for symbol in stock_symbols:\n",
    "    # Download the historical price and volume data using yfiance\n",
    "    train_data_raw = yf.download(symbol, start=train_start, end=train_end)\n",
    "\n",
    "    # Normalize features by percent of changes between today and yesterday\n",
    "    pct_change_open = train_data_raw['Open'].fillna(0)#.pct_change().fillna(0)\n",
    "    pct_change_high = train_data_raw['High'].fillna(0)#.pct_change().fillna(0)\n",
    "    pct_change_high_over_open = (train_data_raw['High']-train_data_raw['Open'])/train_data_raw['Open']\n",
    "    pct_change_low = train_data_raw['Low'].fillna(0)#.pct_change().fillna(0)\n",
    "    pct_change_low_over_open = (train_data_raw['Low']-train_data_raw['Open'])/train_data_raw['Open']\n",
    "    pct_change_close = train_data_raw['Close'].pct_change().fillna(0)\n",
    "    close = train_data_raw['Close'].fillna(0)#.pct_change().fillna(0)\n",
    "    pct_change_close_over_open = (train_data_raw['Close']-train_data_raw['Open'])/train_data_raw['Open']\n",
    "    pct_change_adjclose = train_data_raw['Adj Close'].fillna(0)#.pct_change().fillna(0)\n",
    "    pct_change_adjclose_over_open = (train_data_raw['Adj Close']-train_data_raw['Open'])/train_data_raw['Open']\n",
    "    pct_change_volume = train_data_raw['Volume'].fillna(0)#.pct_change().fillna(0)\n",
    "\n",
    "    # Prepare labels: 2 means the close price of tomorow is higher than today's close price; 1 is down; 0 means the movement is between up_threshold and down_threshold\n",
    "    label = np.where(pct_change_close > up_threshold, 2, np.where(pct_change_close < down_threshold, 1, 0))[1:]\n",
    "    label = np.append(label, 0)\n",
    "\n",
    "    # Construct a train_data_norm data frame\n",
    "    train_data_norm = pd.DataFrame({'Open_norm':pct_change_open,\n",
    "                              'High_norm':pct_change_high,\n",
    "                              'Low_norm': pct_change_low,\n",
    "                              'Close_norm':pct_change_close,\n",
    "                              'Close':close,\n",
    "                              'Volume_norm':pct_change_volume,\n",
    "                              'High-Open_norm':pct_change_high_over_open,\n",
    "                              'Low-Open_norm':pct_change_low_over_open,\n",
    "                              'Close-Open_norm':pct_change_close_over_open,\n",
    "                              'Label_2up1down':label})\n",
    "\n",
    "    # Normalize by min-max normalization after the pct normalization\n",
    "    train_data_norm['Open_norm'] = train_data_norm['Open_norm'].apply(lambda x: (x - train_data_norm['Open_norm'].min()) / (train_data_norm['Open_norm'].max() - train_data_norm['Open_norm'].min()))\n",
    "    train_data_norm['High_norm'] = train_data_norm['High_norm'].apply(lambda x: (x - train_data_norm['High_norm'].min()) / (train_data_norm['High_norm'].max() - train_data_norm['High_norm'].min()))\n",
    "    train_data_norm['Low_norm'] = train_data_norm['Low_norm'].apply(lambda x: (x - train_data_norm['Low_norm'].min()) / (train_data_norm['Low_norm'].max() - train_data_norm['Low_norm'].min()))\n",
    "    train_data_norm['Close_norm'] = train_data_norm['Close_norm'].apply(lambda x: (x - train_data_norm['Close_norm'].min()) / (train_data_norm['Close_norm'].max() - train_data_norm['Close_norm'].min()))\n",
    "    train_data_norm['Close'] = train_data_norm['Close'].apply(lambda x: (x - train_data_norm['Close'].min()) / (train_data_norm['Close'].max() - train_data_norm['Close'].min()))\n",
    "    train_data_norm['Volume_norm'] = train_data_norm['Volume_norm'].apply(lambda x: (x - train_data_norm['Volume_norm'].min()) / (train_data_norm['Volume_norm'].max() - train_data_norm['Volume_norm'].min()))\n",
    "    train_data_norm['High-Open_norm'] = train_data_norm['High-Open_norm'].apply(lambda x: (x - train_data_norm['High-Open_norm'].min()) / (train_data_norm['High-Open_norm'].max() - train_data_norm['High-Open_norm'].min()))\n",
    "    train_data_norm['Low-Open_norm'] = train_data_norm['Low-Open_norm'].apply(lambda x: (x - train_data_norm['Low-Open_norm'].min()) / (train_data_norm['Low-Open_norm'].max() - train_data_norm['Low-Open_norm'].min()))\n",
    "    train_data_norm['Close-Open_norm'] = train_data_norm['Close-Open_norm'].apply(lambda x: (x - train_data_norm['Close-Open_norm'].min()) / (train_data_norm['Close-Open_norm'].max() - train_data_norm['Close-Open_norm'].min()))\n",
    "\n",
    "    # Remove the first and the last row, becuase of NAN values\n",
    "    train_data_raw = train_data_raw.iloc[1:-1]\n",
    "    train_data_norm = train_data_norm.iloc[1:-1]\n",
    "\n",
    "    train_data_raw.to_csv(EXPORT_DATA_FOLDER+symbol+'train_raw_data.csv', index=True)\n",
    "    train_data_norm.to_csv(EXPORT_DATA_FOLDER+symbol+'train_norm_data.csv', index=True)\n",
    "\n",
    "    train_data_final = pd.concat([train_data_final, train_data_norm])\n",
    "\n",
    "# Get test data\n",
    "for symbol in stock_symbols:\n",
    "    # Download the historical price and volume data using yfiance\n",
    "    test_data_raw = yf.download(symbol, start=test_start, end=test_end)\n",
    "\n",
    "    # Normalize features by percent of changes between today and yesterday\n",
    "    pct_change_open = test_data_raw['Open'].fillna(0)#.pct_change().fillna(0)\n",
    "    pct_change_high = test_data_raw['High'].fillna(0)#.pct_change().fillna(0)\n",
    "    pct_change_high_over_open = (test_data_raw['High']-test_data_raw['Open'])/test_data_raw['Open']\n",
    "    pct_change_low = test_data_raw['Low'].fillna(0)#.pct_change().fillna(0)\n",
    "    pct_change_low_over_open = (test_data_raw['Low']-test_data_raw['Open'])/test_data_raw['Open']\n",
    "    pct_change_close = test_data_raw['Close'].pct_change().fillna(0)\n",
    "    close = test_data_raw['Close'].fillna(0)#.pct_change().fillna(0)\n",
    "    pct_change_close_over_open = (test_data_raw['Close']-test_data_raw['Open'])/test_data_raw['Open']\n",
    "    pct_change_adjclose = test_data_raw['Adj Close'].fillna(0)#.pct_change().fillna(0)\n",
    "    pct_change_adjclose_over_open = (test_data_raw['Adj Close']-test_data_raw['Open'])/test_data_raw['Open']\n",
    "    pct_change_volume = test_data_raw['Volume'].fillna(0)#.pct_change().fillna(0)\n",
    "\n",
    "    # Prepare labels: 2 means the close price of tomorow is higher than today's close price; 1 is down; 0 means the movement is between up_threshold and down_threshold\n",
    "    label = np.where(pct_change_close > up_threshold, 2, np.where(pct_change_close < down_threshold, 1, 0))[1:]\n",
    "    label = np.append(label, 0)\n",
    "\n",
    "    # Construct a test_data_norm data frame\n",
    "    test_data_norm = pd.DataFrame({'Open_norm':pct_change_open,\n",
    "                              'High_norm':pct_change_high,\n",
    "                              'Low_norm': pct_change_low,\n",
    "                              'Close_norm':pct_change_close,\n",
    "                              'Close':close,\n",
    "                              'Volume_norm':pct_change_volume,\n",
    "                              'High-Open_norm':pct_change_high_over_open,\n",
    "                              'Low-Open_norm':pct_change_low_over_open,\n",
    "                              'Close-Open_norm':pct_change_close_over_open,\n",
    "                              'Label_2up1down':label})\n",
    "\n",
    "    # Normalize by min-max normalization after the pct normalization\n",
    "    test_data_norm['Open_norm'] = test_data_norm['Open_norm'].apply(lambda x: (x - test_data_norm['Open_norm'].min()) / (test_data_norm['Open_norm'].max() - test_data_norm['Open_norm'].min()))\n",
    "    test_data_norm['High_norm'] = test_data_norm['High_norm'].apply(lambda x: (x - test_data_norm['High_norm'].min()) / (test_data_norm['High_norm'].max() - test_data_norm['High_norm'].min()))\n",
    "    test_data_norm['Low_norm'] = test_data_norm['Low_norm'].apply(lambda x: (x - test_data_norm['Low_norm'].min()) / (test_data_norm['Low_norm'].max() - test_data_norm['Low_norm'].min()))\n",
    "    test_data_norm['Close_norm'] = test_data_norm['Close_norm'].apply(lambda x: (x - test_data_norm['Close_norm'].min()) / (test_data_norm['Close_norm'].max() - test_data_norm['Close_norm'].min()))\n",
    "    test_data_norm['Close'] = test_data_norm['Close'].apply(lambda x: (x - test_data_norm['Close'].min()) / (test_data_norm['Close'].max() - test_data_norm['Close'].min()))\n",
    "    test_data_norm['Volume_norm'] = test_data_norm['Volume_norm'].apply(lambda x: (x - test_data_norm['Volume_norm'].min()) / (test_data_norm['Volume_norm'].max() - test_data_norm['Volume_norm'].min()))\n",
    "    test_data_norm['High-Open_norm'] = test_data_norm['High-Open_norm'].apply(lambda x: (x - test_data_norm['High-Open_norm'].min()) / (test_data_norm['High-Open_norm'].max() - test_data_norm['High-Open_norm'].min()))\n",
    "    test_data_norm['Low-Open_norm'] = test_data_norm['Low-Open_norm'].apply(lambda x: (x - test_data_norm['Low-Open_norm'].min()) / (test_data_norm['Low-Open_norm'].max() - test_data_norm['Low-Open_norm'].min()))\n",
    "    test_data_norm['Close-Open_norm'] = test_data_norm['Close-Open_norm'].apply(lambda x: (x - test_data_norm['Close-Open_norm'].min()) / (test_data_norm['Close-Open_norm'].max() - test_data_norm['Close-Open_norm'].min()))\n",
    "\n",
    "    # Remove the first and the last row, becuase of NAN values\n",
    "    test_data_raw = test_data_raw.iloc[1:-1]\n",
    "    test_data_norm = test_data_norm.iloc[1:-1]\n",
    "\n",
    "    test_data_raw.to_csv(EXPORT_DATA_FOLDER+symbol+'test_raw_data.csv', index=True)\n",
    "    test_data_norm.to_csv(EXPORT_DATA_FOLDER+symbol+'test_norm_data.csv', index=True)\n",
    "    \n",
    "    test_data_final = pd.concat([test_data_final, test_data_norm])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "044f2161",
   "metadata": {
    "id": "044f2161"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f96e1fc8d90>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAAGdCAYAAADaPpOnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABnQElEQVR4nO3dd5hU5dkG8Hv67MzOzja2wS5lWXoRQZGigCBiTaKxYSyJSTRWNFFjjBFNBEtC+NTEHiQaTLMbGyqi0jsLCAgsy8L23md2Zs73x5lz5kzZNjt19/5dFxc7Z86cfY+MO88+7/M+r0oQBAFEREREcUId7QEQERER9QaDFyIiIoorDF6IiIgorjB4ISIiorjC4IWIiIjiCoMXIiIiiisMXoiIiCiuMHghIiKiuKKN9gCC4XK5UFpaCovFApVKFe3hEBERUQ8IgoCmpibk5ORArQ4+fxKXwUtpaSlyc3OjPQwiIiIKQklJCYYMGRL06+MyeLFYLADEm09KSoryaIiIiKgnGhsbkZubK3+OBysugxdpqigpKYnBCxERUZzpa8kHC3aJiIgorjB4ISIiorjC4IWIiIjiCoMXIiIiiisMXoiIiCiuMHghIiKiuMLghYiIiOIKgxciIiKKKwxeiIiIKK4weCEiIqK4wuCFiIiI4gqDFyIiIoorDF6IiIj6A3sL0FzpeVyxH9jwNOCwR29MYRKXu0oTERGRjzeuAY5/Ddy+HUjLB56bKR7X6ICzfhHdsYUYMy9ERETxassLwCsLgeYqoGg9ILjEY0ont0dnbGHEzAsREVG8+ug+8e//3eM51lwBdLR7HuuMkR1TBDDzQkREFG/KC4GVkzyPD3+ieFIAGko8D13OiA0rUhi8EBERxZudfwfqiz2PnTbP1/ZWoE7xXHNF5MYVIQxeiIiI4klLDXBic+fP21uAuiLPY+UKpH6CwQsREVE8+cflQPnezp+3NwFH13keM/NCREREUVW6K/DxUReIf9uagMMfe463VANOR/jHFUEMXoiIiOLV8HM8XycOEv9uqQYEZZGuAOxcDdQWob9g8EJERBSvDEmer80Z4t/2ZvcBFZCYKX75v3uAd26N6NDCicELERFRvFIGL1KgItEnApYsz+MTGyMzpghg8EJERBSvjIrgxToY0Jk8j/Um/4Cmn2DwQkREFK+UmRedCcib4XmsNwOJGd7nf/57sSYmzjF4ISIiilfKzItaA4y50PNYbwZMad7nf/1HYN1jkRlbGDF4ISIiilcGi+KBChh7qedhSw1w5s1A/nzv11R/F5GhhRODFyIionjhu0+RRu/5OinHe5qoqVSsg/nRm96vyZ7c9fco2wNU7O/bOMOMwQsREVG86GjzfiwIwHVvA5e9BKTli8cWPCL+PfVG8W+VqufXb60FXjgHeG5mTG/oqI32AIiIiKgHOtqBL/7gfUxwAvnneh+bdReQOx3IHBf4Oq4uuu1WHvB8bW8GjNbgxhpmzLwQERHFg8MfAVue8z4WKDuiUgFDZ3gHHlmTPF87Ozr/HnXHPV/bmoIaZiQweCEiIooH9Sf8jwk9nNq57h1g3PfEr532zs+rOuj52tbc+XlRxuCFiIgoHjS5d4eeeYfnmMvVs9ea04DBU92v6WLaqEIxbdRW17vxRRCDFyIionjQ7A5eEhUt/4UeBi+AZ2VSV5kXZc3LqkU9v3aEsWCXiIgoHsjBSyYw+kLgyOfAhMt7/nq1+yM/UM3LV08BlQeBprK+jzMCGLwQERHFA1uj+HdCMnD1GsBhA3TGnr9ezry4g5eSrUDlt8DUG/xXMQGAWten4YYTgxciIqJ4YG8R/9YniiuKehO4AIDGHYy4OsR+Lq+cJz7OmRL4/Kk3BDfOCGDwQkREFA/k4MUc3OulTIqzAzi53XO88ZT3eZOuBoZMA6b8KLjvEwEMXoiIiOKBtHQ52OBFowheHO2e42313ucVnAdM/GFw3yNCuNqIiIgo1gmC2PEWEKeNgqGcNlKuOPJdEp05PrjrRxCDFyIioljX0QZAEL8OOvOiWCrdVfCSNjK460cQgxciIqJYJ9W7AIDOFNw15KXSDnGlkkQZvFzwlCdDE8MYvBAREcW6jU97vlYH+dEtZV4q9wOlOz3HpeDFmgtM/3lw144wBi9ERESxbu+/+n4NZUZl5989X7fVin9re7n0OooYvBAREcU6U5r499m/DP4a6k4WGEuZFwYvREREFBIuF1BXLH49eXHw19ElBD4uLZXWGoK/doQxeCEiIoplVd8CHS2AzgykDA3+Op0tsa4rEv+WamLiAIMXIiKiWFb0tfh33ll9WwnU3RLr9vrgrx1hDF6IiIhi2XF38DJsdt+u011zu/xz+3b9CGLwQkREFKtcLqB4g/j18HP6di1tF9NCRiuwMMDO0jGKwQsREVGsOrZOXA2kTwSyJ4fv+2RPFneqjhMMXoiIiGLVnjfEv0ctCm/n23AGRmHA4IWIiChWtTeKf4+YG97vk8XghYiIKGLKG9rRbHNEexjh4WgT/+6sR0uoxMFO0kqdtNsjIiKKfZWN7Tj7yS+QoNPgq/vmIdkUP71KekTaQDFUDeRu/B9w6CPgrFuBE5uAzc+JtS6DRofm+hHC4IWIiOJWSV0bOpwCOpwObDhSg4smZUd7SKHV4c68hKp1/7DZniXXE38ITLhc/DqOinUBBi9ERBTH7A6X/HVDW0cURxImcuYlTPsOxVnQImHNCxERxS27s78HL+3i33G0aWIkMHghIqK41aHIvNS32aM4kjCRghcdgxclBi9ERBS3vDIvrXGceREE4M2fAv+5UeyqK2HmJSAGL0REFLf6Tc1LewNQ+B9g/9vAoQ89xzsYvATC4IWIiOKWMnipj+fMi1SYCwBbXxT/drkAZ5gLduMUVxsREVHcsjmVNS9xHLw4FcFL0Xqg+jvg6DrPMda8eGHwQkREcUuZeWmM5+DF4VNsvO4xcQpJwsyLF04bERFR3OpQZl5a43i1kTLzAngHLslDw7spYxzqdfDy1Vdf4ZJLLkFOTg5UKhXeeecdr+cFQcDSpUuRk5ODhIQEzJ07F/v37/c6x2az4Y477kB6ejrMZjMuvfRSnDx5sk83QkREA48y89Jid3oFM3HFYQt8fNYS4M7dkRxJXOh18NLS0oLJkyfj2WefDfj8k08+iRUrVuDZZ5/Ftm3bkJWVhfPOOw9NTU3yOUuWLMHbb7+Nf/7zn/jmm2/Q3NyMiy++GE6nM/g7ISKiAUcZvABxvOLIGSBrlJwHnPcIoOYkia9e17xccMEFuOCCCwI+JwgCVq5ciQcffBCXXXYZAGD16tXIzMzEmjVrcPPNN6OhoQGvvPIKXnvtNSxYsAAA8PrrryM3NxefffYZzj///D7cDhERDSR2n0xLfWsH0hNDtIlhJAXKvGji8D4iJKThXFFREcrLy7Fw4UL5mMFgwJw5c7Bx40YAwI4dO9DR0eF1Tk5ODiZMmCCf48tms6GxsdHrDxERUb/OvIRqJ+l+KKTBS3l5OQAgMzPT63hmZqb8XHl5OfR6PVJSUjo9x9fy5cthtVrlP7m5uaEcNhERxSmbX/ASp0W7gTIvDF46FZaJNJXPLpWCIPgd89XVOQ888AAaGhrkPyUlJSEbKxERxa/Kxnavx3HbqM53tRHAaaMuhDR4ycrKAgC/DEplZaWcjcnKyoLdbkddXV2n5/gyGAxISkry+kNERAObIAjYeUL8LBmSkgAgjqeNfPu8AIBWH/lxxImQBi/Dhw9HVlYW1q5dKx+z2+1Yv349Zs6cCQCYOnUqdDqd1zllZWXYt2+ffA4REVF3iqpbUNfaAb1WjZn5aQCik3nZcKQaL311DI6+LNNm5qVXer3aqLm5GUeOHJEfFxUVYffu3UhNTUVeXh6WLFmCZcuWoaCgAAUFBVi2bBlMJhMWL14MALBarbjpppvwy1/+EmlpaUhNTcWvfvUrTJw4UV59RERE1JXG9g7sKBazLpOHWDHIIn7QRyPzcvuanahr7UB1iw0PXDA2uIsEzLwweOlMr4OX7du3Y968efLje+65BwBwww034NVXX8V9992HtrY23Hrrrairq8P06dPx6aefwmKxyK/585//DK1WiyuvvBJtbW2YP38+Xn31VWg0mhDcEhER9Wdv7TyJe/69R358+tAUWBPEDrSRDl4cThfq3NmeF9Yfw5xRgzAzP733FwqUeWHw0qleBy9z586FIAidPq9SqbB06VIsXbq003OMRiOeeeYZPPPMM7399kRENMApAxcAmJqXIk8XRXqLgMom76Bj8Utb8MmSczA6y9LJKzrBPi+9wrZ9REQU104fmgKrScy8RHpn6bKGNr9jRdXNvb9Qh/91mHnpHIMXIiKKG76Z/6FpJqQnGqI2bVTTLGZ6JucmB3ze5RLwyf5y1LZ0kxFytPsfY/DSKQYvREQUN6qavadXxmWLrTOS3ZmXk7Vtfl13w6mtQ9yTL9GgkVc82Z2eAGvl59/h5td24L7/7u36Qh2t4t8Gq+eYhkulO8PghYiI4sbRyhavxwUZiQCA3BQTjDo17E4XPv+2ImLjabGJwUuCTgudRvxI7VAET09//h0A4LPuxiRNG9kaPMdyzwzdQPsZBi9ERBQ3jvnUk5yWlwwAMBu0mDsqAwBQ2hBgCiZMWu0OAIBJr/EEL+5+Lx296fsiBS/554p/J+cBYy4K2Tj7m16vNiIiIooWqbcLIHbVPbtgkPxY7vUSwRVHbXYx82I2aOBweQctu07Uy+cZtOqut8qRgpfxPwBm3M6sSzcYvBARUVxYvfE43tp5CgDw24vG4qozcuVsBwCkuOte6iLYZbfF7pk2anV/XdVkgyAI+Oa7Kvk8m8OFxnaHXFjsx+EOXnQmYOT8sI65P+C0ERERxYWH39svfz1pSDIsRu9AwGoSC1wjuVy6zT1tZDZ4po2e/uIIfvDXjXjp6yKvc/+y7ojf62UdiuCFusXghYiI4k7+ILPfsWR3ViOSjeqkbEuCouYFAHaX1MsrkSSrNx7v/EId7jodnTHUQ+yXGLwQEVFcMOk9W8ikmv2XEUtTMo0RzLxIwYtJp4FeE7ie5d7zRwMQp446LeKVlkoz89IjDF6IiCguFGSKLfd/Mmt4wMJXg078SLNFsM+LvNrIoPXKvCidOTwVavdwO21WJ00baZl56QkGL0REFBea28WMysLxmQGf17uDh0g2qZMKdk16DXTawB+pyQk6pJrFlVDVzQH2MAI8mRe9/3QY+WPwQkREcaHZJmY5Eg2BF8oadOK0UiQzL23K4KWTzEtSgg7pieI0V3VzZ5kXadooIeRj7I8YvBARUVxobu86eJEzL71pDteF1zcX4443dqGisfOmd54mddpOa14sRi3S3MFLbUuAzIvTATjdQQ1rXnqEfV6IiCjmuVyCPEWTaOwkeHFP29h8VvkE4x9bivHbd/YBENv9P3/d1IDntXaSeVGpAGkPyQSdBsnuZdx1LQGKiR2KHaUZvPQIMy9ERBTzWtwZDqCLaSNtaDIvbXYnfv/BAfnxZ99WoMXm8DuvorEdTe2e7QE0ak/mRe8VyKiQKvWgCbSM294qncmdpHuIwQsREcU8qd5Fp1HJQYovOXjpY81LfZsd7R0uaNUqaNUqOFwCGtu9Mya7TtRh+rLP5XGZ9FocrmiSn09P9A5CpO6/tYGCF+Uy6c62DyAvDF6IiCjmKetdOtsfSJo2cgmAow/ZFynLYjZo5Wt2OASvcz7aV+712KTXYNbIdADA0DQTnC7v8+Vpo0BbF0jLpPWcMuop1rwQEVHMa1YEFJ3RKzIyNocL2k5W/3SnxeaurTFo0WJ3oNXuhN3pXUfjGz4l6DW4eFIOkow6nJabjPkr1ns9n2IWMy//21uG5vatePzyici2ulcWcaVRrzHzQkREMa+7ZdKAd51JX6aOWmyeOhad3DvGO5PiuwJJr1FDo1Zh3pgMpJj1ft8/xeTpCLz+cBWWfXjQ8yS76/YagxciIop50rSRpZOVRgCgdQcQgHfRbnuHEx/vK5cDoO5Iq5rMBm2ny68rmzxLnu84d6TfVNaQFO8sijJ4AYCqJkXwY2fmpbcYvBARUcxrcO9X1FXmBQjcZfeR9/fjltd3YMk/d/Xoe7UqdoqWa158gpd6d+3Kqz8+A79cONrvGs8uPh1zRg3Cm7+YCcA/ePG6j6Qc4PQbgNEX9Wh8xJoXIiKKYXtP1qOktg3/KywDAGQnd52d0GvVaOtwwubw1Ki8sbUEAPDZt5U9+p7KFUS+wdCmozUYMcgsn2Mx6gJeY2RGIlb/5Ez5sVTzIjFoPZtMInsScOnTPRobiRi8EBFRzLr02Q3y12oVcPM5I7o8X25U14eal1ZFwa5O65mG2nKsBte8tBmZSQZ0OMUamK6msZR8M0ZNPZzCosA4bURERFHncLrw/Pqj2FNSLx9rtXt/wC8Ym4mhaV1vXCgFCdKKoWA0Kwp2lZmXT/ZXAAAqGm3y7tBdrX5S8q2JCdisjnqMmRciIoq6t3adwuMfiStwjj8u1n6U1LZ5nXPdjKHdXseaIE7P9CU4kIKmRINWXm3U4XT5LZeWzglGewi2MBjImHkhIqKoK6pu8Tt2qr7V6/Gs/PRur5Ps7mRb3+bfDE7XycaJvppt0n5FniZ1docLpfX+GzQGG7w4nEL3J1GnGLwQEVHUKWtHpO600r5BErW6++Aj2Z15aQjQydao0/gdC8RrtZEi81LT7L8jtKYHYwokVDtfD1QMXoiIKOqUGQypniSYqRWpDX99m/+0UUIPgpe6Fjve3V0KwHt7ALvDherm0NWpMPPSNwxeiIgo6joUH+ZS99r2jt5nJ6Sal0B7CBl03X/krdp4XP7arKh5sTlcqPbJvNx57shejW1cdpL8tW/fGOodBi9ERBR1yizLwXJxd+a2IDIvUmfb4hr/GhqduvuPPEHwBFFmvadJXX1rh9/y68tOH9Krsb1w3VScNSIVAKeN+orBCxERRZ1NEajsO9UAILhpo1GZFgDAofJmv+d6Up+SavZ0wnW6BDnzcqRSvJ5WcQ2pOLinclNNeOqHkwFw2qivuFSaiIiirl2R1ZCCl2AyLwWZiQCA6maxF0uSohC4J8GLWtGPJdGghcGdefm2vBEAcFpuMq6YNgQ2h0uur+kN5dJrCh4zL0REFHXKLMv+0kY4XQJsQdS8mPRa5KWKuzMfrmjymurR9mCptHIcM/LT5OXVxTXisu38QYm46ow8XD9jWK/HphyDwyV4TVFR7zB4ISKiqFMGDW0dThyrakabPbhGbqPc2Rff4EXTg5oXqUh48fQ8qFQqueZFMshiCGpMEinzAngXKVPvMHghIqKo811ZVHiqAe2OYIMXqe6lyWt3afQg0yF9T2m6SBlsAL2vc/GlbJTncHHqKFgMXoiIqFuCIGDdoUpUNfk3agsF3+LcyiZb0C30R2eJwYuYefFcw+HqPniRpqqkhna+mRdpKXawvDIvDmZegsXghYiIuvXO7lP48aptOO/P6/t0HYfThR8+txE3v7bd67hUsGtQNIVrDXLaqCDDk3lRZnScPQhepMyLUesOXnwyLylBFOkqKVcrdTDzEjSuNiIiom6tPSDuqFwfoPlbb2wtqsX24joAYjAhrQCSsixJCTpUNdlgd7hwotazt9Gan07v8fcYMcgMjVqFxnYHrv/bFvl4TzIv0jiM7oZ2vpmXvk4bqVQq6DQqdDgFrjjqA2ZeiIioW6FYGHOgtBGLX/YEE9IeQoCnz4s0LVPbapdX+Ox66DzMHNn9powSo06Dwclis7qKRs80V08yL9K0Uec1L33LvACA1l04zF4vwWPmhYiIuhWK4OWPnx7yetxmd2JPSQPWHihHg3sXaGmDxj0l9QCAHKsRKebeBwy+GROgh8GLNG2kCzxtJHXw7QudRoW2DnbZ7QsGL0RE1C0BfY9eGtu8p5xa7E786JUtXseSjGLmZX+p2BRuXE4SghGoH113wUuLzYGdJ+oBeJZE6xRBUGaSocc7U3dFCqw4bRQ8ThsREVG3+pp5cbkEHHLvWSRRThtJknxW8yg3M+wNZadcSVdLk8sa2vDjV7ehtsWOYWkmzBk1CACgVyxtHppqDmosvjht1HcMXoiIqFt9/Zj9rrIZTTYHTHqNPPUSqAmdsp0/AIzLsQb1/XLdXXaVusq8XPLMBmwtqgUA3HFuAbQa/4LdvDT/awZDpxUDIk4bBY/BCxERdauvmZddJ8QVRpOGWOWpoZZAwYtP5mWMu2dLby29dLzfsa5WG1U3ewp7L56cLX+tLNgdGiAgCoZUlFzXYg/J9QYiBi9ERNQDfYtedrqDl9PzUmDSi3UjbYGmjYzewUtmkjGo7zc4OUEu/pVImZc3d5zEtS9vloMH3z2GDFpPXYuyYDdUmZccq5h5Kq1vC8n1BiIGL0RE1K2+ZF6cLgGfuvvEnJ6XggR38NJi88+8KAMOvVYt91sJxn3njwYAXD9jKACgqd2Bj/eV4Zf/2YMNR2rw5Cfi6qfqZk8GZGZ+mtc1lAW7eSHKvOS4l3Gfqm8PyfUGIgYvRETULVcfopcP9paivrUDWrUKpw9NQaJBDFBaAmRelMGLSa+BKkDhbU8tnj4UX983D7fMyZeP3bZml/z1gdIGAMBD7+yTj710/TSvayizMkPTQlOwK/WgYeYleAxeiIioW32ZNJJWGZ03LhOpZr3cpbauxb9br0nvCV762s1Xo1YhN9Ukd/EFvIt2yxrEzMfH+8vlY2aD91STcgoppY/ddSVS5uW9PaV45ZsiFNe0hOS6Awn7vBARUbeUiRdBEHqVETnlzjBMGpIMALAmiE3nKpv8p02Uuy6HilEbuDdLZZMNgiAg0aBFs82BVTee4XfO+Jwk3L9oDEYMMvcpC6SUk+yp4/n9BweQmWQIWVZnoGDwQkRE3VJmXpwuAdoeBhnFNS14d3cpAM+HtpR5qWj0D14a2z1TSRdNzPZ7PhhWkw5/vfZ0mA1a/HjVVigXHe0vbUSzzQGdRoXZBf5bEKhUKvxibr7f8b6Qpo0kEwcHtxx8IGPwQkRE3VLWfjhcAjpJZvj5v8++k78e6244l5wgBS82v/PTFFsB/PGKycEMNaAL3YGQ72rpD/aWAQBGpCf67WMULumJBq/HoSoEHkgYvBARUa90OF09apN/rKoZ7+w+BQD40xWTMSpT7NnSVeZlZn4all4yDuMHW+VVSeH0wV4xKzQqyH4ywVD77F0QqumogYTBCxERdUu52qinbe0/2lcOlwCcM2oQLp86RD4u7cxc2eSfeVGpVLhx1vA+jrbnTtaJ9TijMxMj9j2VAu3BRN3jaiMiIupWh8MTsHR0sUeQUpU7OBnvs7licierdkI5TdRbUlYoUvIHiQW6F4aormegYeaFiIi6VdfqaeTW0cPMS737Nakmvdfx5ATvxwUZiXjr1pmwGEOzFDkY2daE7k8KoVd/fCbe3HkSN8wYFtHv218weCEiom7VKvbhsXX4d8YN+Bp3n5YUs0/w4pN5sSboohq4AOhTJ99g5KaasGTBqIh+z/6E00ZERNQll0vwyrx8/y8b4Opik0OJlHnxbe5m1GlgULTdtyZEN3AB0KMCZIodDF6IiKhLDW0dXkuMG9sd2HOyvsvXCIKAGveeQck+00YAkKI4Zg1R59q+MEQ480J9w38tIiLqUo17ysioU8vFt599WyE/v/dkPZ74+CBa3XsVtdodmPzIp3Jn3eHp/t1jlVNHkcy83HzOCADAve5NGyWGnjauoZjA4IWIiLokTRllJhnxs7PFD//Pv60EALR3OHHpsxvw3JdH8es3C7H9eC12n6iXO+WOybIg1eyfeVEGLL4FvOH06wvG4Kt75+HWufkYluZpDhfpmhfqGxbsEhFRl6Tpn1SzHnNHD4JGrcLB8iaU1LbC7vQsm35vTyne21Pq9drHfjAh4DWVmZchKZFb6aNSqZDnDlqyrEYcr2kFAOgj1F2XQoP/WkRE1KU6xZLnZJNezliU1LWisa3znZ9/tXAUpg5NDficMtuSnxGdBnHKqSJ2uY0vzLwQEVGXpGXS0vSP9KHf4RRgd3QevIzvYsNBZcZmxKDo7KjMqaL4xX85IqIB6m/fFOE3bxd6bboYiDxtlCgGLzr3MucOh0uubTlrhH+GZUJO58GLQ7F8KSlKPV5YpBu/mHkhIhqABEHAox8cAABcfvrgTqd3AO9pIwAwuOtDOpwuedoo0IqhQRaD3zHJvQtHo6S2FXfOHxncDYQAMy/xi8ELEdEA1GL3dMntbqPFGp9pI51WrA+xO11obBeDF9/siXIlTyB5aSa8c9us3g06xMwGfgTGK4adREQDULViR2d1N1sb1/kGL3LmRUBjmzhtlOSTefnXzTNCNtZw+cXcfGQlGXHbvPxoD4V6iWEnEdEAVN3sCV7au9mryLdgV6ecNgqQeUkx6ZCZZAzpeMMhw2LEpgfO5UqjOMTMCxHRACMIAj5zN5kDgDZ718FLTYsY6KSZxRoWqSeK3eGpeUlKiM/fhRm4xKeQBy8OhwO//e1vMXz4cCQkJGDEiBF49NFH4XJ5lsUJgoClS5ciJycHCQkJmDt3Lvbv3x/qoRARUQD/2laC59cflR+3dZF5abM70d4h/vxOMYvZFZ1G/MDvcLrQ0OafeWFAQOEW8uDliSeewPPPP49nn30W3377LZ588kk89dRTeOaZZ+RznnzySaxYsQLPPvsstm3bhqysLJx33nloamoK9XCIiMjH+sNVXo+7mjZ66N198teJ7gJXadpILNgVa16sCTpcfUYuAODu80aFdLxEvkKe59u0aRO+973v4aKLLgIADBs2DG+88Qa2b98OQMy6rFy5Eg8++CAuu+wyAMDq1auRmZmJNWvW4Oabbw71kIiISKGkTmyJb9Sp0d7hkjMrgfx3x0n5aymj4unzIqBJnjbS4bEfTMTPzhmBEQE2YiQKpZBnXmbPno3PP/8chw8fBgDs2bMH33zzDS688EIAQFFREcrLy7Fw4UL5NQaDAXPmzMHGjRtDPRwiIlIorW/DvlONAICCDAuArqeNAtErCnYbFDUvGrUK+YMSOW1EYRfyzMv999+PhoYGjBkzBhqNBk6nE4899hiuueYaAEB5eTkAIDMz0+t1mZmZKC4uDnhNm80Gm81TGd/Y2BjqYRMRDQiffVshfz0my4LCUw1+BbuN7R3Qa9Qw6gJ3oNW7My9F1S2oabFDo1YhJzlymysShTzz8q9//Quvv/461qxZg507d2L16tX44x//iNWrV3ud5xuZC4LQabS+fPlyWK1W+U9ubm6oh01ENCBIgcr8MRnyzs5Hq5rx09XbsO14Lepb7Zi09FNc9teNXtsG/OisPPlrqWB37QExEJqalxK1Fv80MIU883Lvvffi17/+Na6++moAwMSJE1FcXIzly5fjhhtuQFZWFgAxA5OdnS2/rrKy0i8bI3nggQdwzz33yI8bGxsZwBARBcHuEOtbMpIMSDSIAccHe8sAAHtONuCBC8YAAA6UNXrVwjxwwVj5a2XBLgDMHTMo/AMnUgh55qW1tRVqtfdlNRqNvFR6+PDhyMrKwtq1a+Xn7XY71q9fj5kzZwa8psFgQFJSktcfIiLqPSng0GvUyE72biTX2NaBkto2+fGBMnGKXqUCEhRTSFLwIpk7KiNcwyUKKOSZl0suuQSPPfYY8vLyMH78eOzatQsrVqzAT37yEwDidNGSJUuwbNkyFBQUoKCgAMuWLYPJZMLixYtDPRwiIlKwuTMveq0aQ3zqVGwOFw5VeGoKL39OXERh0mm8thDQK4KXNLMeY7Mt4RwykZ+QBy/PPPMMHnroIdx6662orKxETk4Obr75Zvzud7+Tz7nvvvvQ1taGW2+9FXV1dZg+fTo+/fRTWCz8H4CIKJykaSODVoPBKf5Fth8Wlvsd893A0Kj3ZGGGpZu5uogiLuTBi8ViwcqVK7Fy5cpOz1GpVFi6dCmWLl0a6m9PRERdUGZe8lJNmD8mA58frOzyNYk+wcugRIP8dbY19vcwov6HexsREQ0gdkXwolKp8PIN0/Ds4ildvsZk8F4ynZnkCV4Gc4k0RQGDFyKiAcTmEJdKS3UrKpUKQ1JMXb7GrPfOvCh3jC7I5HQ/RR6DFyKiAUSZeZEkdNKMTuI3bWTxZF5m5qeFcHREPROfe5gTEVFQpKXSBkXwYtR1/Xusb8GuTqPGO7fNQofTxc66FBUMXoiIBpBgMi9mg//zp+Umh3RcRL3BaSMiogHEs1Ta8+PfoO0meNHz91yKLQxeiIgGEFuAzEuiUQuTXgOdRoW9Sxf6vcZ32ogo2viOJCIaQORpI40n26JRq7D9twsgCIEDFYuRHxUUW/iOJCIaQJptDgD+vVtMiqmh339/Ap7/8igumJCFkrpWLJqQFdExEnWHwQsR0QDS2NYBAEhO0HV6znVnDcV1Zw2N1JCIeo01L0REA0SH04Umd+Yl2aSP8miIgsfMCxFRP1fe0I5rXtqM+la7fCyJdSwUx/juJSLq57YX16KoukV+bDFoodUw8U7xi+9eIqJ+rs3u9HpsNXVe70IUDxi8EBH1c+3u5dGS+WMyojQSotDgtBERUT9n6xAzLxdOzMIfr5jstSyaKB4x80JE1M+1u4OXJKOOgQv1CwxeiIj6KadLAAC0d4jTRsZuNmAkihcMwYmI+qHaFjsW/nk95ozKQLK7QNeg4++r1D/wnUxE1A/9Z3sJqpvteHPnSXnayNjN7tFE8YLBCxFRP9SqWB795aEqAJw2ov6DwQsRUT9kUyyPPlXfBgAwctqI+gm+k4mI+qHqZpvfsQRmXqifYPBCRNQPHa5o8jvGaSPqLxi8EBH1Mw6nCwfL/YMXs4ELTKl/YPBCRNTPHKtugd3hglmvQY7VKB/nTtLUX/CdTETUzxwobQQAjM1OgtmgRWlDOwDAYuSGjNQ/MHghIupnDpSJwcu4nCQIgue4hZkX6if4TiYi6ke2H6/Fi18dAwCMy05CTYtdfi6JmRfqJ1jzQkTUT9S12HH1i5vlx+NykuStAQAgkZkX6if4Tg7A5RKgVquiPQwiol6parbB4fLME43KtKDZ5pAfa/hzjfoJBi8KB0ob8fB7+6CCCv++ZUa0h0NE1CvKLQEAsa/LjBFp+MXcfAxPN0dpVEShx+BFIdmkw7bjdVCrgMb2Ds4PE1FcaVVkWd78hfgLmEqlwv2LxkRrSERhwZoXhZzkBAxPN8MlAMs/PBjt4RAR9YqUeZmcm4ypQ1OjPBqi8GHw4mNstgUA8MbWE/jiYEWUR0NE1HMtdjHzYuI2ANTPMXjxYTF4poqWvncATe0dURwNEVHPtbkzL2YDgxfq3xi8+OhwebaRP1HbiolLP8VbO09GcURERD3T4g5eEvQsZ6T+jcGLj6wko98xqeETEVEs+2BvKQDAoOWPdurfGJ77uHlOPr6rbMbUoSl4Z9cpHCxvQnFNKzqcLug0/IFARLFp54k67DpRD4CddKn/46exD2uCDi9dPw23zMnHe7fPhsWoRVuHEz/7+3a02h3dX4CIKMJsDid+9e898uMfTBkcxdEQhR+Dly7otWo8fc0UGHVqfHmoCr//4NtoD4mIyM+O43U4Vt2CVLMee363EBOHWKM9JKKwYvDSjXmjM/DkDycDAHYW10V5NERE/vaVNgAAzhqRCquJU0bU/zF46YFx7t4vp+rbICj3lyciigFHKpsBAGOykqI8EqLIYPDSA4OTTQCAZpsDjW2seyGi2FLXKvajSkvUR3kkRJHB4KUHEvQapJnFHwon61ujPBoiIm8N7uDFmsApIxoYGLz00OCUBADAybq2KI+EiMhbQ5sYvCQnMPNCAwODlx4anCwGL6cYvBBRjKlvswNg5oUGDgYvPZSbKta9bC2qjfJIiIi8yZkXrjSiAYLBSw9dOjkHAPDJgXLUtdijPBoiIpHN4UR7h7gnGzvr0kDB4KWHJgy2YnSmBYIAfPVdVbSHQ0QEAGi3ezaTNXE3aRogGLz0wtzRgwAA6w8xeCGinitraMOLXx1Fe4cz5Nduc19Tq1Zx/zUaMPhO74U5UvByuAouF5vVEVH3BEHAJc9swLIPD+Llr0O/Q70UvBh1zLrQwMHgpRemDU2FWa9BTYtdbsdNRKTkcglev9x8eqAC1c02AMDaAxUh/37tDF5oANJGewDxRK9VY9bIdHx6oAL/3XESk4YkR3tIRBRDXC4B3/vLBhSeasCi8VkwGTT4394y+fmaMBT7S5mXBD1/F6WBg+/2XrrYveron1tL0GLjVgFE5HGqvg2Fp8Ss7Mf7y/HWzlOwOTwFtdXNtpDvj9ZudwcvzLzQAMLgpZcumZQNlQqwO11yKpiIgldS24prXtyMm17dFpZplUg6URt4+5AfzxoGAGjvcKHFHtqiXTnzwuCFBhAGL72kUqmQYxW77YYjBUzU3wmCgL+sO4IPC8XplH9tK8GmYzX4/GAlfvb37fjz2sNxu3v78ZqWgMfPKRgEk14MLv61rQSXPPMNXt9cHJLvyYJdGogYvARB2rm1tpnBC1FvHapowlOfHMKt/9gJu8OFz771zrb83+ffYVdJfXQG10fFNYEzLyMzEpGeaAAA/P6DAyg81YDfvrMPTe0dff6ebdK0kZ7BCw0cDF6CkOreYbqWmReiXlPWiq3ZUoyD5U1+51Q3xeeUbFF14MzL4OQETM5N9jve2TRTb7Rz2ogGIK42CoIUvFSx5oWox3YU1+FvG4owp2CQfOybI9UAgGlDU7C9uE4+7ojDPkqVTe04WtUsP75hxlCMzU5CptUItVqFO84diU/2lcPu9BTwtoWg/oU1LzQQMXgJwoh0MwDgSGVzN2cS9U+CIOBYdQuGp5mhVqt69JorX9gEp0vwWjp80r1Le2aS0evcUEynRNK/t5fgvv/ulR8/eOFY/OisoV5TOaMyLdjw63PxzBff4e+bxHqXrop3q5ttOFnXhtMCZGyUpH2NjJw2ogGE00ZBKMi0AAAOV/inu4kGglUbjmP+n9bjz58d7vFrnAGyKafqxeBFqiOT1LbEdvDS0NqBLw5WwOF0YdPRGq/AZWZ+Gn52zoiANSiDLAY8+r0JmDY0BQDQZu+83cI9/96D7/9lAz4qLOv0HEBRsKtl8EIDB4OXIIx2By9HKpuxakMRZj3+Radz3UT90aMfHAAAPPPFkT5dp6ld/PBOMxtgMXoSwW9sPYEtx2r6dO1wumHVVvzk1e24/81CXPPSZq/nFk3I6vb1JoN4ry22zjMvXx0W91B75P0DXV7LU7DLH+c0cPDdHoTcVBMMWjVsDhceef8ATtW34dH390d7WEQxq7ulz6lmHQoyEuXHJ2pbsfjlLTjRyeqdaDlS2YzKpnbsdq+GenPnSb9zzh/fg+DFXZ9S39aBv31ThBM1rXht03Ecd/8SdKTSk9Utb2zvcnEAC3ZpIGLNSxA0ahVGZiRif2mjfEz6DZJoIOlhuQuau+lGnWjU4skfTsJlf92Ia6bn4YM9ZThV34aSulbkpZlCMNK+K61vw4IV6wM+99L10/C/vaWYnJvsV78TiMkgBhq/d2ewpExWmlmPHQ+dhwUrvvI6//09pbhh5rCA12KfFxqIGLwEaXSmxSt46YjD1RFEwZB+0wc8K+/qW+1I0Gtg6KTuorKbpc+JBh1GZliw5+GFUKlU2HG8Dqfq29DYFju1LzsUq6GU0hP1OG9cJs4bl9nja5k6Ka6tabGjIcA9K1cx+WKfFxqIOG0UJKloV+JQLH8kiraD5Y1eQUYonazzTOVo1CpUNrXjtEfX4qKnv+n0NVXu4GVEuhlP/nASLAbv35sS3Y9VKjGVk5SgAxBbGc3O/nv2JNPiy6zv/PfGyY986nfMt5t3TbMN1/9tK97dfYpLpWlAYvASpFGZiV6PHU5mXij6appt+OW/92DRyq8x/0/rUdnUHvLvUVTtCV4a2xxYf0gsLO2qdYCUeRlkMeDKabm4c36B1/OJPsFMkrt4tzGGlky3OwL/ghJM8CIFZ925aGI2APHfVenVjcfx1eEq3PXP3ax5oQGJwUuQRvlkXjpcLnxYWIZDAbqFEkXKLa/vkItIT9W3YebyL7DzRODpjmAVK/bvaetwdpkdsTtcuOqFTbjzjV0AxOAFAIw67x89iUbv4MViFD/cY2naqLOxBBO8jMtO6vacxy+biMXT8wAAm4/V4jXFXkjK5nZSrxz2eaGBhMFLkAYnJ3g9PlbVglv/sRPnr/wK33xXHaVR0UC37bh3oOJwCdh3qiGk38N380FlPYvLp/bru8ombCmqlR/PGpkOADD4ZAnMBu/HSQlS5sWB3SX1+PGqrVH/xaCqk7qdzCRDr681Pqfr4GX1T87E1WfmefW/eeidfThZ14rr/7YVL39TJB8vaxCza+zzQgMJg5cgqdUqXHNmXsDn7vvvnrjdFZfil2/gIOlupU9vCIKAncX1XseUU1M2n6mVsnrPcw9cMAZXTssF4L8yxmLwnkZJkjIv7R34/l82YN2hKryw/mifx98XnRXN6jS9/zGakWSEqouVWonuYK4gw4KRiiXkr28+Ifd/UUrQafymson6s7AEL6dOncKPfvQjpKWlwWQy4bTTTsOOHTvk5wVBwNKlS5GTk4OEhATMnTsX+/fHX5+UeaMHBTxe2tCOzcdqAz5HFC5St1pfzSEsei2uacWBskboNCroteKPD2VGwreo9ad/3w5A3Lvo5jn50LjXVp+el4wM9xTSsDST3zSSVBPy1s5T8jHfwCjSjnZS02MxBrdoc1iaudPnTO6CXo1ahU+XnAOr+7/HuoOVAc+/a0EB0hJ7nwEiilchD17q6uowa9Ys6HQ6fPTRRzhw4AD+9Kc/ITk5WT7nySefxIoVK/Dss89i27ZtyMrKwnnnnYempviqF5kzehAm5yYj2eRffPePLcUBXkEUPr67M+eliv1RWkKYeZECpKFpZnmPr+8qPB/q7Q5P8KJcgefyyUQOSTFh8wPz8dk9c/Du7bPlVUYSKfOipOlpU5kwaLE5UNrgXfz88CXjcHZBOn44dUhQ1xzaRf8a5VJqtVol7290yL0lyW8vGou1d5+DybnJmJmfhptmDw9qDETxKuTByxNPPIHc3FysWrUKZ555JoYNG4b58+cjPz8fgJh1WblyJR588EFcdtllmDBhAlavXo3W1lasWbMm1MMJK4NWg3dvm4Utv5mP/EFmDE8349UfnwEA+GBvGeY+tS4ku8YS9YRyr60Jg5PkviPNXbSg761y9wd4VpJRzo6UN3o+1KVNAgGxe6zk99+f4HcttbvZozXAyptA2YxQBmG9IQgCVqz17OFk1Kmx6sdn4MezhuO1m6bLWZLemj48Tf56tM8CAN9rDvMJdAoyLSjItODd22Zhzc/OCmrqiiiehfwd/95772HatGm44oorkJGRgSlTpuCll16Sny8qKkJ5eTkWLlwoHzMYDJgzZw42btwY8Jo2mw2NjY1ef2KJQavB2rvnYO3d52Du6Ax507XjNa3Ye7I+uoOjAUPKvNy/aAw+uONs+QOv2Ra6FTtSoJJlNQbMjiinjercvUmsCTqMz7H26vsolxJLtRyhrN3pjatf3IxX3AWy04enYvfvFmLe6Iw+X/cns4fhmjNz8dy1p+OTu8/BX689XX7Ot4B5qM8Uk7IOhmggCnnwcuzYMTz33HMoKCjAJ598gltuuQV33nkn/v73vwMAysvLAQCZmd7dKDMzM+XnfC1fvhxWq1X+k5ubG+ph95larYLW/duP8odQcYztzUL9l9RnZXSW+MEmLT/uavO/3lJmXlLNgYMXqVhd2o9H6sLbG0mKzMv3ThsMAGjpYgfmcGlo6/BaLTU0zRSyNvwGrQbLL5uEC9y9XJIVAZvvyqFh6Z7MS6JBixxr75dnE/UnId8ewOVyYdq0aVi2bBkAYMqUKdi/fz+ee+45XH/99fJ5vnPcgiD4HZM88MADuOeee+THjY2NMRnASDKSjLjurKF4bXMximq42zRFhtSHJNUsFm5KXVybQpixkDIvmT4fntJGpXf9czfaOpy4/PQhmDxEzLakBKgJ645FkdUZkiK2JQhlENZTFY3edS6dbX8QCqcPTcGQlAQMTk6A2qe+R5l5GZpm6vRnJdFAEfLgJTs7G+PGjfM6NnbsWLz55psAgKwsccfV8vJyZGdny+dUVlb6ZWMkBoMBBkN8VdIPdxczHutiTxKiUPLttJruXs1T1Ri6LrvSh3lWkhG1zZ6W9YOTE3CsugUnasVM4/OKZc2+Ux49oSyCl15fVN2CZpvDrxtvOFU2evd2mdvJCsNQMOo0WPerudAECExyUzyZFy3rW4hCP200a9YsHDp0yOvY4cOHMXToUADA8OHDkZWVhbVr18rP2+12rF+/HjNnzgz1cKJGmpPuqmU6USi1y7sLi/9bD3E3UixtaEdHH/fe+seWYvxz6wmvaSPlZ2xZQ+AASaNW4Rdz83v9/XQaNbb+Zj62/GY+clM8DSE/2eeZWj5V34Y5T63Dkx8f7PX1e0rqYZObmoDnf3Q6zh3T91qXrug0ar+sCwB5WToA2MK0ZxVRPAl58HL33Xdj8+bNWLZsGY4cOYI1a9bgxRdfxG233QZAnC5asmQJli1bhrfffhv79u3DjTfeCJPJhMWLF4d6OFEjBS/Ha1phj3J/Cur/BEHw26AvXdH34+H3gu+j1Gxz4MG39+HXbxXK3XQzrQZcdUYuEg1a/GDKYDx08biAr736jFy/rTR6KiPJiMwkI9ISDfKyYmU3368PV6G4phV//fIovqsIT5uFCnfmZWpeChZNyI6J6Zpg+8oQ9Sch/7/gjDPOwNtvv40HHngAjz76KIYPH46VK1fi2muvlc+577770NbWhltvvRV1dXWYPn06Pv30U1gswf2Qi0XZViPMeg1a7E4U17T47UJNFEodTgFSg12p9b7yN/jdJ+qDvnaTz+aIWrUK6WYD1GoVtv92AQxaNVQqFSYNsSI31eS1K/I9540K+vsqnT8+Cy9+dcxrg0Jlc7wtRbVh+X9MmgaTeuZE04orJ2PF2sNYeun4aA+FKOrCMnl68cUXo7CwEO3t7fj222/xs5/9zOt5lUqFpUuXoqysDO3t7Vi/fj0mTPDvAxHPVCoVp44oYtoUUwnK3YWl4CEjiP13JL79VTIsBjkwMuo0cjZiwmCrV8+Wm+eMCFnX1zT3iqWaFk+dTYViW4IdxZ49nd7edRLPfvFdSL6vtAllMHU7oXbZ6UPwzf3n9nrZOVF/xMqvMBqZIf4myOCFwk2qd1GrAJ3Gk3GRAujWPqzU8W1y197NNOicUWJR6w0zhgX9PX1JU2DV7syLzeHE65tPyM9vLxaXM7tcAu7+1x788dPDKDzZ9w0ppVYHyqXKRBR9nDwNI+mD4zsGLxRmypVGyroMqc18Xxq8tfq89pJJ2Z2cKXrlhmlosTsDds4NlrS7ckVjO+777x6/jrIltW14e9dJr/qysoY2TBwSfJaivcOJ0gZxO4Su9iEioshj8BJGUvDS2W60RKEiF+vqvfuQmN3LilvdDd4cThdK69thNmgCTunsLqnHnpJ6XHVGrtyMTQp8puQl46kfTkZuaoLf65S0GjWsCaFN6kqZl8MVzTis2EtpRLoZeq0aB8ubcPe/9ni9prMVUD1VUtsKQQAsBm1QjfaIKHwYvISRVOTX2W6/RKEi7aHl20RNalTXbHOivcOJ81d+heKaViToNPjiV3Nw9792o8XmxH9/MQMGrQb3/3cvDlU04avDVXjlRnGfLqmzbaJBG7W29FLmxdekIVYMTTP7bUoJ9P2XhuPuKaOh6WwKRxRrWPMSRjnJYhfS+taOqG0qR/2XIAh45P39WLPlhBy8+GdexMetdgdO1LbKNRxtHU48+fEhbD5Wi8JTDXJ9iLRr8ecHK+VaLanmxRzkBoSh0FnmY2iaGUsWFCBQbLHpaE2fvqdUrMspI6LYw+AljCxGndyToayB2RcKrU1Ha7Bqw3H85u1CfLxfbN6W7dO23zNt5JQ3SpS8veuU/PW243W4841dXs8/416xIy2VNkews60vg1bj1d/kjGEpuGt+AX4yazhUKhXev302Lpmcg1/Mzcei8WIX7+8qm+WmesEoqmbwQhSrGLyEWWaS+GGibK5FFArVimDk75uKAQDJJu8MhcWolT/0r3pxc6fXeuLjg3hvT6nXsQ8Ly1DXYkeFOwDI7MNy61BQNt27Ymou7j5vFKzubQQmDLbimWum4P5FY/D8dVMxyV2ou+FIda++x1s7T+Lf20oAACfrxF84YqHHCxF5Y/ASZmZ3Gr8vS1WJAnEEaPk/Jsu7UZtBq8H/XX0aLIqsifLrQRb/gMRi1KIgIxEdTgHbi+tQ6g5espO7LtQNtzTF1FFyN5s9zh6ZDgD4xid4EQQBz315FK9tOu73mla7A/f8ew/ue3MvalvsqGkRf+EI9N+IiKKLwUuYmdx1Aq3cj4RCzCG11HVLNGjx41nD/M47d0wmTstLlh+PzrJg7uhBSDRoseLKyX7nG7RqZLmnn5a+tx+l7oLzHJ8pqUhTZl5Suln9M7vAE7wIgue/0/biOjzx8UE89O5+eXm5RNmxt7bFhrqWjh59LyKKPK42CjO5YJIFuxRivpstPrt4ihws+zIpCnm/q2zGnocXyo8nD7Fij6Khm06jlotzT9W3yavlhqREd/pEueIopZvMy9ShKTDq1KhqsuFwRTNGuzNSHyimxupbO5Bl9fx3URb4VjV5Mi+pJgYvRLGGmZcwkz5MWuzMvFBoNbd7B8Qj0jtfxuxwerIPLsE7Y7P00vG4VbHzs0GrhlbjvXxHr1FjxKDoFq6mmPQBvw7EoNXgzOFpAID/FZYBEIO99/eWyefU+hQw//qtQvnrkrpWtHeIwWFqJ8u0iSh6GLyEGTMvFC5NPsHL4JTOa1LsiiyN7+um5KXgvkVj5Md6rdovwJkwOMmvq22kLZogriJKNet71L133mhxm4LVG49j49FqFDz4kVfAUlTdAkEQ0Gp34H2fYuXD7r4xeo1arlsjotjB4CXMmHmhcPHdM0uj7ryRmrJt/oKxGQHP+cms4QCABy4ciw5FpmZEuhnLL5vUl6GGxITBVqy9+xz855YZ0PYgkFo8PQ8A0NDWgdc3F/s9f9uanbh9zS68tqkYd/gsE9/onkIaZDGwQR1RDGLwEmbyaiN77GdejlQ2Y/XG414fdBSbBEHAFwcr5cfKHiiBKHcifuLywIHIQxePxbYHF2De6Aw4FcXAHy05W64ZibaCTAvyB/Wsy69Bq5FrfUpqxbqd9EQDzh3jCd7+V1iG5R8d9HvtgbJGANFfHk5EgbFgN8xM7mWpLXGwVHrBivUAAKdLwE9mD4/yaKgrNofLayrItzmdr7vPK4BGDVw6eXDAPY0AQKVSycuClcXAvlsOxBOzQYtWuxMldWJn4YcuHovj1a1egV9XpD5NRBRbmHkJswT35nZtHbGfeZHsLqmP9hCoG60+05ATByd3eb7FqMODF43r8S7LC8ZmAvBenhyPpJ429a3isudkkx5nDE/xPO+TsXry8klefXAYvBDFJgYvYWbUif+J2ztcOFrVjB3FtVEeUfdMLFCMecppyHmjB+Ghi8eG9PrXTs/Ds4un4H93zg7pdSPNd0uDFJMOZw1Pw4h0M9QqYMWVp8nPPf+j03HlGblehc+XnT44UkMlol7gtFGYGd2Zl/YOJ27421aUNbRjy2/mx9xvtMqNI30396PYI23EmGzSYdWPzwz59bUaNS6elBPy60aatNpPkmLSQ61W4Z3bZ6GysR2DLJ7MisH9/6pe6/mdbtKQ5IiMk4h6h5mXMJPqBVpsDpysa4PTJfRps7hwUe699MbWE1EcCfVEm7s7rEnHQLMriQbPkmqjTo0c9xYHSUYdRmZYkKScNnLXKN/krveSthggotjDzEuYSdNGZYqApdG9S28sqWz0jK+9w4VvyxoxNjspiiOirkg1L0ZmybqkrGkZnWnxW06uUqlw8aRsHChtxIx8sand904bjKwkI8Zk8f1PFKsYvISZNG1U1ezJbDS2xV7xboXPrtdt3IsppknTRqxP6ppyA8fOgpFnF58OQRC8+rlMH5EW9rERUfAYvISZFLwoG5Y2xXjmBQA0bMwVs/6y7gg+/7YCAGDS8X/hrihry8Zmd96rho3oiOILf/KFmTRtpOTbnj0WVPpkXnx3LKbYcKyqGU99ckh+zOLqrqUpdoQew2lQon6DBbthZgzQ4CsmgxefzIuTwUtM2n68zutxXmp0d3qOdcqVQ2NZw0LUbzB4CTNjgNUgsViwW9Hok3lxcouAWLT1uHefoNvPHRmlkcSHISme4M5q6n4zRyKKD5w2CrPA00axF7xUNnlnXjhtFJt2FHsyL0/9cBI7wHbjjGEpeOLyiRjNrAtRv8LgJcwCZV5ictrIXfNi1mvQYndy2igGtXc4cbymBQDw6d3nYFRmbGyWGMtUKhWuOiMv2sMgohDjtFGYGbT+/4ljbdrI5RLkgCo9wMZ8FBuOVbVAEABrgg4FGT3bWZmIqD9i8BJmKpXKL4CJtcyLzeEJVBLde8Ew8xJ7jlQ1AwAKMhK5tJeIBjROG0WAUafxChBiLXhRNqSTNrJjzUtsaGjtwMXPfo2S2jb52EhmXYhogGPmJQJ8i3Yb22Jr2qjdHbzoNWo5S+RwcdooFnxzpNorcAEYvBARMXiJAN+i3VjNvBh1amjde784nMy8xIKj7qkipXwGL0Q0wDF4iQDfRnV2p0vOdsSCdjl40UCjljIvDF5iwaHyJr9jIwcxeCGigY3BSwRI00bpiQZIdZaxtOKovUOcIjLqNJ7MC4OXmHCowj94GZycEIWREBHFDgYvEWBwTxulJ+rl1TyxNHUkZV4SdBpoNWLw0tjWgZe+OoaS2tZoDm1AszmcKKpu8TuuVnOlERENbAxeIkCqeUkx6ZFkFFuUx2Lwoqx5eeqTQ3jsw29x4dNfR3NoA8LJulY8v/4o2uzeU4lHKpvhdAmwJrCtPRGREpdKR4DRvYIn1axHXasdQGytOGoLUPMiiaUgqz8SBAGzn1gHQMx83TBzmPycVO8yOsuCa6fn4Zf/3oM/XTk5GsMkIoopDF4iQM68mHVIaorFzIun5kWn4ZREJHQ4XbhjzS6vZfS+U3Sn6sQl0sPTzPjeaYNx/visgNtNEBENNAxeIsBsED9w0swGWIziBoixtDmjlAVKNGihYT1FRLy/pxQf7y/3Opag9w5Mat1ZutREPYDA+2QREQ1EDF4i4MppuahqsuP7UwbLfTta7bGzVLqsQfwNP9tq5J5GESAIAu759x6/477viboWd/Bi0kdkXERE8YIFuxEwJS8FL98wDcPTzTC5f7tui6E+L6UNYjYoOzkBWo3/W+KON3ZFekj9WpPNM2W4aHyW/HWr3XsqsbZVzIilmBm8EBEpMXiJMJNeTHb5flBFU2m9mHnJsRrRYvMf1/t7SiEI7PsSKlVNNgCAxaDF89dNxX2LRgPoIvNi5mojIiIlBi8RJtU1hGPa6C/rjuDnf98ORy+nfqqbxQ/TjCQD9pc2dnKOvc/jI1Flo/jfe1CSAQDkpdC+74n6Nrv7eWZeiIiUGLxEmMlddOnb0yMUnvrkED49UIHPD1b26nX1Le7pCZMeCZ0UhZ6qbwt4nHqvyh0sDkoUgxd5KtHu9CrkbrWJ7xGpsSEREYkYvERYuDIvymmd3uyb1OF0yTUYKSY9Hvne+IDnSct2qe8a3KuIUtyFuNJU4jdHqjFx6af4eF8ZAKDFPbVo0nOVERGREoOXCAtX8NKiuF5vljtLTfPUKiApQYex2Um4aGK233m3rdmJ4wFa1VPvSf/2JvcSet/g5OnPj8DpEuT+OwxeiIi8MXiJMM9qo9AW7Co79jp7sKnizhN1eOT9/SipFTMq1gSdHPRkW40BX/PjV7eFYKTUpthLCgDG51gxJMWz2WJ+RqLXajQpM0NERCIGLxGWoBM/iEJd86Ls2Ntqd3YbwNz6+k6s2nAcN/xtKwDv5bjZnexaHGiTQOo96d9eCmRTzXp8+au5mJmfJh7XadDqnspTqeDVhZeIiBi8RJz0gbXzRD1e+aYoZNdtVBR6PvBWIc547DNUNrXj433lKHf3cVEqbxSPNSvqXSQ5nWReKDSkaaMERUZFq1FjwdhMAGKtizy1pNNApWLXYyIiJQYvEaasX/j9BwfQEKINGn03eqxtsWPG8i9wy+s7cPe/dns9F6hnS4rJ00uks8wLhYYcvPis7JK2kfhgbxnK3AGniSuNiIj8MHiJMN/9a+yO0LTjD7TRozR1tOlYDQBxuqLV7ggYMHWWefGtf2Gzur6TVoP5FuIqMzHXvLQZgKehHREReTB4iTDf4ktXD4OB3SX1+Nnft3e64qexm40eW2wOzHlqHcb97hM8+v4Bv+eVNS/p7v4jAHDX/AKv82whCLaabQ6c/+evsOzDb/t8rXgkdVf2DWTNAVYVBTpGRDTQMXiJMN/ftnuaeXnw7UKsPVCB+SvWB3zed9rI196TDah0/xb/1q5Tfs8rMy9qxVLrBL0GG359rvw4UIZH0pNVTgDwwZ5SHKpowotfHevR+f2FIAj489rDWHeoCoD/tJFOsa+UxaiFxaDFGz8/K6JjJCKKB5xQjzDf37Z7msmQdnt2ugRUNdkwyGLwer6roALofqXQ2QXpXo+fu/Z0bDhajQsnZkOnUcNi0KLJ5sDhiia/7w0Ae0/WY/FLW7BkQQF+evaILr+XMsSxOZwwaAdGduHr76rxf59/Jz/2DWSV+0rt+d1CryCSiIg8mHmJMJMuuMzLiPRE+ev/7Cjxe76zaaPB7uLbY1XNnV57zqhBmDDY6nXsgonZ+MP3J8rZAKkL77Uvb0F1sw2rNx7Hsg+/xbpD4lYET31yCM02B/7wv+6ngpQfydI+PwPBV4er5K/zUk2YOMT7v/nM/HSY9BpMyUtm4EJE1AVmXiJMq/GOF+093ERRed4bW0/glnPyoVar4HQJeHvXKRyr8s+s/PPnZ2HlZ4dxqr4tYObl3vNHY/vxWqy8akqv7mHG8s/R4RTksex9eCHMiloeQRC6XN6rDLTKG9uRm2rq1fePRSfrWpGZZPSa+vF1qKIJALD8som45sw8v+etJh22Prig0/2liIhIxOAlyjp6GLzYHJ6mdiW1bThR24ph6Wa8s+sUfvWfPQFfM3GwFRajuAT6iE/mZXByAm6bNzLIMYuBi1olTldVNduQkeSZSnrlmyJYE3Q4b1wmkhW1NGUNbdh4pAY1LZ4dqgP1oIk3/9p2Ave/WYhfzM3H/YvGdHqeFEAWZCR2eg43YSQi6h5/UkZZT6eNfM+ra7VjGMzYeaKu09fotWokuYOX4ppWr+cm+kwT9daZw1NRWt+Gk3VtOFHT6pVxkKaO9Fo1/vHT6ehwuDBzZDoWv7TFLwNU0RjfwYvTJeD+NwsBAM99eRQWoxa3zg0cFEo1LdYEXcDniYioZ1jzEmXBBi/17tVFXU0xaNUqWIze8emQlASkJxrw24vH9mqcSy8Z5/U4PVGPPPd0z4naVq+9eJRjvuL5TVj88hYcKG0MOHUV75mXDwvLvB4/+fEhuDpZdSX9NzJyWoiIqE8YvERZT1cb+Z7X0CoGL7WKKRgAGJ5ulr9WqfyDl78sPh3bHpyPISm9qzO5cdZwHHnsAvlxaX27d/DSzV5N/yssDXi8PM4zL+/u9l92XtHkf08u7hJNRBQyDF6irMcFu+7gJcO9TLm+VQxaals9wctt8/KRpmg2B/j/lj9ikDnovXK0GrW8emnWyDS50PZETffBy6HyJp9xiW+9vScb8NbOk51mK2KZIAjYc7LB7/jx6la/Y8rg03e5PBER9Q6DlyjrybSRw+nCMfeUS2aS2K5fmjaqbhaXGr98/TTce/4YJJu86ymU00qDLAa5gDdY794+Cw9dPA63zyvA0DQxeNlX2oCP95cDABaMzQj4usMVzdAr6mLuPV8sbD1R24p7/r0Hn7hfH0/KG9sDtu8vrvGfHpO66gKAcYD0tSEiChcGL1HWk+Bl1Ybj8tdSg7iGtg44nC58VyGuIhoxSJwusiZ4Z16Uv+Urp5SClZ5owE2zhyNBr8Gs/HToNWocrvCsZPr+lMH47UVjcevcfBQtvxDLL5sIQCwwlrZCeOG6qVg4LtPruhuOVvd5bJG21511SfEJGI/X+GdepHoXg1bNHi5ERH3E1UZRoFIB0pZGdkfX0y0A8ObOk/LXUhv/FpsDx2taYHO4YNZrMCxNDEx8My/S9AwA5A/qe/CilGLWY0hKgpwVAsQaHGWH3XPHiJkYZQfg2SPTodV4f4D7roaKB9+5+7acOTwVn+yvkI8HyrxImzFyyoiIqO+YeYkCndrzn70nNS/KgCTVLH7dYnOiqkmsd8myGuXf5i+dnAMAyE0Va1OU00ahyLz48i0IPqdgkNdjs0/fEpVKLFg1aDX40Vl58jg3HKlGZZwV70pdhwcnexc/B8y82MV/ZzagIyLqO2ZeokCjVgHuhIu0AqUryYqpIGn352abQ66jUDY2m5ybjE/vPgdZVrE2Rlmwm2VN6PPYfSUqgpffXjQWw3wCJN/tEEYOSpQLhv/wfXFKaeGf1+NwRTP2lzYiw13TEw9abeI/YqJRixeum4r9pxrw9BdHUFzTAkEQYHO4sOSfuzElLxlT8lIAMHghIgoFZl6iQDllEqg/ii9lU7N0s1jz0mJzoMW9wsek945BR2Va5OZ0yg/LVJN3PUwoKAOn2T6bOwLiDtVmxVTJJe7MkNJId8fZo1XNKDzZ4FXcGgsEQcCWYzV+xbkt7nGa9RqcPz4Lt59bALUKaLU7sfFoDX77zj58vL8cyz86KN8Te7wQEfUdg5coWDDWU6za3RLjr7+rwp6T9QDEtvJSRqXZ5kCre9rCbOj8A1H5YZliDn1n1zZF5qizaSlpOwEAuHhStt/z+YPE4OUP//sWlzz7DR7/6GCIR9k3247X4aoXN+OC//va6/iWY7UAAJM7gNNr1ciwiP8+1768Bf/d4alV+uOnhwAAg1NCn/0iIhpoGLxEwaPfGy/XsXQVvNS12HHdK1tx0N0j5YppQ+QakmZF5iVB3/nsnzJ4STWHPvNSoeiQa+hkCbCyrmfEIP99fU53T6lI/r6pOESjC40NR8SVUNXNNnl12Nu7TuJUfRsAeGWWmm2erJFUzwMA+041AgBm5aeFfbxERP0dg5cosBh1uPPcAgBAaxfTRsrdlwExEJGmaVqUmZcuVrA4XJ7AISUM00a96Xd3aYApI0BcreO73DiWKDviHixvhCAIuPtfexTPe4JHZRbsq3vn4caZw6BVq7BgbCZunjMCV53hv5s0ERH1TtiDl+XLl0OlUmHJkiXyMUEQsHTpUuTk5CAhIQFz587F/v37wz2UmCItme0q8+JbzGvQquX6l8Z2B1a7MxS+NS9K+YMSYdZrkGM1hqXeYtllE5GXasLzPzq903NeuG4qfjBlMP7wgwkBnzcbtFh7zxz84fvi82a9Bu0dTnl5cbQpt2DYU1KPb45496RRLn9+dvHpyEoy4m83ToNKpcLSS8fjwKOL8PIN0/DABWO5VJqIKATCutpo27ZtePHFFzFp0iSv408++SRWrFiBV199FaNGjcIf/vAHnHfeeTh06BAsFks4hxQzpELato7Oi1N9P7yNOg0ykwywGLVoanfI3XWbfDI0vq/Z+uACr12fQ+n0vBR8dd+8Ls85f3wWzh+f1eU56YkG/GDKYPz2nX1osTtx6bPfoLrZji/vnSsXH0dLVbOnUHfr8Tp5Gk/SpigwPmNYKjb/Zr7X83otE5xERKEUtp+qzc3NuPbaa/HSSy8hJcVT0yAIAlauXIkHH3wQl112GSZMmIDVq1ejtbUVa9asCddwYk5PMi++K5EMWg1UKhXGZiV5He9q6gkQMxvx8AFqNmjlupzDFc2obbHjQGljlEcF1Ld6gsMvvq3wC15G+/x7EBFReIXtE+22227DRRddhAULFngdLyoqQnl5ORYuXCgfMxgMmDNnDjZu3BjwWjabDY2NjV5/4p2Uedl5oh7OTjYl9M286NxLrMdke2enpPqZ/mCkT0FvRQw0rlMGmC12J3YU1wEAnrlmCt65bVZYmv8REVHnwhK8/POf/8TOnTuxfPlyv+fKy8UN+DIzvfe2yczMlJ/ztXz5clitVvlPbm5u6AcdYcoi0M3HagKe4xu8SAW8mYpGboVLF2J0Vv+ZasvP8A5eyhqiH7y0u7dwGJbm3Un3zOGpOC03OQojIiIa2EIevJSUlOCuu+7C66+/DqOx826pKp9lKoIg+B2TPPDAA2hoaJD/lJSUhHTM0TBhsFX+urrZf2diwHvaaEhKAs4dLQZ8V0wdgqFpJtw+b2Sfd4mONTfNHgaLovHde7tLA+7cHEk2d+G072opZfNAIiKKnJAHLzt27EBlZSWmTp0KrVYLrVaL9evX4+mnn4ZWq5UzLr5ZlsrKSr9sjMRgMCApKcnrT7wz6jQ4z72zcostcM2KtNpowdhMfH3fPFjdy4kzkoxYf+88/Or80ZEZbASNzLDgm1+fiz9dMRkAcKCsET9/bXtUxyRlXsbleL/vDHFQR0RE1B+F/Kfv/PnzUVhYiN27d8t/pk2bhmuvvRa7d+/GiBEjkJWVhbVr18qvsdvtWL9+PWbOnBnq4cQ0i9xwLvBqoTa7ZyfizrJS/ZE1QYdpwzxF3rtO1ONYVXPEx/H65mJsOlojZ14yffZdGkj/JkREsSTkS6UtFgsmTPDu52E2m5GWliYfX7JkCZYtW4aCggIUFBRg2bJlMJlMWLx4caiHE9OkTQ2b2wMvl5Z+4zcOwN/wh6aZ8cvzRuFPaw8DAD4sLMPtESxM3uTemwgA0hPFFVDcl4iIKDZEZVfp++67D21tbbj11ltRV1eH6dOn49NPPx0wPV4kUrfcJlvg4EXKvAzUD8075hcg02rEff/diw/2RjZ4OVzhWQ4tTd8ZdRroNCqvvZqIiCjyIvIr/ZdffomVK1fKj1UqFZYuXYqysjK0t7dj/fr1ftmagaCrzEt9qx3PfHHE67yB6PxxYnO7g+VN+Mmr2yAIkQkclCu9pK+NOjVyU02dvYSIiCJk4M1HxBCLYpNFX09/fkT+OtEwcIMXq0mHeaMHAQC+OFiJjUcDLysPNeVKL4e7D49Bq8H/XTUFBq0a9/bDYmkiongxcD8VY4CceQkQvChb0lsGcOYFAB68aCzWHaoCAJTUtkbkewbqfGzUqTFxiBX7Hjk/bNstEBFR9/gTOIoSDeLSZ2XwIggCNh6tht3h+fA0d7Hx4kAwMsOCK6cNASD2xGnvcGLNlhOd9scJhZ0n6vyOGbRi7REDFyKi6BrYn4pRJk0HKWte3t9bhjvf2OV1nnkATxtJ0hMNAIDqZjvu/e9evL+nFDuK6/CnKyeH/Hs1tXdg23Hv4MWgVUOj5tJoIqJYwE/FKLIEmDb6746Tfudp+aEpBy8Hyxux+VgtAODNnSfx1A8nQR2i/z51LWKRtNPl8nvulwtHheR7EBFR3zF4iSJzgMxLpsXgd54zQitsYtlpeckAIAcukv2ljZg4xBrgFb33/FdH8bcNRX7Hpw1Nwc/PyQ/J9yAior7j5H0UydNGdgdc7hUtgeo4lPsgDVSn56Xg2cVT/I6v/bYiZN/jWFVLwOO3zmPgQkQUSxi8RJE0bSQIQKt7aW6lzyaEN8wYisHJCREfWyy6eFIOLpqY7XXs6c+/w8m60KxAOuqzBcHPzh6O926fhXPHBN5zi4iIooPBSxQZtGq5nkWaOmps997naEgKm6IpPXzpOEzOTcatcz3ZkO3H/VcG9ZYgCDhV1+Z1bMJgKyYNSe7ztYmIKLRY8xJFKpUKiUYt6ls73JszGuUdpm+eMwJ7Sxpw5bTc6A4yxmRYjHj3tlkAxCzVf3ecDEnmpcnmgM3hXaibx266REQxiZmXKJP3N3JnXqQMzPUzhuGNn58Fq0kXtbHFulx3VuqkT8YEAF766hjm/fFLnKr3fy6Q6ib/WqNhaea+DZCIiMKCwUuUKYMXu8MFu1P87T9xgDem64nsZCMAoKyh3e+5xz78FkXVLfjNW4U9ulZVgOAlmYEjEVFMYvASZVK31uv/thUtin4vZsPA3Em6N9LMegDAidpW3PnGLnxYWOZ3ziafvZBcLkFe2aVU3WwH4Nlvav6YDKhU7K9DRBSL+Ot9lH1b1ih//eLXxwC4C3nZgr5bqe7gpai6BUXVLXhvTyk2PzAfWVajfI7d6UJ7hxNGnRgM3rBqK4prWvHxkrNhUmS3qprE7M3sgnT89uJxSE5g1oWIKFbxEzLKlA3onvvyKICBvYt0b6SZ/Rv6fXGwEoAYAErGPPQxADHr8vV31ThR24pP93v6w7R3OOXMyyCLAYOTE7glAxFRDGPwEmWBmufyg7NnUhP1fsd2nahDh9Plt3Lo0fcPoKbFLj8+XNEEANhRXIeJSz/Bs+uOAPBsQ0BERLGLwUuU3TR7uN8xBi89Y9b71wWdqm/Dmi0n/I7/bUMR5j61Tn68v1Scrvvdu/vQ4fREkIMCbM9ARESxhcFLlN2/aIzfh3Aii3V7JFBB7bGqFjz83n758RT3nkgA0GJ3yl/vOlEHl0uA7yUGMfNCRBTzGLxEmV6rxnnjvNvPM/PSc6t/ciZyUxOw8qrTAADljd7LpgPVxQBAY7sDx6qbofaJXtKZeSEiinkMXmLA3eeN8nrMgt2emzNqEL6+71wsmpDl99y10/Nw/6LRMOrUGJ+T5Pf8zuJ6v+wNp42IiGIfg5cYMDTNjK/vmyc/TtBx2qi3DFo19D7Ly5deOh4FmRZs+c0CvOPeUkBp54k6+E48Sb1jiIgodjF4iRE5ip2jmxXN6qhnVCoVkhS9We49f7TcANCaoINOo8afrpgMwJNd2V1SjxO13vsiGRk4EhHFPM5PxAiN2pMDUC7ppZ5L0Hti8bmjB/k9f/nUIZgw2Iq6VjuufnEzDpaLy6Wzkoz42TkjMCKdexkREcUDBi8xSM2u9EEpqfVswjgu27/GBQBGZ1lQ1uC9WePi6XkBl6wTEVFs4rRRDFl51WkYlmbCw5eMj/ZQ4ppaFXgZtcS3Ed3VZ+aGe0hERBRCDF5iyPenDMaX987D2E6yBtS1hy8ZB6NOjf/cMrPL83Q+hb0ZFmMnZxIRUSzitBH1Gz+eNRw3zBgGNefdiIj6NWZeqF9h4EJE1P8xeKEB6fLThwAApg1NifJIiIiotzhtRAPS0kvHYeLgJFw4MTvaQyEiol5i8EIDksWow42zuDyaiCgecdqIiIiI4gqDFyIiIoorDF6IiIgorjB4ISIiorjC4IWIiIjiCoMXIiIiiisMXoiIiCiuMHghIiKiuMLghYiIiOIKgxciIiKKKwxeiIiIKK4weCEiIqK4wuCFiIiI4kpc7iotCAIAoLGxMcojISIiop6SPrelz/FgxWXw0tTUBADIzc2N8kiIiIiot5qammC1WoN+vUroa/gTBS6XC6WlpbBYLFCpVH2+XmNjI3Jzc1FSUoKkpKQQjDB6eC+xh/cRe/rLvfA+Yg/vpWuCIKCpqQk5OTlQq4OvXInLzItarcaQIUNCft2kpKS4f7NJeC+xh/cRe/rLvfA+Yg/vpXN9ybhIWLBLREREcYXBCxEREcUVBi8ADAYDHn74YRgMhmgPpc94L7GH9xF7+su98D5iD+8lMuKyYJeIiIgGLmZeiIiIKK4weCEiIqK4wuCFiIiI4gqDFyIiIoorMRu8LF++HGeccQYsFgsyMjLw/e9/H4cOHfI6RxAELF26FDk5OUhISMDcuXOxf/9++fna2lrccccdGD16NEwmE/Ly8nDnnXeioaHB6zp1dXW47rrrYLVaYbVacd1116G+vr7bMRYWFmLOnDlISEjA4MGD8eijj3rt11BWVobJkyfDaDRCpVLBZDLF5X18+eWXUKlUfn8WLFgQd/cCAH/5y1+QkZEBtVoNtVqNpKSksP67PPbYY5g5cyZMJhOSk5O7vYee3kuk31/huo9ovL/CdS8AcOmllyIhIQEqlQparRZTp04Ny30cP34cN910E4YPH46EhATk5+fj4Ycfht1u7/N9RPK9Fc77iPR7K5z3AkT+Z9ell16KvLw8GI1GZGdn47rrrkNpaWmf76WsrAyLFy/G6NGjoVarsWTJkm6v6UeIUeeff76watUqYd++fcLu3buFiy66SMjLyxOam5vlcx5//HHBYrEIb775plBYWChcddVVQnZ2ttDY2CgIgiAUFhYKl112mfDee+8JR44cET7//HOhoKBAuPzyy72+16JFi4QJEyYIGzduFDZu3ChMmDBBuPjii7scX0NDg5CZmSlcffXVQmFhofDmm28KFotF+OMf/yifU1RUJOTl5Qk//elPhdGjRwuLFy+Oy/tYt26dAECYPXu2sHLlSuHLL78UPvvsM+HCCy+Mu3v561//KlgsFmHy5MnCE088ITz55JNCQkKCMG3atLDdy+9+9zthxYoVwj333CNYrdYu76E39xLp91e47iMa769w3ctf//pXQaPRCLfccovw0UcfCY8//rig0WiEQYMGhfw+PvroI+HGG28UPvnkE+Ho0aPCu+++K2RkZAi//OUv+3wfkXxvhfM+Iv3eCue9RONn14oVK4RNmzYJx48fFzZs2CDMmDFDmDFjRp/vpaioSLjzzjuF1atXC6eddppw1113dXnNQGI2ePFVWVkpABDWr18vCIIguFwuISsrS3j88cflc9rb2wWr1So8//zznV7n3//+t6DX64WOjg5BEAThwIEDAgBh8+bN8jmbNm0SAAgHDx7s9Dp//etfBavVKrS3t8vHli9fLuTk5Agul8vv/Dlz5gh33XVXXN6H9AOgrq7O67XxeC8zZswQfvWrX3m97q677hLOPPPMsNyL0qpVq3r8QRlr769w3kek31/hvJdA76+f/exnYb8PyZNPPikMHz68z/ehFMn3VqjvI5rvrVDfSzR/dkneffddQaVSCXa7vU/3oiS9v3orZqeNfEnprNTUVABAUVERysvLsXDhQvkcg8GAOXPmYOPGjV1eJykpCVqtuK3Tpk2bYLVaMX36dPmcs846C1artcvrbNq0CXPmzPFq3nP++eejtLQUx48f75f3MWXKFGRnZ2P+/PlYt25dXN6LzWaD0Wj0el1CQgJ27twZlnsJVqy9vyJxH5F6f4XzXgK9v6TN5ywWS9jvo6GhQf7v1Zf76OzaQGT+PUJ9H9F8b4XqXqL9s6u2thb/+Mc/MHPmTOh0uj7dSyjERfAiCALuuecezJ49GxMmTAAAlJeXAwAyMzO9zs3MzJSf81VTU4Pf//73uPnmm+Vj5eXlyMjI8Ds3IyOj0+tIrwv0vZVj6y/3kZ2djRdffBFvvvkm3nrrLYwePRrnnnsubrjhhri7l/PPPx8vv/wyduzYAUEQsH37drzyyitwOBw488wzQ34vwYq191ewYvH9Fc578X1/bdu2DatWrQIAZGVlhfU+jh49imeeeQa33HJLn+/DVyTfW6G8j2i/t0J5L9H62XX//ffDbDYjLS0NJ06cwLvvvtvnewmFuNhV+vbbb8fevXvxzTff+D2nUqm8HguC4HcMELf2vuiiizBu3Dg8/PDDXV7D9zrjx49HcXExAODss8/GRx991On37ux6gFg81tDQEHf3MXr0aIwePVp+fsaMGfjwww+xa9cuHD58OK7u5aGHHkJ5eTnOOussCIKAzMxMDBkyBFVVVXjhhRfCci/diZf3V7juIxrvr3Ddi+/7y2AwwGg0wm63Q6PRhO0+SktLsWjRIlxxxRX46U9/2uf78BWp91ao7yOa761Q30u0fnbde++9uOmmm1BcXIxHHnkE119/PT744AOoVKqQvb+CEfPByx133IH33nsPX331FYYMGSIfV/4Wk52dLR+vrKz0i/qampqwaNEiJCYm4u233/ZKeWVlZaGiosLv+1ZVVcnX+fDDD9HR0QFATNNJr/ONIisrKwH4R7wA8N1336GxsRF79+6N6/sAxH+Turo6ZGdnx929JCQk4G9/+xteeOEFVFRU4PHHH8c//vEPmM1mTJo0KeT30hPx8P6K1H0A4X9/hfNelO+vn//85/jss8/w85//HE899RTS09PDch+lpaWYN28eZsyYgRdffDEk96EUqfdWuO8DiNx7Kxz3Eq2fXenp6UhPT8eoUaMwduxY5ObmYvPmzXIgGIp/l6D0ukomQlwul3DbbbcJOTk5wuHDhwM+n5WVJTzxxBPyMZvN5leU1NDQIJx11lnCnDlzhJaWFr/rSMWhW7ZskY9t3ry5R8WhycnJgs1mk489/vjjfkVJ0n3o9Xrh+uuvj9v7UN5LTk6OsHDhQmHevHn94l7OOOMM4ZprrgnLvSj1tjg0lt5f4b4P5b2E+/0V7nvx/dl1zjnneL2/QnkfJ0+eFAoKCoSrr75acDgcYbmPSLy3wnkfynuJxHsrkvcSqZ9dkhMnTggAhHXr1oXkXgQh+ILdmA1efvGLXwhWq1X48ssvhbKyMvlPa2urfM7jjz8uWK1W4a233hIKCwuFa665xms5WGNjozB9+nRh4sSJwpEjR7yuo3xTLVq0SJg0aZKwadMmYdOmTcLEiRO7XZZbX18vZGZmCtdcc41QWFgovPXWW0JSUpLXcjBBEIQrrrhCSExMFEaNGiX84Ac/ENauXSt8+eWXcXUff/7zn4W3335bWLx4sWCxWITFixcLAISXX3457v5NDh06JLz22mvC4sWLhcTERGHevHlCcnKysHXr1rDdS3FxsbBr1y7hkUceERITE4Vdu3YJu3btEpqamvp0L4IQ2fdXuO4jGu+vcN3LoUOHhPnz5wsWi0V47rnnhEsvvVR+f4X6Pk6dOiWMHDlSOPfcc4WTJ096ndOVWHtvhfM+Iv3eCue9RPpn15YtW4RnnnlG2LVrl3D8+HHhiy++EGbPni3k5+d7rSQK5l4EQZD/n5s6daqwePFiYdeuXcL+/fu7/O+kFLPBC4CAf1atWiWf43K5hIcffljIysoSDAaDcM455wiFhYXy89IyuUB/ioqK5PNqamqEa6+9VrBYLILFYhGuvfZav6V1gezdu1c4++yzBYPBIGRlZQlLly71iyz7w3088cQTQn5+fr+4lwMHDginnXZaRO/lhhtuCHhOV7+99OReBCGy769w3Uc03l/huhcpaxiJ+1i1alWn53Qnlt5b4byPSL+3wnkvkf7ZtXfvXmHevHlCamqqYDAYhGHDhgm33HKLcPLkyT7fiyAEfn8NHTq022tLVO6LEBEREcWFuFgqTURERCRh8EJERERxhcELERERxRUGL0RERBRXGLwQERFRXGHwQkRERHGFwQsRERHFFQYvREREFFcYvBAREVFcYfBCREREcYXBCxEREcUVBi9EREQUV/4frOLSIWdgXY4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_data_raw\n",
    "# train_data_final\n",
    "plt.plot(train_data_raw['Open'])\n",
    "plt.plot(test_data_raw['Open'])\n",
    "# plt.plot(train_data_final['Close_norm'])\n",
    "# plt.plot(test_data_final['Close_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "153cf18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "688"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac476c4",
   "metadata": {
    "id": "9ac476c4"
   },
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc7891",
   "metadata": {
    "id": "01cc7891"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "(2023-06-05)\n",
    "cuda support check\n",
    "//read textual data into correct shape\n",
    "hyperparam tuning: number of neurons: tune to right number of neurons in FC in model\n",
    "//max_text_per_iter -> code in dataloader to maintain the size\n",
    "\n",
    "(2023-06-07)\n",
    "cuda check\n",
    "roberta encoder fix\n",
    "multi label - how to create target label?\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1b8731",
   "metadata": {
    "id": "0f1b8731"
   },
   "source": [
    "## Prep textual data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SXhjZ2x4UuT2",
   "metadata": {
    "id": "SXhjZ2x4UuT2"
   },
   "source": [
    "### Crawl textual news data from internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4oOXAgDCUuz_",
   "metadata": {
    "id": "4oOXAgDCUuz_"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import Request\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Parameters\n",
    "n = 3 #the # of article headlines displayed per ticker\n",
    "tickers = ['AAPL', 'TSLA', 'AMZN']\n",
    "\n",
    "\n",
    "\n",
    "# Get Data\n",
    "finviz_url = 'https://finviz.com/quote.ashx?t='\n",
    "news_tables = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    url = finviz_url + ticker\n",
    "    req = Request(url=url,\n",
    "                  headers={'user-agent': 'Mozilla/5.0',\n",
    "                                   'referer': 'https://...'})\n",
    "    resp = urlopen(req)\n",
    "    html = BeautifulSoup(resp, features=\"lxml\")\n",
    "    news_table = html.find(id='news-table')\n",
    "    news_tables[ticker] = news_table\n",
    "\n",
    "try:\n",
    "    for ticker in tickers:\n",
    "        df = news_tables[ticker]\n",
    "        df_tr = df.findAll('tr')\n",
    "\n",
    "        print ('\\n')\n",
    "        print ('Recent News Headlines for {}: '.format(ticker))\n",
    "\n",
    "        for i, table_row in enumerate(df_tr):\n",
    "            a_text = table_row.a.text\n",
    "            td_text = table_row.td.text\n",
    "            td_text = td_text.strip()\n",
    "            print(a_text,'(',td_text,')')\n",
    "            if i == n-1:\n",
    "                break\n",
    "except KeyError:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Iterate through the news\n",
    "parsed_news = []\n",
    "for file_name, news_table in news_tables.items():\n",
    "    for x in news_table.findAll('tr'):\n",
    "        text = x.a.get_text()\n",
    "        date_scrape = x.td.text.split()\n",
    "\n",
    "        if len(date_scrape) == 1:\n",
    "            time = date_scrape[0]\n",
    "\n",
    "        else:\n",
    "            date = date_scrape[0]\n",
    "            time = date_scrape[1]\n",
    "\n",
    "        ticker = file_name.split('_')[0]\n",
    "\n",
    "        parsed_news.append([ticker, date, time, text])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yjXll-U6U1fV",
   "metadata": {
    "id": "yjXll-U6U1fV"
   },
   "source": [
    "### Read downloaded data from saved files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4t4-Er1hFOh7",
   "metadata": {
    "id": "4t4-Er1hFOh7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8a06ae3",
   "metadata": {
    "id": "a8a06ae3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_data_df = pd.read_csv('./data/XOM_20200401_20230401_medium (1).csv',\n",
    "                           sep= ',',\n",
    "                           header= 0,\n",
    "                           engine='python',\n",
    "                           on_bad_lines = 'skip')\n",
    "text_data_df = text_data_df[['Date', 'News']]\n",
    "\n",
    "\n",
    "text_data_df = text_data_df.groupby('Date')['News'].apply('$$$###'.join)\n",
    "\n",
    "text_data_df.index = pd.to_datetime(text_data_df.index, dayfirst=True)\n",
    "# text_data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3155ea81",
   "metadata": {
    "id": "3155ea81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-07 00:00:00\n",
      "2022-09-28 00:00:00\n",
      "2022-10-04 00:00:00\n",
      "2022-12-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "all_train_df = train_data_final.join(text_data_df, how = 'left')\n",
    "all_test_df = test_data_final.join(text_data_df, how = 'left')\n",
    "\n",
    "print(all_train_df.index.min())\n",
    "print(all_train_df.index.max())\n",
    "print(all_test_df.index.min())\n",
    "print(all_test_df.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2WWhDJkjpOYV",
   "metadata": {
    "id": "2WWhDJkjpOYV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<Axes: title={'center': 'Open_norm'}>,\n",
       "        <Axes: title={'center': 'High_norm'}>,\n",
       "        <Axes: title={'center': 'Low_norm'}>],\n",
       "       [<Axes: title={'center': 'Close_norm'}>,\n",
       "        <Axes: title={'center': 'Close'}>,\n",
       "        <Axes: title={'center': 'Volume_norm'}>],\n",
       "       [<Axes: title={'center': 'High-Open_norm'}>,\n",
       "        <Axes: title={'center': 'Low-Open_norm'}>,\n",
       "        <Axes: title={'center': 'Close-Open_norm'}>],\n",
       "       [<Axes: title={'center': 'Label_2up1down'}>, <Axes: >, <Axes: >]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check class imbalance\n",
    "# train_data_norm.describe()\n",
    "train_data_final.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffc356b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_norm</th>\n",
       "      <th>High_norm</th>\n",
       "      <th>Low_norm</th>\n",
       "      <th>Close_norm</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume_norm</th>\n",
       "      <th>High-Open_norm</th>\n",
       "      <th>Low-Open_norm</th>\n",
       "      <th>Close-Open_norm</th>\n",
       "      <th>Label_2up1down</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-07</th>\n",
       "      <td>0.542314</td>\n",
       "      <td>0.521109</td>\n",
       "      <td>0.540689</td>\n",
       "      <td>0.457875</td>\n",
       "      <td>0.531036</td>\n",
       "      <td>0.122365</td>\n",
       "      <td>0.002804</td>\n",
       "      <td>0.887856</td>\n",
       "      <td>0.429114</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-08</th>\n",
       "      <td>0.536903</td>\n",
       "      <td>0.517967</td>\n",
       "      <td>0.536023</td>\n",
       "      <td>0.430191</td>\n",
       "      <td>0.516544</td>\n",
       "      <td>0.092914</td>\n",
       "      <td>0.025379</td>\n",
       "      <td>0.892927</td>\n",
       "      <td>0.366102</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-09</th>\n",
       "      <td>0.523446</td>\n",
       "      <td>0.512638</td>\n",
       "      <td>0.528613</td>\n",
       "      <td>0.521458</td>\n",
       "      <td>0.523790</td>\n",
       "      <td>0.088649</td>\n",
       "      <td>0.108659</td>\n",
       "      <td>0.941092</td>\n",
       "      <td>0.507747</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-10</th>\n",
       "      <td>0.534406</td>\n",
       "      <td>0.513458</td>\n",
       "      <td>0.535337</td>\n",
       "      <td>0.455050</td>\n",
       "      <td>0.515313</td>\n",
       "      <td>0.051788</td>\n",
       "      <td>0.004241</td>\n",
       "      <td>0.907498</td>\n",
       "      <td>0.374360</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-13</th>\n",
       "      <td>0.524279</td>\n",
       "      <td>0.512228</td>\n",
       "      <td>0.535200</td>\n",
       "      <td>0.529046</td>\n",
       "      <td>0.524337</td>\n",
       "      <td>0.025064</td>\n",
       "      <td>0.095709</td>\n",
       "      <td>0.989614</td>\n",
       "      <td>0.505794</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-22</th>\n",
       "      <td>0.843368</td>\n",
       "      <td>0.823610</td>\n",
       "      <td>0.829010</td>\n",
       "      <td>0.473955</td>\n",
       "      <td>0.808313</td>\n",
       "      <td>0.106026</td>\n",
       "      <td>0.049319</td>\n",
       "      <td>0.854484</td>\n",
       "      <td>0.332352</td>\n",
       "      <td>1</td>\n",
       "      <td>Exxon Mobil shuts Fos-Sur-Mer refinery in Fran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-23</th>\n",
       "      <td>0.778163</td>\n",
       "      <td>0.753108</td>\n",
       "      <td>0.751201</td>\n",
       "      <td>0.277097</td>\n",
       "      <td>0.742412</td>\n",
       "      <td>0.257849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.758137</td>\n",
       "      <td>0.317074</td>\n",
       "      <td>1</td>\n",
       "      <td>Global Brake Fluid Market Research Report 2022...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-26</th>\n",
       "      <td>0.742092</td>\n",
       "      <td>0.735620</td>\n",
       "      <td>0.738027</td>\n",
       "      <td>0.407868</td>\n",
       "      <td>0.718212</td>\n",
       "      <td>0.183303</td>\n",
       "      <td>0.153690</td>\n",
       "      <td>0.904995</td>\n",
       "      <td>0.377393</td>\n",
       "      <td>2</td>\n",
       "      <td>Chevron Halting Production At Two Offshore Pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-27</th>\n",
       "      <td>0.751942</td>\n",
       "      <td>0.745730</td>\n",
       "      <td>0.750652</td>\n",
       "      <td>0.574854</td>\n",
       "      <td>0.742275</td>\n",
       "      <td>0.121312</td>\n",
       "      <td>0.155879</td>\n",
       "      <td>0.925373</td>\n",
       "      <td>0.458716</td>\n",
       "      <td>2</td>\n",
       "      <td>Kumul Bids for 5% Santos Stake in PNG LNG Sant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-28</th>\n",
       "      <td>0.759018</td>\n",
       "      <td>0.777702</td>\n",
       "      <td>0.765884</td>\n",
       "      <td>0.636800</td>\n",
       "      <td>0.784933</td>\n",
       "      <td>0.150579</td>\n",
       "      <td>0.365008</td>\n",
       "      <td>0.981453</td>\n",
       "      <td>0.658145</td>\n",
       "      <td>0</td>\n",
       "      <td>Carbon Capture, Utilization, and Storage Marke...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>688 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Open_norm  High_norm  Low_norm  Close_norm     Close  Volume_norm  \\\n",
       "Date                                                                            \n",
       "2020-01-07   0.542314   0.521109  0.540689    0.457875  0.531036     0.122365   \n",
       "2020-01-08   0.536903   0.517967  0.536023    0.430191  0.516544     0.092914   \n",
       "2020-01-09   0.523446   0.512638  0.528613    0.521458  0.523790     0.088649   \n",
       "2020-01-10   0.534406   0.513458  0.535337    0.455050  0.515313     0.051788   \n",
       "2020-01-13   0.524279   0.512228  0.535200    0.529046  0.524337     0.025064   \n",
       "...               ...        ...       ...         ...       ...          ...   \n",
       "2022-09-22   0.843368   0.823610  0.829010    0.473955  0.808313     0.106026   \n",
       "2022-09-23   0.778163   0.753108  0.751201    0.277097  0.742412     0.257849   \n",
       "2022-09-26   0.742092   0.735620  0.738027    0.407868  0.718212     0.183303   \n",
       "2022-09-27   0.751942   0.745730  0.750652    0.574854  0.742275     0.121312   \n",
       "2022-09-28   0.759018   0.777702  0.765884    0.636800  0.784933     0.150579   \n",
       "\n",
       "            High-Open_norm  Low-Open_norm  Close-Open_norm  Label_2up1down  \\\n",
       "Date                                                                         \n",
       "2020-01-07        0.002804       0.887856         0.429114               1   \n",
       "2020-01-08        0.025379       0.892927         0.366102               0   \n",
       "2020-01-09        0.108659       0.941092         0.507747               0   \n",
       "2020-01-10        0.004241       0.907498         0.374360               0   \n",
       "2020-01-13        0.095709       0.989614         0.505794               0   \n",
       "...                    ...            ...              ...             ...   \n",
       "2022-09-22        0.049319       0.854484         0.332352               1   \n",
       "2022-09-23        0.000000       0.758137         0.317074               1   \n",
       "2022-09-26        0.153690       0.904995         0.377393               2   \n",
       "2022-09-27        0.155879       0.925373         0.458716               2   \n",
       "2022-09-28        0.365008       0.981453         0.658145               0   \n",
       "\n",
       "                                                         News  \n",
       "Date                                                           \n",
       "2020-01-07                                                NaN  \n",
       "2020-01-08                                                NaN  \n",
       "2020-01-09                                                NaN  \n",
       "2020-01-10                                                NaN  \n",
       "2020-01-13                                                NaN  \n",
       "...                                                       ...  \n",
       "2022-09-22  Exxon Mobil shuts Fos-Sur-Mer refinery in Fran...  \n",
       "2022-09-23  Global Brake Fluid Market Research Report 2022...  \n",
       "2022-09-26  Chevron Halting Production At Two Offshore Pla...  \n",
       "2022-09-27  Kumul Bids for 5% Santos Stake in PNG LNG Sant...  \n",
       "2022-09-28  Carbon Capture, Utilization, and Storage Marke...  \n",
       "\n",
       "[688 rows x 11 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2c49d9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1392896946702041], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train[i-window_size: i, 0]\n",
    "all_train[i:i+no_of_days_to_lookforward, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuUgt-MnU-aV",
   "metadata": {
    "id": "nuUgt-MnU-aV"
   },
   "source": [
    "### create windows and label\n",
    "### also split the text seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "56e91800",
   "metadata": {
    "id": "56e91800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "678\n",
      "678\n",
      "678\n",
      "torch.Size([678, 10])\n",
      "678\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "all_train = all_train_df.values\n",
    "\n",
    "window_size = no_of_days_to_lookback\n",
    "\n",
    "X_numerical_train = []\n",
    "y_train = []\n",
    "X_text_train = []\n",
    "X_text_train_curr = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(window_size, len(all_train) - no_of_days_to_lookforward + 1):\n",
    "    X_numerical_train.append(all_train[i-window_size: i, 0])# :-2])\n",
    "\n",
    "    # split and append sequence of text\n",
    "    curr_seq = all_train[i-window_size: i, -1]\n",
    "    for j in range(window_size):\n",
    "        if (curr_seq[window_size - 1 -j] is not np.NaN):\n",
    "            split_curr_seq = curr_seq[window_size - 1 -j].split('$$$###')\n",
    "        else:\n",
    "            split_curr_seq = []\n",
    "#         X_text_train_curr = X_text_train_curr + split_curr_seq\n",
    "#         else:\n",
    "            \n",
    "\n",
    "    if len(X_text_train_curr) > max_text_per_iter:\n",
    "        X_text_train_curr = X_text_train_curr[:max_text_per_iter]\n",
    "\n",
    "    X_text_train.append(X_text_train_curr)\n",
    "\n",
    "    # target labels\n",
    "    y_train.append(all_train[i:i+no_of_days_to_lookforward, 0])#-2])\n",
    "\n",
    "X_numerical_train, y_train = np.array(X_numerical_train).astype(np.float16), np.array(y_train).astype(np.float16)\n",
    "print(type(X_numerical_train))\n",
    "print(type(y_train))\n",
    "\n",
    "X_numerical_train = torch.from_numpy(X_numerical_train).type(torch.Tensor)\n",
    "y_train = torch.from_numpy(y_train).type(torch.Tensor)#.long()\n",
    "\n",
    "print(len(X_numerical_train))\n",
    "print(len(X_text_train))\n",
    "print(len(y_train))\n",
    "print(X_numerical_train.shape)\n",
    "\n",
    "print(len(X_text_train))\n",
    "print(len(X_text_train[2]))\n",
    "# print(X_text_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e404c5e2",
   "metadata": {
    "id": "e404c5e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "52\n",
      "52\n",
      "52\n",
      "torch.Size([52, 10])\n",
      "52\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "all_test = all_test_df.values\n",
    "\n",
    "\n",
    "X_numerical_test = []\n",
    "y_test = []\n",
    "X_text_test = []\n",
    "X_text_test_curr = []\n",
    "\n",
    "for i in range(window_size, len(all_test) - no_of_days_to_lookforward + 1):\n",
    "    X_numerical_test.append(all_test[i-window_size: i, 0])#:-2])\n",
    "\n",
    "    # split and append sequence of text (in reverse order to add the latest news first)\n",
    "    curr_seq = all_test[i-window_size: i, -1]\n",
    "    for j in range(window_size):\n",
    "        if (curr_seq[window_size - 1 -j] is not np.NaN):\n",
    "            split_curr_seq = curr_seq[window_size - 1 -j].split('$$$###')\n",
    "        else:\n",
    "            split_curr_seq = []\n",
    "            \n",
    "#         split_curr_seq = curr_seq[window_size - 1 -j].split('$$$###')\n",
    "        X_text_test_curr = X_text_test_curr + split_curr_seq\n",
    "\n",
    "    if len(X_text_test_curr) > max_text_per_iter:\n",
    "        X_text_test_curr = X_text_test_curr[:max_text_per_iter]\n",
    "\n",
    "    X_text_test.append(X_text_test_curr)\n",
    "\n",
    "    # target labels\n",
    "    y_test.append(all_test[i:i+no_of_days_to_lookforward, 0])#-2])\n",
    "\n",
    "X_numerical_test, y_test = np.array(X_numerical_test).astype(np.float16), np.array(y_test).astype(np.float16)\n",
    "print(type(X_numerical_test))\n",
    "print(type(y_test))\n",
    "\n",
    "X_numerical_test = torch.from_numpy(X_numerical_test).type(torch.Tensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.Tensor)#.long()\n",
    "\n",
    "print(len(X_numerical_test))\n",
    "print(len(X_text_test))\n",
    "print(len(y_test))\n",
    "print(X_numerical_test.shape)\n",
    "\n",
    "print(len(X_text_test))\n",
    "print(len(X_text_test[2]))\n",
    "# print(X_text_test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dcd7fcba",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5015],\n",
       "        [0.4897],\n",
       "        [0.4873],\n",
       "        [0.4719],\n",
       "        [0.4675],\n",
       "        [0.4663],\n",
       "        [0.4490],\n",
       "        [0.4409],\n",
       "        [0.4158],\n",
       "        [0.4167],\n",
       "        [0.4070],\n",
       "        [0.4373],\n",
       "        [0.4163],\n",
       "        [0.4033],\n",
       "        [0.4048],\n",
       "        [0.4153],\n",
       "        [0.4114],\n",
       "        [0.4126],\n",
       "        [0.3987],\n",
       "        [0.3982],\n",
       "        [0.3999],\n",
       "        [0.3899],\n",
       "        [0.3635],\n",
       "        [0.3516],\n",
       "        [0.3210],\n",
       "        [0.2817],\n",
       "        [0.2408],\n",
       "        [0.2939],\n",
       "        [0.3127],\n",
       "        [0.2888],\n",
       "        [0.2637],\n",
       "        [0.2418],\n",
       "        [0.1429],\n",
       "        [0.2047],\n",
       "        [0.1523],\n",
       "        [0.1178],\n",
       "        [0.1193],\n",
       "        [0.0415],\n",
       "        [0.0527],\n",
       "        [0.0435],\n",
       "        [0.0322],\n",
       "        [0.0476],\n",
       "        [0.0155],\n",
       "        [0.0417],\n",
       "        [0.0707],\n",
       "        [0.0845],\n",
       "        [0.0793],\n",
       "        [0.0669],\n",
       "        [0.0961],\n",
       "        [0.0756],\n",
       "        [0.1017],\n",
       "        [0.1393],\n",
       "        [0.1175],\n",
       "        [0.1594],\n",
       "        [0.1462],\n",
       "        [0.1949],\n",
       "        [0.1809],\n",
       "        [0.1608],\n",
       "        [0.1331],\n",
       "        [0.1259],\n",
       "        [0.1175],\n",
       "        [0.1296],\n",
       "        [0.1247],\n",
       "        [0.1553],\n",
       "        [0.1626],\n",
       "        [0.1802],\n",
       "        [0.1689],\n",
       "        [0.1798],\n",
       "        [0.2061],\n",
       "        [0.2257],\n",
       "        [0.1973],\n",
       "        [0.1541],\n",
       "        [0.2061],\n",
       "        [0.1876],\n",
       "        [0.1880],\n",
       "        [0.1929],\n",
       "        [0.2040],\n",
       "        [0.1943],\n",
       "        [0.1710],\n",
       "        [0.1340],\n",
       "        [0.1522],\n",
       "        [0.1802],\n",
       "        [0.1962],\n",
       "        [0.1832],\n",
       "        [0.1947],\n",
       "        [0.1754],\n",
       "        [0.1989],\n",
       "        [0.2115],\n",
       "        [0.2084],\n",
       "        [0.1860],\n",
       "        [0.1930],\n",
       "        [0.2144],\n",
       "        [0.2301],\n",
       "        [0.2423],\n",
       "        [0.2808],\n",
       "        [0.3235],\n",
       "        [0.2979],\n",
       "        [0.3064],\n",
       "        [0.2236],\n",
       "        [0.2307],\n",
       "        [0.1934],\n",
       "        [0.2520],\n",
       "        [0.2301],\n",
       "        [0.2040],\n",
       "        [0.2332],\n",
       "        [0.1981],\n",
       "        [0.2148],\n",
       "        [0.2012],\n",
       "        [0.1754],\n",
       "        [0.1869],\n",
       "        [0.1720],\n",
       "        [0.1667],\n",
       "        [0.1815],\n",
       "        [0.1832],\n",
       "        [0.1840],\n",
       "        [0.1733],\n",
       "        [0.1628],\n",
       "        [0.1594],\n",
       "        [0.1348],\n",
       "        [0.1605],\n",
       "        [0.1543],\n",
       "        [0.1890],\n",
       "        [0.1790],\n",
       "        [0.1792],\n",
       "        [0.1637],\n",
       "        [0.1628],\n",
       "        [0.1768],\n",
       "        [0.1664],\n",
       "        [0.1730],\n",
       "        [0.1639],\n",
       "        [0.1735],\n",
       "        [0.1707],\n",
       "        [0.1626],\n",
       "        [0.1353],\n",
       "        [0.1476],\n",
       "        [0.1516],\n",
       "        [0.1768],\n",
       "        [0.1664],\n",
       "        [0.1639],\n",
       "        [0.1746],\n",
       "        [0.1954],\n",
       "        [0.1810],\n",
       "        [0.1677],\n",
       "        [0.1553],\n",
       "        [0.1632],\n",
       "        [0.1552],\n",
       "        [0.1517],\n",
       "        [0.1426],\n",
       "        [0.1372],\n",
       "        [0.1370],\n",
       "        [0.1422],\n",
       "        [0.1281],\n",
       "        [0.1192],\n",
       "        [0.1169],\n",
       "        [0.1281],\n",
       "        [0.1157],\n",
       "        [0.1085],\n",
       "        [0.1081],\n",
       "        [0.1117],\n",
       "        [0.0949],\n",
       "        [0.0963],\n",
       "        [0.0952],\n",
       "        [0.0793],\n",
       "        [0.0763],\n",
       "        [0.0748],\n",
       "        [0.0701],\n",
       "        [0.0824],\n",
       "        [0.0849],\n",
       "        [0.0696],\n",
       "        [0.0701],\n",
       "        [0.0588],\n",
       "        [0.0415],\n",
       "        [0.0359],\n",
       "        [0.0518],\n",
       "        [0.0498],\n",
       "        [0.0448],\n",
       "        [0.0330],\n",
       "        [0.0117],\n",
       "        [0.0265],\n",
       "        [0.0376],\n",
       "        [0.0309],\n",
       "        [0.0343],\n",
       "        [0.0592],\n",
       "        [0.0434],\n",
       "        [0.0461],\n",
       "        [0.0368],\n",
       "        [0.0318],\n",
       "        [0.0411],\n",
       "        [0.0373],\n",
       "        [0.0312],\n",
       "        [0.0291],\n",
       "        [0.0246],\n",
       "        [0.0494],\n",
       "        [0.0305],\n",
       "        [0.0241],\n",
       "        [0.0096],\n",
       "        [0.0000],\n",
       "        [0.0139],\n",
       "        [0.0240],\n",
       "        [0.0427],\n",
       "        [0.0258],\n",
       "        [0.0272],\n",
       "        [0.0235],\n",
       "        [0.0695],\n",
       "        [0.0782],\n",
       "        [0.0788],\n",
       "        [0.0617],\n",
       "        [0.0554],\n",
       "        [0.0858],\n",
       "        [0.0888],\n",
       "        [0.1056],\n",
       "        [0.0756],\n",
       "        [0.0824],\n",
       "        [0.0845],\n",
       "        [0.1262],\n",
       "        [0.1423],\n",
       "        [0.1289],\n",
       "        [0.1164],\n",
       "        [0.1047],\n",
       "        [0.0969],\n",
       "        [0.1201],\n",
       "        [0.1299],\n",
       "        [0.1389],\n",
       "        [0.1266],\n",
       "        [0.1567],\n",
       "        [0.1636],\n",
       "        [0.1705],\n",
       "        [0.1794],\n",
       "        [0.1531],\n",
       "        [0.1711],\n",
       "        [0.1747],\n",
       "        [0.1646],\n",
       "        [0.1345],\n",
       "        [0.1411],\n",
       "        [0.1401],\n",
       "        [0.1421],\n",
       "        [0.1426],\n",
       "        [0.1475],\n",
       "        [0.1376],\n",
       "        [0.1395],\n",
       "        [0.1393],\n",
       "        [0.1447],\n",
       "        [0.1803],\n",
       "        [0.1886],\n",
       "        [0.1913],\n",
       "        [0.1891],\n",
       "        [0.2227],\n",
       "        [0.2375],\n",
       "        [0.2371],\n",
       "        [0.2428],\n",
       "        [0.2365],\n",
       "        [0.2456],\n",
       "        [0.2487],\n",
       "        [0.2164],\n",
       "        [0.2148],\n",
       "        [0.2198],\n",
       "        [0.1886],\n",
       "        [0.2015],\n",
       "        [0.1962],\n",
       "        [0.1965],\n",
       "        [0.1998],\n",
       "        [0.1973],\n",
       "        [0.2285],\n",
       "        [0.2466],\n",
       "        [0.2708],\n",
       "        [0.2671],\n",
       "        [0.2666],\n",
       "        [0.2703],\n",
       "        [0.2500],\n",
       "        [0.2852],\n",
       "        [0.2905],\n",
       "        [0.2969],\n",
       "        [0.2871],\n",
       "        [0.2939],\n",
       "        [0.3269],\n",
       "        [0.3298],\n",
       "        [0.3547],\n",
       "        [0.3191],\n",
       "        [0.3477],\n",
       "        [0.3501],\n",
       "        [0.3469],\n",
       "        [0.3567],\n",
       "        [0.3943],\n",
       "        [0.4185],\n",
       "        [0.4050],\n",
       "        [0.4038],\n",
       "        [0.4214],\n",
       "        [0.4182],\n",
       "        [0.4172],\n",
       "        [0.3904],\n",
       "        [0.3784],\n",
       "        [0.3801],\n",
       "        [0.3511],\n",
       "        [0.3481],\n",
       "        [0.3264],\n",
       "        [0.3423],\n",
       "        [0.3301],\n",
       "        [0.3591],\n",
       "        [0.3596],\n",
       "        [0.3530],\n",
       "        [0.3459],\n",
       "        [0.3457],\n",
       "        [0.3591],\n",
       "        [0.3528],\n",
       "        [0.3467],\n",
       "        [0.3401],\n",
       "        [0.3411],\n",
       "        [0.3518],\n",
       "        [0.3315],\n",
       "        [0.3469],\n",
       "        [0.3608],\n",
       "        [0.3601],\n",
       "        [0.3516],\n",
       "        [0.3477],\n",
       "        [0.3203],\n",
       "        [0.3411],\n",
       "        [0.3311],\n",
       "        [0.3337],\n",
       "        [0.3413],\n",
       "        [0.3538],\n",
       "        [0.3816],\n",
       "        [0.3735],\n",
       "        [0.3687],\n",
       "        [0.3828],\n",
       "        [0.3936],\n",
       "        [0.4087],\n",
       "        [0.4114],\n",
       "        [0.4417],\n",
       "        [0.4192],\n",
       "        [0.3967],\n",
       "        [0.3828],\n",
       "        [0.3958],\n",
       "        [0.4041],\n",
       "        [0.4243],\n",
       "        [0.3843],\n",
       "        [0.3821],\n",
       "        [0.3887],\n",
       "        [0.3838],\n",
       "        [0.3896],\n",
       "        [0.3723],\n",
       "        [0.3860],\n",
       "        [0.3789],\n",
       "        [0.3889],\n",
       "        [0.4070],\n",
       "        [0.4043],\n",
       "        [0.4170],\n",
       "        [0.4150],\n",
       "        [0.4175],\n",
       "        [0.4336],\n",
       "        [0.4468],\n",
       "        [0.4385],\n",
       "        [0.4285],\n",
       "        [0.4312],\n",
       "        [0.4565],\n",
       "        [0.4502],\n",
       "        [0.4104],\n",
       "        [0.4094],\n",
       "        [0.4363],\n",
       "        [0.4529],\n",
       "        [0.4541],\n",
       "        [0.4607],\n",
       "        [0.4600],\n",
       "        [0.4412],\n",
       "        [0.4341],\n",
       "        [0.4568],\n",
       "        [0.4385],\n",
       "        [0.4348],\n",
       "        [0.4131],\n",
       "        [0.3889],\n",
       "        [0.4062],\n",
       "        [0.4026],\n",
       "        [0.4104],\n",
       "        [0.4128],\n",
       "        [0.3850],\n",
       "        [0.3853],\n",
       "        [0.3362],\n",
       "        [0.3342],\n",
       "        [0.3530],\n",
       "        [0.3652],\n",
       "        [0.3591],\n",
       "        [0.3572],\n",
       "        [0.3691],\n",
       "        [0.3689],\n",
       "        [0.3828],\n",
       "        [0.3811],\n",
       "        [0.3625],\n",
       "        [0.3630],\n",
       "        [0.3577],\n",
       "        [0.3569],\n",
       "        [0.3643],\n",
       "        [0.3569],\n",
       "        [0.3608],\n",
       "        [0.3716],\n",
       "        [0.3606],\n",
       "        [0.3555],\n",
       "        [0.3413],\n",
       "        [0.3342],\n",
       "        [0.3330],\n",
       "        [0.3062],\n",
       "        [0.2893],\n",
       "        [0.3127],\n",
       "        [0.3313],\n",
       "        [0.3306],\n",
       "        [0.3301],\n",
       "        [0.3328],\n",
       "        [0.3445],\n",
       "        [0.3264],\n",
       "        [0.3203],\n",
       "        [0.3174],\n",
       "        [0.3289],\n",
       "        [0.3228],\n",
       "        [0.3245],\n",
       "        [0.3081],\n",
       "        [0.3298],\n",
       "        [0.3225],\n",
       "        [0.3369],\n",
       "        [0.3325],\n",
       "        [0.3474],\n",
       "        [0.3320],\n",
       "        [0.3103],\n",
       "        [0.3186],\n",
       "        [0.3193],\n",
       "        [0.3330],\n",
       "        [0.3508],\n",
       "        [0.3806],\n",
       "        [0.3953],\n",
       "        [0.3945],\n",
       "        [0.3982],\n",
       "        [0.3884],\n",
       "        [0.4150],\n",
       "        [0.4316],\n",
       "        [0.4031],\n",
       "        [0.4050],\n",
       "        [0.4141],\n",
       "        [0.4387],\n",
       "        [0.4185],\n",
       "        [0.4075],\n",
       "        [0.4204],\n",
       "        [0.4346],\n",
       "        [0.4395],\n",
       "        [0.4365],\n",
       "        [0.4368],\n",
       "        [0.4448],\n",
       "        [0.4351],\n",
       "        [0.4482],\n",
       "        [0.4590],\n",
       "        [0.4683],\n",
       "        [0.4497],\n",
       "        [0.4668],\n",
       "        [0.4670],\n",
       "        [0.4663],\n",
       "        [0.4526],\n",
       "        [0.4617],\n",
       "        [0.4668],\n",
       "        [0.4736],\n",
       "        [0.4768],\n",
       "        [0.4705],\n",
       "        [0.4548],\n",
       "        [0.4517],\n",
       "        [0.4487],\n",
       "        [0.4568],\n",
       "        [0.4575],\n",
       "        [0.4543],\n",
       "        [0.4229],\n",
       "        [0.4011],\n",
       "        [0.4270],\n",
       "        [0.4324],\n",
       "        [0.4038],\n",
       "        [0.4324],\n",
       "        [0.4031],\n",
       "        [0.4092],\n",
       "        [0.3931],\n",
       "        [0.4233],\n",
       "        [0.4204],\n",
       "        [0.4292],\n",
       "        [0.4329],\n",
       "        [0.4250],\n",
       "        [0.4419],\n",
       "        [0.4309],\n",
       "        [0.4141],\n",
       "        [0.4131],\n",
       "        [0.4165],\n",
       "        [0.4099],\n",
       "        [0.3799],\n",
       "        [0.3931],\n",
       "        [0.4014],\n",
       "        [0.4128],\n",
       "        [0.4094],\n",
       "        [0.4231],\n",
       "        [0.4172],\n",
       "        [0.4136],\n",
       "        [0.4070],\n",
       "        [0.4138],\n",
       "        [0.4539],\n",
       "        [0.4868],\n",
       "        [0.5078],\n",
       "        [0.5146],\n",
       "        [0.5210],\n",
       "        [0.5200],\n",
       "        [0.5513],\n",
       "        [0.5508],\n",
       "        [0.5444],\n",
       "        [0.5747],\n",
       "        [0.5874],\n",
       "        [0.5698],\n",
       "        [0.5796],\n",
       "        [0.5474],\n",
       "        [0.5713],\n",
       "        [0.6108],\n",
       "        [0.6113],\n",
       "        [0.6045],\n",
       "        [0.6045],\n",
       "        [0.6250],\n",
       "        [0.6748],\n",
       "        [0.6753],\n",
       "        [0.6860],\n",
       "        [0.6899],\n",
       "        [0.7056],\n",
       "        [0.6685],\n",
       "        [0.6562],\n",
       "        [0.6548],\n",
       "        [0.6685],\n",
       "        [0.6313],\n",
       "        [0.6572],\n",
       "        [0.6528],\n",
       "        [0.6348],\n",
       "        [0.6694],\n",
       "        [0.6250],\n",
       "        [0.6602],\n",
       "        [0.6284],\n",
       "        [0.6255],\n",
       "        [0.6572],\n",
       "        [0.6826],\n",
       "        [0.6724],\n",
       "        [0.6885],\n",
       "        [0.7422],\n",
       "        [0.7998],\n",
       "        [0.7300],\n",
       "        [0.7300],\n",
       "        [0.7305],\n",
       "        [0.7168],\n",
       "        [0.6577],\n",
       "        [0.6436],\n",
       "        [0.6489],\n",
       "        [0.6611],\n",
       "        [0.6758],\n",
       "        [0.6968],\n",
       "        [0.7202],\n",
       "        [0.7222],\n",
       "        [0.7148],\n",
       "        [0.7183],\n",
       "        [0.6855],\n",
       "        [0.7168],\n",
       "        [0.7139],\n",
       "        [0.7017],\n",
       "        [0.7236],\n",
       "        [0.7227],\n",
       "        [0.7217],\n",
       "        [0.7271],\n",
       "        [0.7456],\n",
       "        [0.7544],\n",
       "        [0.7534],\n",
       "        [0.7656],\n",
       "        [0.7666],\n",
       "        [0.7920],\n",
       "        [0.7852],\n",
       "        [0.7847],\n",
       "        [0.7983],\n",
       "        [0.7666],\n",
       "        [0.7070],\n",
       "        [0.7119],\n",
       "        [0.7212],\n",
       "        [0.7417],\n",
       "        [0.7871],\n",
       "        [0.7437],\n",
       "        [0.7607],\n",
       "        [0.8062],\n",
       "        [0.8413],\n",
       "        [0.8267],\n",
       "        [0.8052],\n",
       "        [0.7524],\n",
       "        [0.7632],\n",
       "        [0.7568],\n",
       "        [0.7764],\n",
       "        [0.7988],\n",
       "        [0.8369],\n",
       "        [0.8477],\n",
       "        [0.7988],\n",
       "        [0.8398],\n",
       "        [0.8496],\n",
       "        [0.8521],\n",
       "        [0.8765],\n",
       "        [0.9043],\n",
       "        [0.8955],\n",
       "        [0.9316],\n",
       "        [0.9102],\n",
       "        [0.9141],\n",
       "        [0.9224],\n",
       "        [0.9404],\n",
       "        [0.9473],\n",
       "        [1.0000],\n",
       "        [0.9990],\n",
       "        [0.9785],\n",
       "        [0.9170],\n",
       "        [0.9155],\n",
       "        [0.8960],\n",
       "        [0.8462],\n",
       "        [0.8262],\n",
       "        [0.7930],\n",
       "        [0.7788],\n",
       "        [0.7944],\n",
       "        [0.7705],\n",
       "        [0.7822],\n",
       "        [0.8286],\n",
       "        [0.8540],\n",
       "        [0.7593],\n",
       "        [0.7676],\n",
       "        [0.7622],\n",
       "        [0.7261],\n",
       "        [0.7485],\n",
       "        [0.7705],\n",
       "        [0.7456],\n",
       "        [0.7266],\n",
       "        [0.7212],\n",
       "        [0.6997],\n",
       "        [0.7407],\n",
       "        [0.7656],\n",
       "        [0.7632],\n",
       "        [0.7793],\n",
       "        [0.7700],\n",
       "        [0.7788],\n",
       "        [0.7871],\n",
       "        [0.8286],\n",
       "        [0.8169],\n",
       "        [0.8433],\n",
       "        [0.8745],\n",
       "        [0.8794],\n",
       "        [0.8779],\n",
       "        [0.8740],\n",
       "        [0.8174],\n",
       "        [0.7622],\n",
       "        [0.7935],\n",
       "        [0.8135],\n",
       "        [0.8223],\n",
       "        [0.8384],\n",
       "        [0.8472],\n",
       "        [0.8203],\n",
       "        [0.8408],\n",
       "        [0.8306],\n",
       "        [0.8564],\n",
       "        [0.8706],\n",
       "        [0.8604],\n",
       "        [0.8896],\n",
       "        [0.9238],\n",
       "        [0.9365],\n",
       "        [0.9346],\n",
       "        [0.9263],\n",
       "        [0.9287],\n",
       "        [0.8687],\n",
       "        [0.8740],\n",
       "        [0.9009],\n",
       "        [0.8984],\n",
       "        [0.8569],\n",
       "        [0.8774],\n",
       "        [0.9033],\n",
       "        [0.9189],\n",
       "        [0.9048],\n",
       "        [0.8989],\n",
       "        [0.8989],\n",
       "        [0.8833],\n",
       "        [0.8267],\n",
       "        [0.8506],\n",
       "        [0.8623],\n",
       "        [0.8433],\n",
       "        [0.7783],\n",
       "        [0.7422],\n",
       "        [0.7520],\n",
       "        [0.7588]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b453b0",
   "metadata": {
    "id": "b5b453b0"
   },
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc5998e6",
   "metadata": {
    "id": "dc5998e6"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large', truncation=True, do_lower_case=True)\n",
    "\n",
    "class SiameseDataloader(Dataset):\n",
    "\n",
    "    def __init__(self, X_numerical_train, y_train, X_text_train, MAX_LEN, tokenizer):\n",
    "        self.X_numerical_train = X_numerical_train\n",
    "        self.X_text_train = X_text_train\n",
    "        self.MAX_LEN = MAX_LEN\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        token_type_ids = []\n",
    "        input_seq = []\n",
    "\n",
    "        for sent in X_text_train[index]:\n",
    "            encoded_sent = self.tokenizer.encode_plus(\n",
    "                text=sent,\n",
    "                add_special_tokens=True,        # Add `[CLS]` and `[SEP]` special tokens\n",
    "                max_length=self.MAX_LEN,             # Choose max length to truncate/pad\n",
    "                pad_to_max_length=True,         # Pad sentence to max length\n",
    "                #return_attention_mask=True      # Return attention mask\n",
    "                return_token_type_ids=True\n",
    "                )\n",
    "            input_ids.append(encoded_sent.get('input_ids'))\n",
    "            attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "            token_type_ids.append(encoded_sent.get('token_type_ids'))\n",
    "\n",
    "        # Convert lists to tensors\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        attention_masks = torch.tensor(attention_masks)\n",
    "        token_type_ids = torch.tensor(token_type_ids)\n",
    "\n",
    "\n",
    "        return {\n",
    "            'x_numerical': X_numerical_train[index],\n",
    "            'ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(attention_masks, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(y_train[index])#, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_numerical_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28f5fe04",
   "metadata": {
    "id": "28f5fe04"
   },
   "outputs": [],
   "source": [
    "train_set = SiameseDataloader(X_numerical_train, y_train, X_text_train, MAX_LEN, tokenizer)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "test_set = SiameseDataloader(X_numerical_test, y_test, X_text_test, MAX_LEN, tokenizer)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f256f7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/tmp/ipykernel_11411/2984344807.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_11411/2984344807.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'mask': torch.tensor(attention_masks, dtype=torch.long),\n",
      "/tmp/ipykernel_11411/2984344807.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_11411/2984344807.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'targets': torch.tensor(y_train[index])#, dtype=torch.long)\n",
      "2it [00:00, 234.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x_numerical': tensor([[0.5425, 0.5371, 0.5234, 0.5342, 0.5244, 0.5269, 0.5244, 0.5259, 0.5181,\n",
      "         0.5127],\n",
      "        [0.5371, 0.5234, 0.5342, 0.5244, 0.5269, 0.5244, 0.5259, 0.5181, 0.5127,\n",
      "         0.5015],\n",
      "        [0.5234, 0.5342, 0.5244, 0.5269, 0.5244, 0.5259, 0.5181, 0.5127, 0.5015,\n",
      "         0.4897],\n",
      "        [0.5342, 0.5244, 0.5269, 0.5244, 0.5259, 0.5181, 0.5127, 0.5015, 0.4897,\n",
      "         0.4873],\n",
      "        [0.5244, 0.5269, 0.5244, 0.5259, 0.5181, 0.5127, 0.5015, 0.4897, 0.4873,\n",
      "         0.4719],\n",
      "        [0.5269, 0.5244, 0.5259, 0.5181, 0.5127, 0.5015, 0.4897, 0.4873, 0.4719,\n",
      "         0.4675],\n",
      "        [0.5244, 0.5259, 0.5181, 0.5127, 0.5015, 0.4897, 0.4873, 0.4719, 0.4675,\n",
      "         0.4663],\n",
      "        [0.5259, 0.5181, 0.5127, 0.5015, 0.4897, 0.4873, 0.4719, 0.4675, 0.4663,\n",
      "         0.4490],\n",
      "        [0.5181, 0.5127, 0.5015, 0.4897, 0.4873, 0.4719, 0.4675, 0.4663, 0.4490,\n",
      "         0.4409],\n",
      "        [0.5127, 0.5015, 0.4897, 0.4873, 0.4719, 0.4675, 0.4663, 0.4490, 0.4409,\n",
      "         0.4158],\n",
      "        [0.5015, 0.4897, 0.4873, 0.4719, 0.4675, 0.4663, 0.4490, 0.4409, 0.4158,\n",
      "         0.4167],\n",
      "        [0.4897, 0.4873, 0.4719, 0.4675, 0.4663, 0.4490, 0.4409, 0.4158, 0.4167,\n",
      "         0.4070],\n",
      "        [0.4873, 0.4719, 0.4675, 0.4663, 0.4490, 0.4409, 0.4158, 0.4167, 0.4070,\n",
      "         0.4373],\n",
      "        [0.4719, 0.4675, 0.4663, 0.4490, 0.4409, 0.4158, 0.4167, 0.4070, 0.4373,\n",
      "         0.4163],\n",
      "        [0.4675, 0.4663, 0.4490, 0.4409, 0.4158, 0.4167, 0.4070, 0.4373, 0.4163,\n",
      "         0.4033],\n",
      "        [0.4663, 0.4490, 0.4409, 0.4158, 0.4167, 0.4070, 0.4373, 0.4163, 0.4033,\n",
      "         0.4048]]), 'ids': tensor([], size=(16, 0), dtype=torch.int64), 'mask': tensor([], size=(16, 0), dtype=torch.int64), 'token_type_ids': tensor([], size=(16, 0), dtype=torch.int64), 'targets': tensor([[0.5015],\n",
      "        [0.4897],\n",
      "        [0.4873],\n",
      "        [0.4719],\n",
      "        [0.4675],\n",
      "        [0.4663],\n",
      "        [0.4490],\n",
      "        [0.4409],\n",
      "        [0.4158],\n",
      "        [0.4167],\n",
      "        [0.4070],\n",
      "        [0.4373],\n",
      "        [0.4163],\n",
      "        [0.4033],\n",
      "        [0.4048],\n",
      "        [0.4153]])}\n",
      "{'x_numerical': tensor([[0.4490, 0.4409, 0.4158, 0.4167, 0.4070, 0.4373, 0.4163, 0.4033, 0.4048,\n",
      "         0.4153],\n",
      "        [0.4409, 0.4158, 0.4167, 0.4070, 0.4373, 0.4163, 0.4033, 0.4048, 0.4153,\n",
      "         0.4114],\n",
      "        [0.4158, 0.4167, 0.4070, 0.4373, 0.4163, 0.4033, 0.4048, 0.4153, 0.4114,\n",
      "         0.4126],\n",
      "        [0.4167, 0.4070, 0.4373, 0.4163, 0.4033, 0.4048, 0.4153, 0.4114, 0.4126,\n",
      "         0.3987],\n",
      "        [0.4070, 0.4373, 0.4163, 0.4033, 0.4048, 0.4153, 0.4114, 0.4126, 0.3987,\n",
      "         0.3982],\n",
      "        [0.4373, 0.4163, 0.4033, 0.4048, 0.4153, 0.4114, 0.4126, 0.3987, 0.3982,\n",
      "         0.3999],\n",
      "        [0.4163, 0.4033, 0.4048, 0.4153, 0.4114, 0.4126, 0.3987, 0.3982, 0.3999,\n",
      "         0.3899],\n",
      "        [0.4033, 0.4048, 0.4153, 0.4114, 0.4126, 0.3987, 0.3982, 0.3999, 0.3899,\n",
      "         0.3635],\n",
      "        [0.4048, 0.4153, 0.4114, 0.4126, 0.3987, 0.3982, 0.3999, 0.3899, 0.3635,\n",
      "         0.3516],\n",
      "        [0.4153, 0.4114, 0.4126, 0.3987, 0.3982, 0.3999, 0.3899, 0.3635, 0.3516,\n",
      "         0.3210],\n",
      "        [0.4114, 0.4126, 0.3987, 0.3982, 0.3999, 0.3899, 0.3635, 0.3516, 0.3210,\n",
      "         0.2817],\n",
      "        [0.4126, 0.3987, 0.3982, 0.3999, 0.3899, 0.3635, 0.3516, 0.3210, 0.2817,\n",
      "         0.2408],\n",
      "        [0.3987, 0.3982, 0.3999, 0.3899, 0.3635, 0.3516, 0.3210, 0.2817, 0.2408,\n",
      "         0.2939],\n",
      "        [0.3982, 0.3999, 0.3899, 0.3635, 0.3516, 0.3210, 0.2817, 0.2408, 0.2939,\n",
      "         0.3127],\n",
      "        [0.3999, 0.3899, 0.3635, 0.3516, 0.3210, 0.2817, 0.2408, 0.2939, 0.3127,\n",
      "         0.2888],\n",
      "        [0.3899, 0.3635, 0.3516, 0.3210, 0.2817, 0.2408, 0.2939, 0.3127, 0.2888,\n",
      "         0.2637]]), 'ids': tensor([], size=(16, 0), dtype=torch.int64), 'mask': tensor([], size=(16, 0), dtype=torch.int64), 'token_type_ids': tensor([], size=(16, 0), dtype=torch.int64), 'targets': tensor([[0.4114],\n",
      "        [0.4126],\n",
      "        [0.3987],\n",
      "        [0.3982],\n",
      "        [0.3999],\n",
      "        [0.3899],\n",
      "        [0.3635],\n",
      "        [0.3516],\n",
      "        [0.3210],\n",
      "        [0.2817],\n",
      "        [0.2408],\n",
      "        [0.2939],\n",
      "        [0.3127],\n",
      "        [0.2888],\n",
      "        [0.2637],\n",
      "        [0.2418]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, data in tqdm(enumerate(train_loader, 0)):\n",
    "    if idx > 1:\n",
    "        break\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf87d32",
   "metadata": {
    "id": "bcf87d32"
   },
   "source": [
    "## Build model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mfKG9WSMQ74y",
   "metadata": {
    "id": "mfKG9WSMQ74y"
   },
   "source": [
    "### lstm1, lstm2, roberta, concat, lrelu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KJUOPHMMRVK2",
   "metadata": {
    "id": "KJUOPHMMRVK2"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SiameseModel11(nn.Module):\n",
    "    def __init__(self, input_dim1, input_dim2,\n",
    "                 hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4,\n",
    "                 num_layers1, num_layers2, output_dim1, output_dim2):\n",
    "        super(SiameseModel11, self).__init__()\n",
    "        self.input_dim1 = input_dim1\n",
    "        self.input_dim2 = input_dim2\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.hidden_dim3 = hidden_dim3\n",
    "        self.hidden_dim4 = hidden_dim4\n",
    "        self.num_layers1 = num_layers1\n",
    "        self.num_layers2 = num_layers2\n",
    "        self.output_dim1 = output_dim1\n",
    "        self.output_dim2 = output_dim2\n",
    "\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta-large\").to(device)\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_dim1, hidden_dim1, num_layers1, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_dim2, hidden_dim2, num_layers2, batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim1, output_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim2, output_dim2)\n",
    "        self.fc3 = nn.Linear(output_dim1+output_dim2, hidden_dim3)\n",
    "        self.fc4 = nn.Linear(hidden_dim3, 3)\n",
    "#         self.fc5 = nn.Linear(hidden_dim4, 3)\n",
    "        self.lrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x1, ids, masks, token_type_ids):\n",
    "        #left tower with numerical features\n",
    "        h_10 = Variable(torch.zeros(self.num_layers1, x1.size(0), self.hidden_dim1)).to(device)\n",
    "        c_10 = Variable(torch.zeros(self.num_layers1, x1.size(0), self.hidden_dim1)).to(device)\n",
    "        ula1, (h_out1, _) = self.lstm1(x1, (h_10, c_10))\n",
    "        h_out1 = h_out1.view(-1, self.hidden_dim1)\n",
    "        out1 = self.fc1(h_out1)\n",
    "\n",
    "\n",
    "        # right tower with roberta on textual features\n",
    "        batch_size_here = ids.shape[0]\n",
    "        e2 = torch.zeros(batch_size_here, max_text_per_iter,1024).to(device)\n",
    "\n",
    "        for k in range(ids.shape[1]):\n",
    "            seq_ids = ids[:,k,:]\n",
    "            seq_masks = masks[:,k,:]\n",
    "            seq_token_type_ids = token_type_ids[:,k,:]\n",
    "\n",
    "            e2k = self.roberta(input_ids= seq_ids, attention_mask=seq_masks, token_type_ids=seq_token_type_ids)\n",
    "            # first 0 is for last_hidden_state: https://huggingface.co/docs/transformers/main/en/model_doc/roberta#transformers.RobertaModel.forward.returns:~:text=transformers.modeling_outputs.-,BaseModelOutputWithPoolingAndCrossAttentions%20or%20tuple(torch.FloatTensor),-A%20transformers.modeling_outputs\n",
    "            # the shape of e2k[0] is (batch_size, sequence_length (<=MAX_LEN), hidden_size (=1024))\n",
    "            e2k1 = e2k[0][:, 0, :]\n",
    "            e2[:,k,:] = e2k1\n",
    "\n",
    "\n",
    "        print(e2.shape)\n",
    "        h_20 = Variable(torch.zeros(self.num_layers2, e2.size(0), self.hidden_dim2)).to(device)\n",
    "        c_20 = Variable(torch.zeros(self.num_layers2, e2.size(0), self.hidden_dim2)).to(device)\n",
    "        ula2, (h_out2, _) = self.lstm2(e2, (h_20, c_20))\n",
    "        h_out2 = h_out2.view(-1, self.hidden_dim2)\n",
    "        out2 = self.fc2(h_out2)\n",
    "\n",
    "        # siamese merging layers\n",
    "        output = torch.cat((out1, out2),1)\n",
    "        output = self.lrelu(self.fc3(output))\n",
    "        output = self.fc4(output)\n",
    "\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vMwCdhg6Qtzj",
   "metadata": {
    "id": "vMwCdhg6Qtzj"
   },
   "source": [
    "### fcn1, fcn2, roberta, concat, lrelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s8ktZOz3QykD",
   "metadata": {
    "id": "s8ktZOz3QykD"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SiameseModel11_fcn(nn.Module):\n",
    "    def __init__(self, input_dim1, input_dim2,\n",
    "                 hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4, hidden_dim5, hidden_dim6):\n",
    "        super(SiameseModel11_fcn, self).__init__()\n",
    "        self.input_dim1 = input_dim1\n",
    "        self.input_dim2 = input_dim2\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.hidden_dim3 = hidden_dim3\n",
    "        self.hidden_dim4 = hidden_dim4\n",
    "        self.hidden_dim5 = hidden_dim5\n",
    "        self.hidden_dim6 = hidden_dim6\n",
    "\n",
    "\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta-large\").to(device)\n",
    "\n",
    "\n",
    "        self.fc11 = nn.Linear(input_dim1, hidden_dim1)\n",
    "        self.fc12 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "\n",
    "        self.fc21 = nn.Linear(input_dim2, hidden_dim3)\n",
    "        self.fc22 = nn.Linear(hidden_dim3, hidden_dim4)\n",
    "\n",
    "        self.fc31 = nn.Linear(hidden_dim2+hidden_dim4, hidden_dim5)\n",
    "        self.fc32 = nn.Linear(hidden_dim5, hidden_dim6)\n",
    "        self.fc33 = nn.Linear(hidden_dim6, 3)\n",
    "        self.lrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x1, ids, masks, token_type_ids):\n",
    "        #left tower with numerical features\n",
    "\n",
    "        batch_size, seq_len, embedding_size = x1.size()\n",
    "        x1 = torch.reshape(x1, (batch_size, seq_len*embedding_size))\n",
    "\n",
    "        out1 = self.fc11(x1)\n",
    "        print('out1 shape:', out1.shape)\n",
    "\n",
    "        out1 = self.fc12(out1)\n",
    "        print('out1 shape:', out1.shape)\n",
    "\n",
    "        # right tower with roberta on textual features\n",
    "        batch_size_here = ids.shape[0]\n",
    "        e2 = torch.zeros(batch_size_here, max_text_per_iter,1024).to(device)\n",
    "\n",
    "        for k in range(ids.shape[1]):\n",
    "            seq_ids = ids[:,k,:]\n",
    "            seq_masks = masks[:,k,:]\n",
    "            seq_token_type_ids = token_type_ids[:,k,:]\n",
    "\n",
    "            e2k = self.roberta(input_ids= seq_ids, attention_mask=seq_masks, token_type_ids=seq_token_type_ids)\n",
    "            # first 0 is for last_hidden_state: https://huggingface.co/docs/transformers/main/en/model_doc/roberta#transformers.RobertaModel.forward.returns:~:text=transformers.modeling_outputs.-,BaseModelOutputWithPoolingAndCrossAttentions%20or%20tuple(torch.FloatTensor),-A%20transformers.modeling_outputs\n",
    "            # the shape of e2k[0] is (batch_size, sequence_length (<=MAX_LEN), hidden_size (=1024))\n",
    "            e2k1 = e2k[0][:, 0, :]\n",
    "            e2[:,k,:] = e2k1\n",
    "\n",
    "        print(e2.shape)\n",
    "        batch_size, seq_len, embedding_size = e2.size()\n",
    "        e2 = torch.reshape(e2, (batch_size, seq_len*embedding_size))\n",
    "        print(e2.shape)\n",
    "\n",
    "        out2 = self.fc21(e2)\n",
    "        print('out2 shape:', out2.shape)\n",
    "        out2 = self.fc22(out2)\n",
    "        print('out2 shape:', out2.shape)\n",
    "\n",
    "\n",
    "        # siamese merging layers\n",
    "        output = torch.cat((out1, out2),1)\n",
    "        output = self.lrelu(self.fc31(output))\n",
    "        output = self.lrelu(self.fc32(output))\n",
    "        output = self.fc33(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maQVSQvVR6CP",
   "metadata": {
    "id": "maQVSQvVR6CP"
   },
   "source": [
    "### lstm1, mlp, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Yc1ZHghiRKPr",
   "metadata": {
    "id": "Yc1ZHghiRKPr"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SiameseModel10(nn.Module):\n",
    "    def __init__(self, input_dim1, input_dim2,\n",
    "                 hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4,\n",
    "                 num_layers1, num_layers2, output_dim1, output_dim2):\n",
    "        super(SiameseModel10, self).__init__()\n",
    "        self.input_dim1 = input_dim1\n",
    "        self.input_dim2 = input_dim2\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.hidden_dim3 = hidden_dim3\n",
    "        self.hidden_dim4 = hidden_dim4\n",
    "        self.num_layers1 = num_layers1\n",
    "        self.num_layers2 = num_layers2\n",
    "        self.output_dim1 = output_dim1\n",
    "        self.output_dim2 = output_dim2\n",
    "\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_dim1, hidden_dim1, num_layers1, batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim1, output_dim1)\n",
    "        self.fc3 = nn.Linear(output_dim1, hidden_dim3)\n",
    "        self.fc4 = nn.Linear(hidden_dim3, hidden_dim4)\n",
    "        self.fc5 = nn.Linear(hidden_dim4, 3)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x1, ids, masks, token_type_ids):\n",
    "        #left tower with numerical features\n",
    "        h_10 = Variable(torch.zeros(self.num_layers1, x1.size(0), self.hidden_dim1)).to(device)\n",
    "        c_10 = Variable(torch.zeros(self.num_layers1, x1.size(0), self.hidden_dim1)).to(device)\n",
    "        ula1, (h_out1, _) = self.lstm1(x1, (h_10, c_10))\n",
    "        h_out1 = h_out1.view(-1, self.hidden_dim1)\n",
    "        out1 = self.fc1(h_out1)\n",
    "\n",
    "\n",
    "        # mlp layers\n",
    "        output = out1\n",
    "        output = F.relu(self.fc3(output))\n",
    "        output = F.relu(self.fc4(output))\n",
    "        output = self.fc5(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcRte1yrTlYo",
   "metadata": {
    "id": "dcRte1yrTlYo"
   },
   "source": [
    "### lstm1, mlp, lrelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7Sm6o_cITpaX",
   "metadata": {
    "id": "7Sm6o_cITpaX"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SiameseModel10leaky(nn.Module):\n",
    "    def __init__(self, input_dim1,\n",
    "                 hidden_dim1, hidden_dim3, hidden_dim4,\n",
    "                 num_layers1, output_dim1):\n",
    "        super(SiameseModel10leaky, self).__init__()\n",
    "        self.input_dim1 = input_dim1\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim3 = hidden_dim3\n",
    "        self.hidden_dim4 = hidden_dim4\n",
    "        self.num_layers1 = num_layers1\n",
    "        self.output_dim1 = output_dim1\n",
    "\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_dim1, hidden_dim1, num_layers1, batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim1, output_dim1)\n",
    "        self.fc3 = nn.Linear(output_dim1, hidden_dim3)\n",
    "        # self.fc4 = nn.Linear(hidden_dim3, hidden_dim4)\n",
    "        self.fc5 = nn.Linear(hidden_dim3, 1)\n",
    "        self.lrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x1, ids, masks, token_type_ids):\n",
    "        #left tower with numerical features\n",
    "        h_10 = Variable(torch.zeros(self.num_layers1, x1.size(0), self.hidden_dim1)).to(device)\n",
    "        c_10 = Variable(torch.zeros(self.num_layers1, x1.size(0), self.hidden_dim1)).to(device)\n",
    "        if (x1.dim() == 2):\n",
    "            x1 = torch.unsqueeze(x1, 2)\n",
    "        ula1, (h_out1, _) = self.lstm1(x1, (h_10, c_10))\n",
    "        h_out1 = h_out1.view(-1, self.hidden_dim1)\n",
    "        out1 = self.fc1(h_out1)\n",
    "\n",
    "\n",
    "        # mlp layers\n",
    "        output = out1\n",
    "        output = self.lrelu(self.fc3(output))\n",
    "        # output = self.lrelu(self.fc4(output))\n",
    "        output = self.fc5(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vo098_3sU_Hk",
   "metadata": {
    "id": "Vo098_3sU_Hk"
   },
   "source": [
    "### tranformer, mlp, relu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4e33819",
   "metadata": {
    "id": "e4e33819"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=MAX_LEN):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        #pe.requires_grad = False\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "\n",
    "class SiameseModel20(nn.Module):\n",
    "    def __init__(self, input_dim1,\n",
    "                 hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4,\n",
    "                 num_layers1,  output_dim1,\n",
    "                 nhead, d_transformer, dropout=0.2 ):\n",
    "        super(SiameseModel20, self).__init__()\n",
    "\n",
    "        self.input_dim1 = input_dim1\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.hidden_dim3 = hidden_dim3\n",
    "        self.hidden_dim4 = hidden_dim4\n",
    "        self.num_layers1 = num_layers1\n",
    "        self.output_dim1 = output_dim1\n",
    "        self.lrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "        self.input_embedding  = nn.Linear(input_dim1, d_transformer)\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(d_model=d_transformer)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_transformer,\n",
    "                                                        nhead=nhead,\n",
    "                                                        dropout=dropout,\n",
    "                                                        batch_first=True,\n",
    "                                                        dim_feedforward = 24)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers1)\n",
    "\n",
    "\n",
    "        # self.lstm1 = nn.LSTM(input_dim1, hidden_dim1, num_layers1, batch_first=True)\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(5*d_transformer, output_dim1)\n",
    "        self.fc3 = nn.Linear(output_dim1, hidden_dim3)\n",
    "        self.fc4 = nn.Linear(hidden_dim3, hidden_dim4)\n",
    "        self.fc5 = nn.Linear(hidden_dim4, 3)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x1, ids, masks, token_type_ids):\n",
    "        #left tower with numerical features\n",
    "        # h_10 = Variable(torch.zeros(self.num_layers1, x1.size(0), self.hidden_dim1)).to(device)\n",
    "        # c_10 = Variable(torch.zeros(self.num_layers1, x1.size(0), self.hidden_dim1)).to(device)\n",
    "        # ula1, (h_out1, _) = self.lstm1(x1, (h_10, c_10))\n",
    "        # h_out1 = h_out1.view(-1, self.hidden_dim1)\n",
    "        # out1 = self.fc1(h_out1)\n",
    "\n",
    "        #transformer encoder\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(x1):\n",
    "            device = x1.device\n",
    "            mask = self._generate_square_subsequent_mask(len(x1)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        x1 = self.input_embedding(x1) # linear transformation before positional embedding\n",
    "\n",
    "        x1 = self.pos_encoder(x1)\n",
    "        output = self.transformer_encoder(x1, self.src_mask)#, self.src_mask)\n",
    "        output = torch.flatten(output, start_dim=1)\n",
    "        # output = output.view(-1, self.hidden_dim1)\n",
    "\n",
    "        out1 = self.fc1(output)\n",
    "\n",
    "\n",
    "        # mlp  layers\n",
    "        output = out1\n",
    "        output = self.lrelu(self.fc3(output))\n",
    "        output = self.lrelu(self.fc4(output))\n",
    "        output = self.fc5(output)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p8paRxDJVjWE",
   "metadata": {
    "id": "p8paRxDJVjWE"
   },
   "source": [
    "### Choose a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33bdcfab",
   "metadata": {
    "id": "33bdcfab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSiameseModel10\\nSiameseModel10leaky\\nSiameseModel20\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = SiameseModel11(input_dim1 = 8, input_dim2 = 1024,\n",
    "#                        hidden_dim1 = 20, hidden_dim2 = 768, output_dim1 = 10, output_dim2 = 256,\n",
    "#                        hidden_dim3 = 10, hidden_dim4 = 8,\n",
    "#                        num_layers1 = 1, num_layers2 = 1, ).to(device)\n",
    "\n",
    "# model = SiameseModel11_fcn(input_dim1 = 8*5, input_dim2 = 1024*20,\n",
    "#                  hidden_dim1 = 20, hidden_dim2 = 10, hidden_dim3 = 768, hidden_dim4 = 256, hidden_dim5 = 128, hidden_dim6 = 64 ).to(device)\n",
    "\n",
    "model = SiameseModel10leaky(input_dim1 = 1,\n",
    "                 hidden_dim1 = 20, output_dim1 = 10, hidden_dim3 = 10, hidden_dim4 = 8,\n",
    "                 num_layers1 = 1).to(device)\n",
    "\n",
    "# model = SiameseModel20(input_dim1 = 8,\n",
    "#                  hidden_dim1 = 20, hidden_dim2 = 768, hidden_dim3 = 10, hidden_dim4 = 8,\n",
    "#                  num_layers1 = 1, output_dim1 = 10,  nhead = 4, d_transformer = 20, dropout = 0.2).to(device)\n",
    "\n",
    "# model = SiameseModel10(input_dim1 = 8, input_dim2 = 1024,\n",
    "#                  hidden_dim1 = 20, hidden_dim2 = 768, hidden_dim3 = 10, hidden_dim4 = 8,\n",
    "#                  num_layers1 = 1, num_layers2 = 1, output_dim1 = 10, output_dim2 = 256).to(device)\n",
    "# for param in model.roberta.parameters():\n",
    "#     param.requires_grad = False\n",
    "# print(model)\n",
    "\n",
    "'''\n",
    "SiameseModel10\n",
    "SiameseModel10leaky\n",
    "SiameseModel20\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d36SUCJmNGd",
   "metadata": {
    "id": "7d36SUCJmNGd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8569, 0.8774, 0.9033, 0.9189, 0.9048, 0.8989, 0.8989, 0.8833, 0.8267,\n",
      "         0.8506],\n",
      "        [0.8774, 0.9033, 0.9189, 0.9048, 0.8989, 0.8989, 0.8833, 0.8267, 0.8506,\n",
      "         0.8623],\n",
      "        [0.9033, 0.9189, 0.9048, 0.8989, 0.8989, 0.8833, 0.8267, 0.8506, 0.8623,\n",
      "         0.8433],\n",
      "        [0.9189, 0.9048, 0.8989, 0.8989, 0.8833, 0.8267, 0.8506, 0.8623, 0.8433,\n",
      "         0.7783],\n",
      "        [0.9048, 0.8989, 0.8989, 0.8833, 0.8267, 0.8506, 0.8623, 0.8433, 0.7783,\n",
      "         0.7422],\n",
      "        [0.8989, 0.8989, 0.8833, 0.8267, 0.8506, 0.8623, 0.8433, 0.7783, 0.7422,\n",
      "         0.7520]], device='cuda:0')\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(x_numerical)#.dim()\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "BhZdTSfZmiVj",
   "metadata": {
    "id": "BhZdTSfZmiVj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2588],\n",
      "        [0.2588],\n",
      "        [0.2588],\n",
      "        [0.2589],\n",
      "        [0.2589],\n",
      "        [0.2589]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "y_pred = model(x_numerical, ids, masks, token_type_ids)\n",
    "print(y_pred)\n",
    "print(y_pred.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FnrD0WkxQr4U",
   "metadata": {
    "id": "FnrD0WkxQr4U"
   },
   "outputs": [],
   "source": [
    "x1 = x_numerical\n",
    "batch_size, seq_len, embedding_size = x1.size()\n",
    "\n",
    "x1 = torch.reshape(x1, (batch_size, seq_len*embedding_size))\n",
    "print(x1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7tT0kAmpY015",
   "metadata": {
    "id": "7tT0kAmpY015"
   },
   "outputs": [],
   "source": [
    "torch.flatten(y_pred, start_dim=1).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f5b46a",
   "metadata": {
    "id": "05f5b46a"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "088c2407",
   "metadata": {
    "id": "088c2407"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss_arr = np.zeros(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "WZI4ATJNYcfp",
   "metadata": {
    "id": "WZI4ATJNYcfp"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2u6boszt) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "977b6120c4914ab49311ef393822bfce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg test loss in this batch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>avg test loss in this epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>avg train loss in this batch</td><td>▇▁▆▂▃▂▁▂▆▄▂▅▃▄▅▁▅▄▅▇▅▆▅▃█▄▁▃▇▃▁▅▂▅▂▂▆▇▄▅</td></tr><tr><td>avg train loss in this epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch_id</td><td>▇▄█▃▇▃▆▄▇▃▇▂▆▁▇▃▆▂▅▁▇▃▆▂▅▁▄▂▆▁▅▁▄▂▅▁▅█▄▇</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>gradient_fc1.weight</td><td>▆▆▃▅▅▆▆▆▆▆▆▆▆▇▇▅███▆▆▂▃▁▆▆▆▅▆▅▅▆▆▆▆▆▆▆▇▇</td></tr><tr><td>gradient_fc3.weight</td><td>██▆████████████▆███▇▇▁▃▁▇██▇████████████</td></tr><tr><td>gradient_fc5.weight</td><td>██▅▇▇██████████▅▇▆▆▇▇▄▅▁▇██▇█▇▇█████████</td></tr><tr><td>gradient_lstm1.weight_hh_l0</td><td>▇▆▄▅▅▆▆▆▇▇▇▇▇▇█▅██▇▆▆▂▃▁▆▇▆▅▆▅▅▆▆▇▆▇▇▇▇█</td></tr><tr><td>gradient_lstm1.weight_ih_l0</td><td>█▇▆▆▆▅▄▄▄▃▃▃▃▃▂▁▁▁▂▃▅▆▆▆▆█▇▇▇▆▅▅▄▄▃▃▃▄▃▂</td></tr><tr><td>test accuracy in this epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train accuracy in this epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg test loss in this batch</td><td>1.08203</td></tr><tr><td>avg test loss in this epoch</td><td>1.10456</td></tr><tr><td>avg train loss in this batch</td><td>1.1002</td></tr><tr><td>avg train loss in this epoch</td><td>1.10055</td></tr><tr><td>batch_id</td><td>3</td></tr><tr><td>epoch</td><td>199</td></tr><tr><td>gradient_fc1.weight</td><td>0.00321</td></tr><tr><td>gradient_fc3.weight</td><td>0.01422</td></tr><tr><td>gradient_fc5.weight</td><td>0.06527</td></tr><tr><td>gradient_lstm1.weight_hh_l0</td><td>0.00019</td></tr><tr><td>gradient_lstm1.weight_ih_l0</td><td>0.00244</td></tr><tr><td>test accuracy in this epoch</td><td>25.0</td></tr><tr><td>train accuracy in this epoch</td><td>25.36873</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">solar-vortex-51</strong> at: <a href='https://wandb.ai/visriv/stock_prediction/runs/2u6boszt' target=\"_blank\">https://wandb.ai/visriv/stock_prediction/runs/2u6boszt</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230720_040021-2u6boszt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2u6boszt). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61c531b8db645afb3b72b961f8fbf47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668580900053107, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ec2-user/SageMaker/wandb/run-20230720_044221-8rgnzgeb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/visriv/stock_prediction/runs/8rgnzgeb' target=\"_blank\">toasty-serenity-52</a></strong> to <a href='https://wandb.ai/visriv/stock_prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/visriv/stock_prediction' target=\"_blank\">https://wandb.ai/visriv/stock_prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/visriv/stock_prediction/runs/8rgnzgeb' target=\"_blank\">https://wandb.ai/visriv/stock_prediction/runs/8rgnzgeb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/visriv/stock_prediction/runs/8rgnzgeb?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f96e8cb7af0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"stock_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "699d13c0",
   "metadata": {
    "id": "699d13c0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/tmp/ipykernel_11411/2984344807.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_11411/2984344807.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'mask': torch.tensor(attention_masks, dtype=torch.long),\n",
      "/tmp/ipykernel_11411/2984344807.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_11411/2984344807.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'targets': torch.tensor(y_train[index])#, dtype=torch.long)\n",
      "41it [00:00, 200.92it/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "43it [00:00, 199.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 0: 0.08906666934490204\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "4it [00:00, 444.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 0: 0.06914562731981277\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 202.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 1: 0.08517821133136749\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 492.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 1: 0.07575760036706924\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 200.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 2: 0.08112531155347824\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 519.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 2: 0.08300858736038208\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 202.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 3: 0.07793831080198288\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 540.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 3: 0.08961793035268784\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 201.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 4: 0.07527755945920944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 469.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 4: 0.09581266343593597\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 203.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 5: 0.07295659184455872\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 466.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 5: 0.10172738879919052\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 201.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 6: 0.07088394463062286\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 504.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 6: 0.10742282122373581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 199.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 7: 0.06900279223918915\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 470.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 7: 0.11292397975921631\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 201.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 8: 0.06727335602045059\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 458.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 8: 0.11823946982622147\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 201.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 9: 0.06566325575113297\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 452.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 9: 0.12337059527635574\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 199.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 10: 0.06414362788200378\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 446.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 10: 0.1283147782087326\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 201.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 11: 0.0626915842294693\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 457.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 11: 0.1330544352531433\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 204.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 12: 0.06128391996026039\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 535.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 12: 0.13756808638572693\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 200.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 13: 0.059896320104599\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 429.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 13: 0.1418313980102539\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 201.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 14: 0.05851409584283829\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 488.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 14: 0.1457800567150116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 199.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 15: 0.05712040513753891\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 469.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 15: 0.14932745695114136\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 203.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 16: 0.05566241592168808\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 438.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 16: 0.15249201655387878\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 207.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 17: 0.05411061644554138\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 479.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 17: 0.15524688363075256\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 202.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 18: 0.05242880433797836\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 459.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 18: 0.15755760669708252\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 203.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 19: 0.05057661607861519\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 503.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 19: 0.1593780517578125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 213.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 20: 0.04850761219859123\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 548.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 20: 0.16065052151679993\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 216.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 21: 0.046167995780706406\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 534.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 21: 0.16130627691745758\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 228.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 22: 0.04349660128355026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 438.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 22: 0.16125960648059845\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 232.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 23: 0.040425486862659454\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 553.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 23: 0.16040287911891937\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 230.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 24: 0.03688604012131691\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 542.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 24: 0.158596009016037\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 240.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 25: 0.0328228622674942\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 474.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 25: 0.15568546950817108\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 233.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 26: 0.02821452170610428\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 578.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 26: 0.1515347957611084\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 231.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 27: 0.02314550243318081\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 582.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 27: 0.14600512385368347\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 225.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 28: 0.01791302114725113\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 417.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 28: 0.14213642477989197\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 219.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 29: 0.013536285609006882\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 452.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 29: 0.13597574830055237\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 230.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 30: 0.009692131541669369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 601.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 30: 0.12770134210586548\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 235.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 31: 0.006738442927598953\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 562.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 31: 0.12173301726579666\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 231.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 32: 0.0049725123681128025\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 535.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 32: 0.11678452044725418\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 211.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 33: 0.003994944971054792\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 551.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 33: 0.11400976032018661\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 220.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 34: 0.0035084718838334084\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 528.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 34: 0.11290203034877777\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 226.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 35: 0.003237058874219656\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 537.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 35: 0.11266990751028061\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 228.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 36: 0.0030695677269250154\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 534.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 36: 0.1126408725976944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 231.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 37: 0.002948782406747341\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 425.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 37: 0.11265666037797928\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 224.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 38: 0.002857473911717534\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 419.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 38: 0.11265929043292999\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 226.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 39: 0.002783403964713216\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 572.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 39: 0.11274265497922897\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 223.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 40: 0.0027298384811729193\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 482.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 40: 0.11274770647287369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 234.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 41: 0.0026890637818723917\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 447.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 41: 0.11280050128698349\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 199.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 42: 0.0026591909117996693\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 432.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 42: 0.11280573159456253\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 215.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 43: 0.0026358691975474358\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 559.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 43: 0.11281397938728333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 228.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 44: 0.0026174827944487333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 541.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 44: 0.11284463107585907\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 233.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 45: 0.002603453118354082\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 598.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 45: 0.11286721378564835\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 219.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 46: 0.002592119388282299\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 474.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 46: 0.11286596953868866\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 229.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 47: 0.002583005465567112\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 483.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 47: 0.11288765072822571\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 234.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 48: 0.0025751369539648294\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [00:00, 568.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 48: 0.11289317160844803\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 229.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 49: 0.0025682877749204636\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 439.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 49: 0.11290929466485977\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 226.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 50: 0.0025626036804169416\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 518.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 50: 0.11290940642356873\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 233.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 51: 0.002557779662311077\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 512.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 51: 0.11289982497692108\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 229.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 52: 0.0025530443526804447\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 570.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 52: 0.11289019882678986\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 232.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 53: 0.002549725817516446\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 503.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 53: 0.11289100348949432\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 218.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 54: 0.002546305302530527\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 536.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 54: 0.11288360506296158\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 228.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 55: 0.0025430379901081324\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 544.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 55: 0.11286907643079758\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 228.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 56: 0.002539686392992735\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 564.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 56: 0.11285202950239182\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 234.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 57: 0.002536801155656576\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 576.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 57: 0.11283095926046371\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 224.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 58: 0.0025335936807096004\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 481.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 58: 0.11281169205904007\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 234.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 59: 0.002530822530388832\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 589.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 59: 0.11279039829969406\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 227.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 60: 0.002527739852666855\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 509.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 60: 0.11277081072330475\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 225.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 61: 0.002525045769289136\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 454.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 61: 0.1127239242196083\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 235.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 62: 0.0025219665840268135\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 492.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 62: 0.11270622164011002\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 219.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 63: 0.002519435714930296\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 550.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 63: 0.11268529295921326\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 219.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 64: 0.002517047571018338\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 470.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 64: 0.11263565719127655\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 228.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 65: 0.0025142142549157143\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 443.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 65: 0.11261679232120514\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 223.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 66: 0.002511918079108\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 492.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 66: 0.11256930977106094\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 233.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 67: 0.00250925961881876\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 514.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 67: 0.11255192011594772\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 229.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 68: 0.0025070796255022287\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 553.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 68: 0.11253312975168228\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 215.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 69: 0.002505445620045066\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 577.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 69: 0.11242181807756424\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 242.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 70: 0.0025019552558660507\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 450.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 70: 0.11237703263759613\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 235.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 71: 0.0024995815474539995\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 508.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 71: 0.11236267536878586\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 230.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 72: 0.002497725421562791\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 567.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 72: 0.11234237998723984\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 224.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 73: 0.0024957815185189247\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 477.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 73: 0.11232449114322662\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 227.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 74: 0.0024939156137406826\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 489.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 74: 0.11230563372373581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 228.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 75: 0.0024920543655753136\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 515.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 75: 0.11228837072849274\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 222.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 76: 0.0024902368895709515\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 557.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 76: 0.11226966232061386\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 224.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 77: 0.002488431753590703\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 514.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 77: 0.11225241422653198\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 225.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 78: 0.0024866568855941296\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 558.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 78: 0.11223609745502472\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 222.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 79: 0.0024850128684192896\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 516.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 79: 0.11222073435783386\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 229.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 80: 0.002483345801010728\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 481.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 80: 0.11220294237136841\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 225.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 81: 0.0024818223901093006\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 464.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 81: 0.11215287446975708\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 228.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 82: 0.00247946591116488\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 428.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 82: 0.11214780062437057\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 229.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 83: 0.0024781501851975918\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 568.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 83: 0.1121319830417633\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 221.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 84: 0.0024766551796346903\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 442.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 84: 0.11211109161376953\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 216.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 85: 0.0024752349127084017\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 445.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 85: 0.11206148564815521\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 226.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 86: 0.002473254455253482\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 471.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 86: 0.11204400658607483\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 226.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 87: 0.0024716341868042946\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 524.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 87: 0.11202516406774521\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 232.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 88: 0.0024699794594198465\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 519.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 88: 0.11200807243585587\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 232.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 89: 0.0024682863149791956\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 590.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 89: 0.11200500279664993\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 224.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 90: 0.002466494683176279\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 465.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 90: 0.11198877543210983\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 222.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 91: 0.0024650872219353914\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 562.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 91: 0.11196763068437576\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 226.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 92: 0.0024634019937366247\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 438.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 92: 0.11194848269224167\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 204.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 93: 0.002461866708472371\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 484.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 93: 0.11192863434553146\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 229.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 94: 0.002460267161950469\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 480.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 94: 0.11191071569919586\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 232.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 95: 0.0024587931111454964\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 558.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 95: 0.11189049482345581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 228.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 96: 0.002456983318552375\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 574.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 96: 0.11188916116952896\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 222.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 97: 0.0024559067096561193\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 574.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 97: 0.11186930537223816\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 220.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 98: 0.0024546219501644373\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 558.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 98: 0.11184784024953842\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 227.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 99: 0.0024531385861337185\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 513.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 99: 0.11182622611522675\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 223.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 100: 0.0024517031852155924\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 543.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 100: 0.1118057519197464\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 224.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 101: 0.0024499574210494757\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 567.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 101: 0.11180528253316879\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 236.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 102: 0.0024487171322107315\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [00:00, 583.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 102: 0.11180168390274048\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 235.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 103: 0.0024476302787661552\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 471.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 103: 0.11177469789981842\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 220.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 104: 0.002446010010316968\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 598.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 104: 0.11175018548965454\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 236.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 105: 0.002444291254505515\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 592.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 105: 0.11174563318490982\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 216.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 106: 0.002442882629111409\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 525.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 106: 0.11173136532306671\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 226.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 107: 0.002440571552142501\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 507.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 107: 0.11169490218162537\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 233.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 108: 0.002438608091324568\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 549.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 108: 0.11167716234922409\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 226.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 109: 0.0024371077306568623\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 467.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 109: 0.11165708303451538\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 232.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 110: 0.0024357482325285673\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 502.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 110: 0.11163608729839325\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 229.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 111: 0.0024341396056115627\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 537.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 111: 0.11161291599273682\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 224.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 112: 0.0024326995480805635\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 467.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 112: 0.11159169673919678\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 235.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 113: 0.002431012922897935\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 588.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 113: 0.11160609126091003\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 225.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 114: 0.0024304583203047514\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 464.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 114: 0.11159085482358932\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 231.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 115: 0.002429125364869833\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 537.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 115: 0.11156467348337173\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 211.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 116: 0.0024277386255562305\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 573.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 116: 0.1115289106965065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 239.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 117: 0.0024260913487523794\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 438.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 117: 0.11150594800710678\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 228.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 118: 0.0024247881956398487\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 442.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 118: 0.11148537695407867\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 224.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 119: 0.002423370024189353\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 537.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 119: 0.11146969348192215\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 230.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 120: 0.002422252669930458\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 484.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 120: 0.11144780367612839\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 230.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 121: 0.002420992823317647\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 531.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 121: 0.11142431199550629\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 223.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 122: 0.0024195595178753138\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 514.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 122: 0.11140933632850647\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 224.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 123: 0.002418503165245056\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 455.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 123: 0.1113872304558754\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 221.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 124: 0.00241727358661592\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 564.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 124: 0.11136318743228912\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 230.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 125: 0.0024160423781722784\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 511.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 125: 0.11134104430675507\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 236.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 126: 0.0024148141965270042\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 434.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 126: 0.1113261878490448\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 209.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 127: 0.002414162503555417\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 437.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 127: 0.1112983226776123\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 224.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 128: 0.002412923611700535\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 498.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 128: 0.11127384006977081\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 231.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 129: 0.0024117245338857174\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 464.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 129: 0.1112518310546875\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 235.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 130: 0.0024106381461024284\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 586.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 130: 0.11122903227806091\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 226.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 131: 0.002409482840448618\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 482.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 131: 0.11122208833694458\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 226.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 132: 0.0024086269550025463\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 548.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 132: 0.11119940131902695\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 222.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 133: 0.0024077827110886574\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 583.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 133: 0.11117205768823624\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 222.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 134: 0.002406600397080183\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 523.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 134: 0.11114992201328278\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 228.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 135: 0.0024057948030531406\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 484.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 135: 0.11109881848096848\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 226.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 136: 0.002404013415798545\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 504.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 136: 0.1111057698726654\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 226.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 137: 0.0024038469418883324\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 440.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 137: 0.11105744540691376\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 222.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 138: 0.002402346581220627\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 516.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 138: 0.11105703562498093\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 212.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 139: 0.0024017628747969866\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 502.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 139: 0.11103926599025726\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 236.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 140: 0.002401071134954691\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 463.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 140: 0.11098582297563553\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 229.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 141: 0.0023994932416826487\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 549.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 141: 0.110963374376297\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 222.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 142: 0.0023983900900930166\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 571.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 142: 0.11097491532564163\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 234.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 143: 0.0023981458507478237\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 568.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 143: 0.11095407605171204\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 227.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 144: 0.002397410571575165\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 465.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 144: 0.11092592775821686\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 233.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 145: 0.0023965192958712578\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 550.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 145: 0.11087680608034134\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 223.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 146: 0.002394895302131772\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 520.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 146: 0.11088526248931885\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 224.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 147: 0.002394595416262746\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 479.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 147: 0.11083798855543137\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 223.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 148: 0.002393231028690934\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 309.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 148: 0.11081631481647491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 215.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 149: 0.002392220078036189\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 519.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 149: 0.110826276242733\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 231.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 150: 0.0023920119274407625\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 481.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 150: 0.11080551892518997\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 224.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 151: 0.0023911253083497286\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 521.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 151: 0.11078189313411713\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 228.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 152: 0.0023904137779027224\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 563.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 152: 0.11075786501169205\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 227.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 153: 0.0023894794285297394\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 441.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 153: 0.11074118316173553\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 226.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 154: 0.0023887234274297953\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 458.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 154: 0.11072103679180145\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 237.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 155: 0.0023881071247160435\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 567.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 155: 0.11067090183496475\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 227.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 156: 0.00238647498190403\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 426.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 156: 0.11067909002304077\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 224.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 157: 0.0023862142115831375\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 505.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 157: 0.11066285520792007\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 222.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 158: 0.002385438419878483\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 443.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 158: 0.1106393039226532\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 233.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 159: 0.0023845515679568052\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 571.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 159: 0.11062157154083252\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 223.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 160: 0.002383878454566002\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 551.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 160: 0.1106017678976059\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 216.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 161: 0.0023830567952245474\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 547.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 161: 0.1105809435248375\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 212.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 162: 0.0023822402581572533\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 532.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 162: 0.11056089401245117\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 219.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 163: 0.0023816898465156555\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 537.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 163: 0.11051300913095474\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 247.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 164: 0.002380277030169964\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 605.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 164: 0.11051633954048157\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 226.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 165: 0.0023800174240022898\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 496.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 165: 0.11049935966730118\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 224.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 166: 0.0023792667780071497\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 418.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 166: 0.11047634482383728\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 227.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 167: 0.0023784267250448465\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 533.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 167: 0.11045672744512558\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 225.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 168: 0.0023777401074767113\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 536.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 168: 0.1104326993227005\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 223.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 169: 0.0023770539555698633\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 430.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 169: 0.11038290709257126\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 200.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 170: 0.002375456504523754\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 443.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 170: 0.1103917583823204\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 214.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 171: 0.0023752874694764614\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 444.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 171: 0.110375314950943\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 225.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 172: 0.0023747659288346767\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 559.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 172: 0.11032335460186005\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 226.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 173: 0.002373119816184044\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 465.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 173: 0.11033188551664352\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 230.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 174: 0.0023729989770799875\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 612.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 174: 0.1103176474571228\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 227.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 175: 0.00237241480499506\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 537.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 175: 0.11029689759016037\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 229.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 176: 0.0023717964068055153\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 578.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 176: 0.1102733165025711\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 225.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 177: 0.002370955189689994\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 403.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 177: 0.11025286465883255\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 229.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 178: 0.0023701058235019445\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 547.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 178: 0.11023347824811935\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 232.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 179: 0.0023693698458373547\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 430.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 179: 0.11021281033754349\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 227.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 180: 0.002368590794503689\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 539.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 180: 0.11019241809844971\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 224.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 181: 0.0023677630815654993\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 573.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 181: 0.11017494648694992\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 232.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 182: 0.0023670971859246492\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 585.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 182: 0.11015667021274567\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 235.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 183: 0.0023664962500333786\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 559.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 183: 0.11013229936361313\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 226.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 184: 0.0023657113779336214\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 474.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 184: 0.11011181026697159\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 228.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 185: 0.0023649344220757484\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 461.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 185: 0.11009174585342407\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 209.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 186: 0.002364175394177437\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 484.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 186: 0.11007168143987656\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 224.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 187: 0.0023634154349565506\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 430.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 187: 0.11005152761936188\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 238.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 188: 0.0023626554757356644\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 578.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 188: 0.11003127694129944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 228.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 189: 0.0023620605934411287\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 567.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 189: 0.11001040786504745\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 228.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 190: 0.0023612743243575096\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 539.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 190: 0.10998830944299698\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 221.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 191: 0.002360490383580327\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 539.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 191: 0.109967902302742\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 230.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 192: 0.002359737642109394\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 547.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 192: 0.109946608543396\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 225.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 193: 0.002359016565605998\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 469.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 193: 0.10992738604545593\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 238.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 194: 0.0023583127185702324\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 545.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 194: 0.10987863689661026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 227.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 195: 0.0023567432072013617\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 562.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 195: 0.10988278687000275\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 226.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 196: 0.0023566565942019224\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 439.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 196: 0.10986436903476715\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 225.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 197: 0.00235581467859447\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 539.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 197: 0.10984072834253311\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 223.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 198: 0.002355028875172138\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 535.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 198: 0.10981814563274384\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 221.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 199: 0.0023542880080640316\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 455.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss at epoch 199: 0.10979600250720978\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# roberta = RobertaModel.from_pretrained(\"roberta-large\").to(device)\n",
    "train_loss_record = []\n",
    "wandb.watch(model, log = 'all')\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    train_loss_sum = []\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for idx, data in tqdm(enumerate(train_loader, 0)):\n",
    "        x_numerical = data['x_numerical'].to(device, dtype = torch.float)\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        masks = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        y_pred = model(x_numerical, ids, masks, token_type_ids)\n",
    "        # print('y_pred:', y_pred)\n",
    "        _, pred_label = torch.max(y_pred.data, 1)\n",
    "\n",
    "        loss = criterion(y_pred, targets.reshape(-1))\n",
    "\n",
    "         # Zero out gradient, else they will accumulate between epochs\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "\n",
    "        for name, p in model.named_parameters():\n",
    "            if(p.requires_grad) and (\"bias\" not in name):\n",
    "                if p.grad is not None:\n",
    "                    wandb.log({'gradient_' + name: p.grad.abs().mean().cpu().data.numpy()})\n",
    "                else:\n",
    "                    wandb.log({'gradient_' + name: 0})\n",
    "\n",
    "\n",
    "        # plot_grad_flow(model.named_parameters(), idx, epoch) # version 1\n",
    "\n",
    "        # Update parameters\n",
    "        optimiser.step()\n",
    "\n",
    "\n",
    "\n",
    "        train_loss.append(loss.data.cpu())\n",
    "        train_loss_sum.append(loss.data.cpu())\n",
    "\n",
    "\n",
    "        wandb.log({'avg train loss in this batch': loss.item(), 'epoch': epoch, 'batch_id': idx})\n",
    "        # Get accuracy\n",
    "        train_total += targets.reshape(-1).size(0)\n",
    "        train_correct += (pred_label == targets.reshape(-1)).sum()\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "    # Record at every epoch\n",
    "    print('Train Loss at epoch {}: {}\\n'.format(epoch, np.mean(train_loss_sum)))\n",
    "    train_loss_record.append(np.mean(train_loss_sum))\n",
    "    wandb.log({'avg train loss in this epoch': np.mean(train_loss_sum), 'epoch': epoch})\n",
    "    wandb.log({'train accuracy in this epoch': train_accuracy, 'epoch': epoch})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # evaluate on test set every epoch\n",
    "    test_loss = []\n",
    "    test_loss_sum = []\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for idx, data in tqdm(enumerate(test_loader, 0)):\n",
    "        test_x_numerical = data['x_numerical'].to(device, dtype = torch.float)\n",
    "        test_ids = data['ids'].to(device, dtype = torch.long)\n",
    "        test_masks = data['mask'].to(device, dtype = torch.long)\n",
    "        test_token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        test_targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        y_pred = model(test_x_numerical, test_ids, test_masks, test_token_type_ids)\n",
    "        _, pred_label = torch.max(y_pred.data, 1)\n",
    "\n",
    "#         print('y_pred:', y_pred)\n",
    "        tloss = criterion(y_pred, test_targets.reshape(-1))\n",
    "\n",
    "        test_loss.append(tloss.data.cpu())\n",
    "        test_loss_sum.append(tloss.data.cpu())\n",
    "\n",
    "        wandb.log({'avg test loss in this batch': tloss.item(), 'epoch': epoch, 'batch_id': idx})\n",
    "\n",
    "        # Get accuracy\n",
    "        total += test_targets.reshape(-1).size(0)\n",
    "        correct += (pred_label == test_targets.reshape(-1)).sum()\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    # Record at every epoch\n",
    "    print('test Loss at epoch {}: {}\\n'.format(epoch, np.mean(test_loss_sum)))\n",
    "    wandb.log({'avg test loss in this epoch': np.mean(test_loss_sum), 'epoch': epoch})\n",
    "    wandb.log({'test accuracy in this epoch': accuracy, 'epoch': epoch})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # save the model weights every save_freq_epoch epochs\n",
    "    # if (epoch % save_freq_epoch == 0):\n",
    "    #     output_model_file = output_path + '/roberta_stock_pred_' + str(epoch) + '.bin'\n",
    "    #     output_vocab_file = output_path\n",
    "\n",
    "    #     model_to_save = model\n",
    "    #     torch.save(model_to_save, output_model_file)\n",
    "    #     tokenizer.save_                                                                           vocabulary(output_vocab_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Czyuam3JfJj2",
   "metadata": {
    "id": "Czyuam3JfJj2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TB_GL_Ilbfy4",
   "metadata": {
    "id": "TB_GL_Ilbfy4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6bbec1",
   "metadata": {
    "id": "bb6bbec1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac5f8f",
   "metadata": {
    "id": "4aac5f8f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "Kcgrof1frlbM",
   "metadata": {
    "id": "Kcgrof1frlbM"
   },
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1122c8c",
   "metadata": {
    "id": "e1122c8c"
   },
   "outputs": [],
   "source": [
    "output_model_file = output_path + '/roberta_stock_pred.bin'\n",
    "output_vocab_file = output_path\n",
    "\n",
    "model_to_save = model\n",
    "torch.save(model_to_save, output_model_file)\n",
    "tokenizer.save_vocabulary(output_vocab_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3049d3f4",
   "metadata": {
    "id": "3049d3f4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1807d2b1",
   "metadata": {
    "id": "1807d2b1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf315ee",
   "metadata": {
    "id": "2cf315ee"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
