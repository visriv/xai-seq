{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e36d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from numbers import Number\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import brewer2mpl\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a05db6",
   "metadata": {},
   "source": [
    "## Get training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f27b7ae",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_set_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m X_train \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m y_train \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m60\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mtraining_set_scaled\u001b[49m)):\n\u001b[1;32m     11\u001b[0m     X_train\u001b[38;5;241m.\u001b[39mappend(training_set_scaled[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m60\u001b[39m: i, \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     12\u001b[0m     y_train\u001b[38;5;241m.\u001b[39mappend(training_set_scaled[i, \u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_set_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/XOM_norm_data.csv', sep = ',', header = 0)\n",
    "\n",
    "\n",
    "# date.  input features   x.     delta.   label (0,1,2)\n",
    "\n",
    "\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(60, len(training_set_scaled)):\n",
    "    X_train.append(training_set_scaled[i-60: i, 0])\n",
    "    y_train.append(training_set_scaled[i, 0])\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95497ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01/02/2021'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Date'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8656f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "args = Object()\n",
    "args.dataset = 'mnist'\n",
    "args.dist = 'normal'\n",
    "args.num_epochs = 10\n",
    "args.batch_size = 32\n",
    "args.learning_rate = 1e-3\n",
    "args.latent_dim = 10\n",
    "args.beta = 10\n",
    "args.tcvae = True\n",
    "args.exclude_mutinfo = False\n",
    "args.beta_anneal = False\n",
    "args.lambda_anneal = False\n",
    "args.mss = False\n",
    "args.conv = True\n",
    "args.gpu = 0\n",
    "args.visdom = False\n",
    "args.save = 'mnist_10_10'\n",
    "args.log_freq = 200\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42f87c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df3d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"stock_pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117754f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading and batching datasets\n",
    "def setup_data_loaders(args, use_cuda=False):\n",
    "    from torchvision.transforms import ToTensor, Lambda\n",
    "    from torchvision import datasets\n",
    "\n",
    "\n",
    "    if args.dataset == 'shapes':\n",
    "        train_set = dset.Shapes()\n",
    "    elif args.dataset == 'faces':\n",
    "        train_set = dset.Faces()\n",
    "    elif args.dataset == 'mnist':\n",
    "        train_set = datasets.MNIST(\n",
    "        root=\"data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=ToTensor())\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Unknown dataset ' + str(args.dataset))\n",
    "\n",
    "    kwargs = {'num_workers': 0, 'pin_memory': use_cuda}\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_set,\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    return train_loader\n",
    "\n",
    "class SiameseDataLoader(Dataset):\n",
    "    \"\"\"\n",
    "    Train: For each sample creates randomly a positive or a negative pair\n",
    "    Test: Creates fixed pairs for testing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mnist_dataset):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "\n",
    "        self.train = self.mnist_dataset.train\n",
    "        self.transform = self.mnist_dataset.transform\n",
    "\n",
    "        if self.train:\n",
    "            self.train_labels = self.mnist_dataset.train_labels\n",
    "            self.train_data = self.mnist_dataset.train_data\n",
    "            self.labels_set = set(self.train_labels.numpy())\n",
    "            self.label_to_indices = {label: np.where(self.train_labels.numpy() == label)[0]\n",
    "                                     for label in self.labels_set}\n",
    "       \n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            target = np.random.randint(0, 2)\n",
    "            img1, label1 = self.train_data[index], self.train_labels[index].item()\n",
    "            if target == 1:\n",
    "                siamese_index = index\n",
    "                while siamese_index == index:\n",
    "                    siamese_index = np.random.choice(self.label_to_indices[label1])\n",
    "            else:\n",
    "                siamese_label = np.random.choice(list(self.labels_set - set([label1])))\n",
    "                siamese_index = np.random.choice(self.label_to_indices[siamese_label])\n",
    "            img2 = self.train_data[siamese_index]\n",
    "        \n",
    "        img1 = Image.fromarray(img1.numpy(), mode='L')\n",
    "        img2 = Image.fromarray(img2.numpy(), mode='L')\n",
    "        if self.transform is not None:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "        return (img1, img2), target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807fa6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        self.convnet = nn.Sequential(nn.Conv2d(1, 32, 5), nn.PReLU(),\n",
    "                                     nn.MaxPool2d(2, stride=2),\n",
    "                                     nn.Conv2d(32, 64, 5), nn.PReLU(),\n",
    "                                     nn.MaxPool2d(2, stride=2))\n",
    "\n",
    "        self.fc = nn.Sequential(nn.Linear(64 * 4 * 4, 256),\n",
    "                                nn.PReLU(),\n",
    "                                nn.Linear(256, 256),\n",
    "                                nn.PReLU(),\n",
    "                                nn.Linear(256, 2)\n",
    "                                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.convnet(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self, embedding_net):\n",
    "        super(SiameseNet, self).__init__()\n",
    "        self.embedding_net = embedding_net\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        output1 = self.embedding_net(x1)\n",
    "        output2 = self.embedding_net(x2)\n",
    "        return output1, output2\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.embedding_net(x)\n",
    "\n",
    "class ClassificationNet(nn.Module):\n",
    "    def __init__(self, embedding_net, n_classes):\n",
    "        super(ClassificationNet, self).__init__()\n",
    "        self.embedding_net = embedding_net\n",
    "        self.n_classes = n_classes\n",
    "        self.nonlinear = nn.PReLU()\n",
    "        self.fc1 = nn.Linear(2, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.embedding_net(x)\n",
    "        output = self.nonlinear(output)\n",
    "        scores = F.log_softmax(self.fc1(output), dim=-1)\n",
    "        return scores\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.nonlinear(self.embedding_net(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b01d28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.set_device(args.gpu)\n",
    "use_cuda = False\n",
    "# data loader\n",
    "train_loader = setup_data_loaders(args, use_cuda=False)\n",
    "\n",
    "\n",
    "\n",
    "# setup the VAE\n",
    "# if args.dist == 'normal':\n",
    "#     prior_dist = dist.Normal()\n",
    "#     q_dist = dist.Normal()\n",
    "# elif args.dist == 'laplace':\n",
    "#     prior_dist = dist.Laplace()\n",
    "#     q_dist = dist.Laplace()\n",
    "# elif args.dist == 'flow':\n",
    "#     prior_dist = FactorialNormalizingFlow(dim=args.latent_dim, nsteps=32)\n",
    "#     q_dist = dist.Normal()\n",
    "\n",
    "\n",
    "model = SiameseNet()\n",
    "\n",
    "\n",
    "# vae = VAE(z_dim=args.latent_dim, use_cuda=use_cuda, prior_dist=prior_dist, q_dist=q_dist, image_dim = 28,\n",
    "#     include_mutinfo=not args.exclude_mutinfo, tcvae=args.tcvae, conv=args.conv, mss=args.mss)\n",
    "\n",
    "# setup the optimizer\n",
    "optimizer = optim.Adam(vae.parameters(), lr=args.learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss(loss_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b7d160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log info\n",
    "train_log_string = '%s :: Epoch %i :: Iter %i / %i :: train loss: %0.4f'\n",
    "valid_log_string = '%s :: Epoch %i :: valid loss: %0.4f\\n'\n",
    "\n",
    "# Restore saved model (if one exists).\n",
    "ckpt_path = os.path.join(args.ckpt_dir, args.exp_name+'.pt')\n",
    "\n",
    "if os.path.exists(ckpt_path):\n",
    "    print('Loading checkpoint: %s' % ckpt_path)\n",
    "    ckpt = torch.load(ckpt_path)\n",
    "    epoch = ckpt['epoch']\n",
    "    model.load_state_dict(ckpt['siamese'])\n",
    "    optimizer.load_state_dict(ckpt['optimizer'])\n",
    "else:\n",
    "    epoch = 1\n",
    "    print('Fresh start!\\n')\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    criterion = criterion.cuda()\n",
    "    model = model.cuda()\n",
    "\n",
    "\n",
    "    \n",
    "# save every epoch for visualization\n",
    "train_loss_record = []\n",
    "valid_loss_record = []\n",
    "best_record = 10.0\n",
    "\n",
    "# training\n",
    "print('Experiment: {}\\n'.format(config['experiment_name']))\n",
    "\n",
    "while epoch < args.num_epochs:\n",
    "\n",
    "    print('Start Epoch {} Training...'.format(epoch))\n",
    "\n",
    "    # loss\n",
    "    train_loss = []\n",
    "    train_loss_sum = []\n",
    "    # dataloader\n",
    "    train_dataloader = DataLoader(dataset=trainDS, shuffle=True, num_workers=1, batch_size=32)\n",
    "\n",
    "    for idx, data in enumerate(train_dataloader, 0):\n",
    "        print('idx:', idx)\n",
    "        # get data\n",
    "        s1, s2, label = data\n",
    "\n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # input\n",
    "        output = model(s1, s2)\n",
    "        output = output.squeeze(0)\n",
    "\n",
    "        # label cuda\n",
    "        label = Variable(label)\n",
    "        if torch.cuda.is_available():\n",
    "            label = label.cuda()\n",
    "\n",
    "        # loss backward\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.cpu())\n",
    "        train_loss_sum.append(loss.data.cpu())\n",
    "        \n",
    "        wandb.log({'train_loss': loss.data.cpu(), 'iteration': idx, 'epoch': epoch})\n",
    "\n",
    "            \n",
    "            \n",
    "        # Every once and a while check on the loss\n",
    "        if ((idx + 1) % 5000) == 0:\n",
    "            print(train_log_string % (datetime.now(), epoch, idx + 1, len(train), np.mean(train_loss)))\n",
    "            train_loss = []\n",
    "\n",
    "    # Record at every epoch\n",
    "    print('Train Loss at epoch {}: {}\\n'.format(epoch, np.mean(train_loss_sum)))\n",
    "    train_loss_record.append(np.mean(train_loss_sum))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09430dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_elbo = []\n",
    "\n",
    "# training loop\n",
    "dataset_size = len(train_loader.dataset)\n",
    "num_iterations = len(train_loader) * args.num_epochs\n",
    "iteration = 0\n",
    "# initialize loss accumulator\n",
    "elbo_running_mean = utils.RunningAverageMeter()\n",
    "while iteration < num_iterations:\n",
    "    for i, x in enumerate(train_loader):\n",
    "        iteration += 1\n",
    "        print(iteration)\n",
    "        batch_time = time.time()\n",
    "        vae.train()\n",
    "        utils.anneal_kl(args, vae, iteration)\n",
    "        optimizer.zero_grad()\n",
    "        # transfer to GPU\n",
    "        x = x[0]\n",
    "        # print(type(x))\n",
    "        # print(x)\n",
    "        # print(len(x), len(x[0]), len(x[0][0]), len(x[0][0][0]))\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        # wrap the mini-batch in a PyTorch Variable\n",
    "        x = Variable(x)\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        obj, elbo = vae.elbo(x, dataset_size, 28)\n",
    "#         print('elbo', elbo)\n",
    "        if utils.isnan(obj).any():\n",
    "            raise ValueError('NaN spotted in objective.')\n",
    "        obj.mean().mul(-1).backward()\n",
    "        elbo_running_mean.update(torch.mean(elbo.mean()))\n",
    "        optimizer.step()\n",
    "        \n",
    "        wandb.log({'elbo_rm': elbo_running_mean.avg, 'iteration': iteration, 'obj': obj.mean().item()})\n",
    "\n",
    "\n",
    "        # report training diagnostics\n",
    "        if iteration % args.log_freq == 0:\n",
    "            train_elbo.append(elbo_running_mean.avg)\n",
    "            print('[iteration %03d] time: %.2f \\tbeta %.2f \\tlambda %.2f training ELBO: %.4f (%.4f)' % (\n",
    "                iteration, time.time() - batch_time, vae.beta, vae.lamb,\n",
    "                elbo_running_mean.val, elbo_running_mean.avg))\n",
    "\n",
    "            vae.eval()\n",
    "\n",
    "            # plot training and test ELBOs\n",
    "#             if args.visdom:\n",
    "            utils_plot.display_samples(vae, x, vis, 28)\n",
    "            utils_plot.plot_elbo(train_elbo, vis)\n",
    "\n",
    "            utils.save_checkpoint({\n",
    "                'state_dict': vae.state_dict(),\n",
    "                'args': args}, args.save, 0)\n",
    "            eval('plot_vs_gt_' + args.dataset)(vae, train_loader.dataset,\n",
    "                                               os.path.join(args.save, 'gt_vs_latent_{:05d}.png'.format(iteration)),\n",
    "                                               use_cuda=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b4cd00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf187436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
