diff --git a/stock_pred_seq.ipynb b/stock_pred_seq.ipynb
index 02c088b..c467967 100644
--- a/stock_pred_seq.ipynb
+++ b/stock_pred_seq.ipynb
@@ -3,40 +3,27 @@
   {
    "cell_type": "code",
    "execution_count": null,
-   "id": "0035c6ed",
+   "id": "5e0bf92b",
    "metadata": {},
    "outputs": [],
    "source": [
     "!pip install yfinance\n",
-    "!pip install transformers\n"
+    "!pip install transformers"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 36,
-   "id": "64983989",
-   "metadata": {
-    "scrolled": true
-   },
+   "execution_count": 4,
+   "id": "075d5390",
+   "metadata": {},
    "outputs": [
     {
      "name": "stderr",
      "output_type": "stream",
      "text": [
-      "/var/folders/tw/t5br5nkd2k3f065pr50zxvn80000gn/T/ipykernel_22948/1880521505.py:6: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
-      "  plt.style.use('seaborn')\n",
-      "/var/folders/tw/t5br5nkd2k3f065pr50zxvn80000gn/T/ipykernel_22948/1880521505.py:10: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
-      "  from pandas import datetime\n",
       "/Users/visriv/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
       "  from .autonotebook import tqdm as notebook_tqdm\n"
      ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "[*********************100%***********************]  1 of 1 completed\n"
-     ]
     }
    ],
    "source": [
@@ -45,31 +32,97 @@
     "import numpy as np\n",
     "import pandas as pd\n",
     "from pylab import mpl, plt\n",
-    "plt.style.use('seaborn')\n",
-    "# mpl.rcParams['font.family'] = 'serif'\n",
-    "%matplotlib inline\n",
-    "\n",
-    "from pandas import datetime\n",
     "import math, time\n",
     "import itertools\n",
-    "import datetime\n",
+    "from datetime import datetime\n",
     "from operator import itemgetter\n",
-    "\n",
+    "from tqdm import tqdm\n",
     "from math import sqrt\n",
     "import torch\n",
     "import torch.nn as nn\n",
-    "from torch.autograd import Variable\n",
-    "\n",
-    "# Set hyperparameters\n",
-    "stock_symbols = [ 'XOM']\n",
-    "no_of_days = 4*365\n",
+    "from torch.autograd import Variable\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "6b072c6a",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "from torch import cuda\n",
+    "device = 'cuda' if cuda.is_available() else 'cpu'"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "09f48896",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import wandb\n",
+    "wandb.login()\n",
+    "wandb.init(project=\"stock_prediction\")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "1522fd38",
+   "metadata": {},
+   "source": [
+    "### Hyperparams"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 2,
+   "id": "95e83d70",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "no_of_days_to_lookforward = 2\n",
+    "no_of_days_to_lookback = 5\n",
     "up_threshold = 0.015\n",
     "down_threshold = -0.015\n",
+    "max_text_per_iter = 100\n",
+    "batch_size = 1\n",
+    "MAX_LEN = 1000\n",
+    "num_epochs = 20"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "d5409aee",
+   "metadata": {},
+   "source": [
+    "### Get stocks data for last N days"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "612e5bd8",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "64983989",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "stock_symbols = [ 'XOM']\n",
+    "no_of_days = 4*365\n",
+    "\n",
     "EXPORT_DATA_FOLDER = './data/'\n",
     "\n",
-    "# Set the start and end dates for the data\n",
-    "start = datetime.strptime('04-01-2019', '%m/%d/%y ')\n",
-    "end = datetime.strptime('04-01-2023', '%m/%d/%y ')\n",
+    "# Set the start and end dates for the data \n",
+    "# here matching it with dates of news text available\n",
+    "start = datetime.strptime('2019/01/04', '%Y/%m/%d')\n",
+    "end = datetime.strptime('2023/01/04', '%Y/%m/%d')\n",
     "\n",
     "\n",
     "# start = datetime.datetime.now() - datetime.timedelta(days=no_of_days)\n",
@@ -127,35 +180,15 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 92,
-   "id": "f10e33cb",
+   "execution_count": null,
+   "id": "5cece986",
    "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "DatetimeIndex(['2021-06-08', '2021-06-09', '2021-06-10', '2021-06-11',\n",
-       "               '2021-06-14', '2021-06-15', '2021-06-16', '2021-06-17',\n",
-       "               '2021-06-18', '2021-06-21',\n",
-       "               ...\n",
-       "               '2023-05-18', '2023-05-19', '2023-05-22', '2023-05-23',\n",
-       "               '2023-05-24', '2023-05-25', '2023-05-26', '2023-05-30',\n",
-       "               '2023-05-31', '2023-06-01'],\n",
-       "              dtype='datetime64[ns]', name='Date', length=500, freq=None)"
-      ]
-     },
-     "execution_count": 92,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "data_norm.index"
-   ]
+   "outputs": [],
+   "source": []
   },
   {
    "cell_type": "markdown",
-   "id": "22ef7030",
+   "id": "be02ae72",
    "metadata": {},
    "source": [
     "## TODO (2023-06-05)\n"
@@ -164,21 +197,21 @@
   {
    "cell_type": "code",
    "execution_count": null,
-   "id": "23b593c6",
+   "id": "50b4c52c",
    "metadata": {},
    "outputs": [],
    "source": [
     "'''\n",
     "cuda support check\n",
-    "read textual data into correct shape\n",
+    "//read textual data into correct shape\n",
     "hyperparam tuning: number of neurons: tune to right number of neurons in FC in model\n",
-    "max_text_per_iter -> code in dataloader to maintain the size \n",
+    "//max_text_per_iter -> code in dataloader to maintain the size \n",
     "'''"
    ]
   },
   {
    "cell_type": "markdown",
-   "id": "9e43d39e",
+   "id": "e7bb5289",
    "metadata": {},
    "source": [
     "## Prep textual data"
@@ -186,35 +219,10 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 96,
-   "id": "11b67ab0",
-   "metadata": {
-    "collapsed": true
-   },
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "Date\n",
-       "2021-01-01    Tomato processor's accrued production costs fa...\n",
-       "2022-01-01    Industrial Alcohol Market Forecasts to 2028 – ...\n",
-       "2023-01-01    Global Cumene Market Research Report 2022-2032...\n",
-       "2020-10-01    Press Release: SBM Offshore awarded contracts ...\n",
-       "2021-10-01    JPMorgan 's Own Employee Travel Numbers Now He...\n",
-       "                                    ...                        \n",
-       "2021-08-09    Climate Change Is a ‘Hammer Hitting Us on the ...\n",
-       "2022-08-09    KASE - Trading in common shares US30231G1022 (...\n",
-       "2020-09-09    TAP Clouds Italy’s LNG Import Plans The immine...\n",
-       "2021-09-09    Storm's Fallout Cripples U.S. Oil Output --- S...\n",
-       "2022-09-09    Thermoplastic Elastomer Market Forecasts to 20...\n",
-       "Name: News, Length: 1065, dtype: object"
-      ]
-     },
-     "execution_count": 96,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
+   "execution_count": null,
+   "id": "7b5d2b1a",
+   "metadata": {},
+   "outputs": [],
    "source": [
     "text_data_df = pd.read_csv('./data/XOM_20200401_20230401_medium.csv', sep= ',', header= 0)\n",
     "text_data_df = text_data_df[['Date', 'News']]\n",
@@ -223,263 +231,15 @@
     "text_data_df = text_data_df.groupby('Date')['News'].apply('$$$###'.join)\n",
     "\n",
     "text_data_df.index = pd.to_datetime(text_data_df.index, dayfirst=True)\n",
-    "text_data_df\n"
+    "# text_data_df\n"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 102,
-   "id": "95276237",
-   "metadata": {
-    "collapsed": true
-   },
-   "outputs": [
-    {
-     "data": {
-      "text/html": [
-       "<div>\n",
-       "<style scoped>\n",
-       "    .dataframe tbody tr th:only-of-type {\n",
-       "        vertical-align: middle;\n",
-       "    }\n",
-       "\n",
-       "    .dataframe tbody tr th {\n",
-       "        vertical-align: top;\n",
-       "    }\n",
-       "\n",
-       "    .dataframe thead th {\n",
-       "        text-align: right;\n",
-       "    }\n",
-       "</style>\n",
-       "<table border=\"1\" class=\"dataframe\">\n",
-       "  <thead>\n",
-       "    <tr style=\"text-align: right;\">\n",
-       "      <th></th>\n",
-       "      <th>Open_norm</th>\n",
-       "      <th>High_norm</th>\n",
-       "      <th>Low_norm</th>\n",
-       "      <th>Close_norm</th>\n",
-       "      <th>Volume_norm</th>\n",
-       "      <th>High-Open_norm</th>\n",
-       "      <th>Low-Open_norm</th>\n",
-       "      <th>Close-Open_norm</th>\n",
-       "      <th>Label_2up1down</th>\n",
-       "      <th>News</th>\n",
-       "    </tr>\n",
-       "    <tr>\n",
-       "      <th>Date</th>\n",
-       "      <th></th>\n",
-       "      <th></th>\n",
-       "      <th></th>\n",
-       "      <th></th>\n",
-       "      <th></th>\n",
-       "      <th></th>\n",
-       "      <th></th>\n",
-       "      <th></th>\n",
-       "      <th></th>\n",
-       "      <th></th>\n",
-       "    </tr>\n",
-       "  </thead>\n",
-       "  <tbody>\n",
-       "    <tr>\n",
-       "      <th>2021-06-08</th>\n",
-       "      <td>0.624690</td>\n",
-       "      <td>0.576438</td>\n",
-       "      <td>0.482223</td>\n",
-       "      <td>0.675290</td>\n",
-       "      <td>0.490718</td>\n",
-       "      <td>0.240756</td>\n",
-       "      <td>0.785477</td>\n",
-       "      <td>0.583539</td>\n",
-       "      <td>0</td>\n",
-       "      <td>Blowing Agent Market by Type (HC, HFC, HCFC), ...</td>\n",
-       "    </tr>\n",
-       "    <tr>\n",
-       "      <th>2021-06-09</th>\n",
-       "      <td>0.757691</td>\n",
-       "      <td>0.598956</td>\n",
-       "      <td>0.716982</td>\n",
-       "      <td>0.610093</td>\n",
-       "      <td>0.229087</td>\n",
-       "      <td>0.180812</td>\n",
-       "      <td>0.878255</td>\n",
-       "      <td>0.490038</td>\n",
-       "      <td>0</td>\n",
-       "      <td>Global Asphalt Market 2021-2025 Published By: ...</td>\n",
-       "    </tr>\n",
-       "    <tr>\n",
-       "      <th>2021-06-10</th>\n",
-       "      <td>0.724752</td>\n",
-       "      <td>0.550617</td>\n",
-       "      <td>0.533313</td>\n",
-       "      <td>0.562715</td>\n",
-       "      <td>0.210127</td>\n",
-       "      <td>0.087882</td>\n",
-       "      <td>0.660152</td>\n",
-       "      <td>0.373188</td>\n",
-       "      <td>0</td>\n",
-       "      <td>Global Benzene Market 2021-2025 Published By: ...</td>\n",
-       "    </tr>\n",
-       "    <tr>\n",
-       "      <th>2021-06-11</th>\n",
-       "      <td>0.524817</td>\n",
-       "      <td>0.385896</td>\n",
-       "      <td>0.512483</td>\n",
-       "      <td>0.486898</td>\n",
-       "      <td>0.118679</td>\n",
-       "      <td>0.043161</td>\n",
-       "      <td>0.780528</td>\n",
-       "      <td>0.374858</td>\n",
-       "      <td>0</td>\n",
-       "      <td>Keystone Illustrates Pipelines' Hurdles The fa...</td>\n",
-       "    </tr>\n",
-       "    <tr>\n",
-       "      <th>2021-06-14</th>\n",
-       "      <td>0.509796</td>\n",
-       "      <td>0.431470</td>\n",
-       "      <td>0.456017</td>\n",
-       "      <td>0.540300</td>\n",
-       "      <td>0.216318</td>\n",
-       "      <td>0.116407</td>\n",
-       "      <td>0.813746</td>\n",
-       "      <td>0.460184</td>\n",
-       "      <td>2</td>\n",
-       "      <td>C-Suite Strategies (A Special Report): Managem...</td>\n",
-       "    </tr>\n",
-       "    <tr>\n",
-       "      <th>...</th>\n",
-       "      <td>...</td>\n",
-       "      <td>...</td>\n",
-       "      <td>...</td>\n",
-       "      <td>...</td>\n",
-       "      <td>...</td>\n",
-       "      <td>...</td>\n",
-       "      <td>...</td>\n",
-       "      <td>...</td>\n",
-       "      <td>...</td>\n",
-       "      <td>...</td>\n",
-       "    </tr>\n",
-       "    <tr>\n",
-       "      <th>2023-03-27</th>\n",
-       "      <td>0.865728</td>\n",
-       "      <td>0.678916</td>\n",
-       "      <td>0.752635</td>\n",
-       "      <td>0.704917</td>\n",
-       "      <td>0.258925</td>\n",
-       "      <td>0.250995</td>\n",
-       "      <td>0.874040</td>\n",
-       "      <td>0.582445</td>\n",
-       "      <td>0</td>\n",
-       "      <td>Exxon Eyes Staggered, But Larger, Rovuma LNG S...</td>\n",
-       "    </tr>\n",
-       "    <tr>\n",
-       "      <th>2023-03-28</th>\n",
-       "      <td>0.656699</td>\n",
-       "      <td>0.567402</td>\n",
-       "      <td>0.619880</td>\n",
-       "      <td>0.638819</td>\n",
-       "      <td>0.150382</td>\n",
-       "      <td>0.316675</td>\n",
-       "      <td>0.962312</td>\n",
-       "      <td>0.633900</td>\n",
-       "      <td>2</td>\n",
-       "      <td>PDF China National Chemical Corporation Ltd. ...</td>\n",
-       "    </tr>\n",
-       "    <tr>\n",
-       "      <th>2023-03-29</th>\n",
-       "      <td>0.805350</td>\n",
-       "      <td>0.591039</td>\n",
-       "      <td>0.692672</td>\n",
-       "      <td>0.671698</td>\n",
-       "      <td>0.314097</td>\n",
-       "      <td>0.153800</td>\n",
-       "      <td>0.916154</td>\n",
-       "      <td>0.566586</td>\n",
-       "      <td>0</td>\n",
-       "      <td>Global Polyolefin Market 2023-2027 Published B...</td>\n",
-       "    </tr>\n",
-       "    <tr>\n",
-       "      <th>2023-03-30</th>\n",
-       "      <td>0.714846</td>\n",
-       "      <td>0.511150</td>\n",
-       "      <td>0.599019</td>\n",
-       "      <td>0.585574</td>\n",
-       "      <td>0.170996</td>\n",
-       "      <td>0.002758</td>\n",
-       "      <td>0.850549</td>\n",
-       "      <td>0.488014</td>\n",
-       "      <td>0</td>\n",
-       "      <td>30 March The high-yielding shares powering thi...</td>\n",
-       "    </tr>\n",
-       "    <tr>\n",
-       "      <th>2023-03-31</th>\n",
-       "      <td>0.611718</td>\n",
-       "      <td>0.524529</td>\n",
-       "      <td>0.563195</td>\n",
-       "      <td>0.562412</td>\n",
-       "      <td>0.301445</td>\n",
-       "      <td>0.067498</td>\n",
-       "      <td>0.908698</td>\n",
-       "      <td>0.491249</td>\n",
-       "      <td>2</td>\n",
-       "      <td>Global Commodity Plastics Market 2023-2027 Pub...</td>\n",
-       "    </tr>\n",
-       "  </tbody>\n",
-       "</table>\n",
-       "<p>458 rows × 10 columns</p>\n",
-       "</div>"
-      ],
-      "text/plain": [
-       "            Open_norm  High_norm  Low_norm  Close_norm  Volume_norm  \\\n",
-       "Date                                                                  \n",
-       "2021-06-08   0.624690   0.576438  0.482223    0.675290     0.490718   \n",
-       "2021-06-09   0.757691   0.598956  0.716982    0.610093     0.229087   \n",
-       "2021-06-10   0.724752   0.550617  0.533313    0.562715     0.210127   \n",
-       "2021-06-11   0.524817   0.385896  0.512483    0.486898     0.118679   \n",
-       "2021-06-14   0.509796   0.431470  0.456017    0.540300     0.216318   \n",
-       "...               ...        ...       ...         ...          ...   \n",
-       "2023-03-27   0.865728   0.678916  0.752635    0.704917     0.258925   \n",
-       "2023-03-28   0.656699   0.567402  0.619880    0.638819     0.150382   \n",
-       "2023-03-29   0.805350   0.591039  0.692672    0.671698     0.314097   \n",
-       "2023-03-30   0.714846   0.511150  0.599019    0.585574     0.170996   \n",
-       "2023-03-31   0.611718   0.524529  0.563195    0.562412     0.301445   \n",
-       "\n",
-       "            High-Open_norm  Low-Open_norm  Close-Open_norm  Label_2up1down  \\\n",
-       "Date                                                                         \n",
-       "2021-06-08        0.240756       0.785477         0.583539               0   \n",
-       "2021-06-09        0.180812       0.878255         0.490038               0   \n",
-       "2021-06-10        0.087882       0.660152         0.373188               0   \n",
-       "2021-06-11        0.043161       0.780528         0.374858               0   \n",
-       "2021-06-14        0.116407       0.813746         0.460184               2   \n",
-       "...                    ...            ...              ...             ...   \n",
-       "2023-03-27        0.250995       0.874040         0.582445               0   \n",
-       "2023-03-28        0.316675       0.962312         0.633900               2   \n",
-       "2023-03-29        0.153800       0.916154         0.566586               0   \n",
-       "2023-03-30        0.002758       0.850549         0.488014               0   \n",
-       "2023-03-31        0.067498       0.908698         0.491249               2   \n",
-       "\n",
-       "                                                         News  \n",
-       "Date                                                           \n",
-       "2021-06-08  Blowing Agent Market by Type (HC, HFC, HCFC), ...  \n",
-       "2021-06-09  Global Asphalt Market 2021-2025 Published By: ...  \n",
-       "2021-06-10  Global Benzene Market 2021-2025 Published By: ...  \n",
-       "2021-06-11  Keystone Illustrates Pipelines' Hurdles The fa...  \n",
-       "2021-06-14  C-Suite Strategies (A Special Report): Managem...  \n",
-       "...                                                       ...  \n",
-       "2023-03-27  Exxon Eyes Staggered, But Larger, Rovuma LNG S...  \n",
-       "2023-03-28   PDF China National Chemical Corporation Ltd. ...  \n",
-       "2023-03-29  Global Polyolefin Market 2023-2027 Published B...  \n",
-       "2023-03-30  30 March The high-yielding shares powering thi...  \n",
-       "2023-03-31  Global Commodity Plastics Market 2023-2027 Pub...  \n",
-       "\n",
-       "[458 rows x 10 columns]"
-      ]
-     },
-     "execution_count": 102,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
+   "execution_count": null,
+   "id": "4df926c6",
+   "metadata": {},
+   "outputs": [],
    "source": [
     "all_train_df = data_norm.join(text_data_df, how = 'inner')\n",
     "all_train_df"
@@ -487,114 +247,52 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 72,
-   "id": "99b3be01",
-   "metadata": {
-    "collapsed": true
-   },
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Defaulting to user installation because normal site-packages is not writeable\n",
-      "Requirement already satisfied: yfinance in /Users/visriv/Library/Python/3.9/lib/python/site-packages (0.2.18)\n",
-      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from yfinance) (4.11.2)\n",
-      "Requirement already satisfied: numpy>=1.16.5 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from yfinance) (1.24.1)\n",
-      "Requirement already satisfied: frozendict>=2.3.4 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from yfinance) (2.3.8)\n",
-      "Requirement already satisfied: appdirs>=1.4.4 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from yfinance) (1.4.4)\n",
-      "Requirement already satisfied: html5lib>=1.1 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from yfinance) (1.1)\n",
-      "Requirement already satisfied: requests>=2.26 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from yfinance) (2.28.2)\n",
-      "Requirement already satisfied: pytz>=2022.5 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from yfinance) (2022.7.1)\n",
-      "Requirement already satisfied: multitasking>=0.0.7 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from yfinance) (0.0.11)\n",
-      "Requirement already satisfied: pandas>=1.3.0 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from yfinance) (1.5.3)\n",
-      "Requirement already satisfied: cryptography>=3.3.2 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from yfinance) (41.0.1)\n",
-      "Requirement already satisfied: lxml>=4.9.1 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from yfinance) (4.9.2)\n",
-      "Requirement already satisfied: soupsieve>1.2 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.4)\n",
-      "Requirement already satisfied: cffi>=1.12 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from cryptography>=3.3.2->yfinance) (1.15.1)\n",
-      "Requirement already satisfied: six>=1.9 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from html5lib>=1.1->yfinance) (1.15.0)\n",
-      "Requirement already satisfied: webencodings in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
-      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
-      "Requirement already satisfied: certifi>=2017.4.17 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from requests>=2.26->yfinance) (2022.12.7)\n",
-      "Requirement already satisfied: idna<4,>=2.5 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from requests>=2.26->yfinance) (3.4)\n",
-      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from requests>=2.26->yfinance) (2.1.1)\n",
-      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from requests>=2.26->yfinance) (1.26.14)\n",
-      "Requirement already satisfied: pycparser in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from cffi>=1.12->cryptography>=3.3.2->yfinance) (2.21)\n",
-      "\n",
-      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
-      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
-      "Defaulting to user installation because normal site-packages is not writeable\n",
-      "Collecting transformers\n",
-      "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
-      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
-      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from transformers) (4.64.1)\n",
-      "Collecting huggingface-hub<1.0,>=0.14.1\n",
-      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
-      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
-      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from transformers) (23.0)\n",
-      "Requirement already satisfied: pyyaml>=5.1 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0)\n",
-      "Collecting regex!=2019.12.17\n",
-      "  Downloading regex-2023.6.3-cp39-cp39-macosx_10_9_x86_64.whl (294 kB)\n",
-      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
-      "\u001b[?25hRequirement already satisfied: requests in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from transformers) (2.28.2)\n",
-      "Requirement already satisfied: numpy>=1.17 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from transformers) (1.24.1)\n",
-      "Collecting filelock\n",
-      "  Downloading filelock-3.12.0-py3-none-any.whl (10 kB)\n",
-      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
-      "  Downloading tokenizers-0.13.3-cp39-cp39-macosx_10_11_x86_64.whl (4.0 MB)\n",
-      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
-      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.4.0)\n",
-      "Requirement already satisfied: fsspec in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.1.0)\n",
-      "Requirement already satisfied: certifi>=2017.4.17 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2022.12.7)\n",
-      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.1.1)\n",
-      "Requirement already satisfied: idna<4,>=2.5 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.4)\n",
-      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/visriv/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (1.26.14)\n",
-      "Installing collected packages: tokenizers, regex, filelock, huggingface-hub, transformers\n",
-      "Successfully installed filelock-3.12.0 huggingface-hub-0.15.1 regex-2023.6.3 tokenizers-0.13.3 transformers-4.29.2\n",
-      "\n",
-      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
-      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
-     ]
-    }
-   ],
-   "source": []
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 103,
+   "execution_count": 3,
    "id": "56e91800",
-   "metadata": {
-    "scrolled": true
-   },
+   "metadata": {},
    "outputs": [
     {
-     "ename": "TypeError",
-     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
+     "ename": "NameError",
+     "evalue": "name 'all_train_df' is not defined",
      "output_type": "error",
      "traceback": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
-      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
-      "Cell \u001b[0;32mIn[103], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m     y_train\u001b[38;5;241m.\u001b[39mappend(all_train[i, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;66;03m#TODO\u001b[39;00m\n\u001b[1;32m     15\u001b[0m X_numerical_train, y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(X_numerical_train), np\u001b[38;5;241m.\u001b[39marray(y_train)\n\u001b[0;32m---> 17\u001b[0m X_numerical_train \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_numerical_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     18\u001b[0m y_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(y_train)\u001b[38;5;241m.\u001b[39mlong()\n",
-      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
+      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
+      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_train \u001b[38;5;241m=\u001b[39m \u001b[43mall_train_df\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      3\u001b[0m window_size \u001b[38;5;241m=\u001b[39m no_of_days_to_lookback\n\u001b[1;32m      5\u001b[0m X_numerical_train \u001b[38;5;241m=\u001b[39m []\n",
+      "\u001b[0;31mNameError\u001b[0m: name 'all_train_df' is not defined"
      ]
     }
    ],
    "source": [
     "all_train = all_train_df.values\n",
     "\n",
-    "window_size = 5\n",
+    "window_size = no_of_days_to_lookback\n",
     "\n",
     "X_numerical_train = []\n",
     "y_train = []\n",
     "X_text_train = []\n",
+    "X_text_train_curr = []\n",
+    "\n",
     "\n",
     "for i in range(window_size, len(all_train)):\n",
     "    X_numerical_train.append(all_train[i-window_size: i, :-2])\n",
-    "    X_text_train.append(all_train[i-window_size: i, -1])\n",
     "    \n",
-    "    y_train.append(all_train[i, -2]) #TODO\n",
+    "    # split and append sequence of text\n",
+    "    curr_seq = all_train[i-window_size: i, -1]\n",
+    "    for j in range(window_size):\n",
+    "        split_curr_seq = curr_seq[window_size - 1 -j].split('$$$###')\n",
+    "        X_text_train_curr = X_text_train_curr + split_curr_seq\n",
     "    \n",
-    "X_numerical_train, y_train = np.array(X_numerical_train), np.array(y_train)\n",
+    "    if len(X_text_train_curr) > max_text_per_iter:\n",
+    "        X_text_train_curr = X_text_train_curr[:100]\n",
+    "    \n",
+    "    X_text_train.append(X_text_train_curr)\n",
+    "        \n",
+    "    # target labels\n",
+    "    y_train.append(all_train[i:i+no_of_days_to_lookforward, -2]) \n",
+    "\n",
+    "X_numerical_train, y_train = np.array(X_numerical_train).astype(np.float16), np.array(y_train)\n",
+    "print(type(X_numerical_train))\n",
     "\n",
     "X_numerical_train = torch.from_numpy(X_numerical_train).type(torch.Tensor)\n",
     "y_train = torch.from_numpy(y_train).long()\n",
@@ -603,39 +301,27 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 107,
-   "id": "4f42b932",
+   "execution_count": null,
+   "id": "b4a3a91e",
    "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "numpy.ndarray"
-      ]
-     },
-     "execution_count": 107,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
+   "outputs": [],
    "source": [
-    "type(all_train[0: 5, :-2])"
+    "print(len(X_text_train))\n",
+    "print(len(X_text_train[2]))\n",
+    "print(X_text_train[2])"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 65,
-   "id": "3a75b237",
+   "execution_count": null,
+   "id": "345c74c1",
    "metadata": {},
    "outputs": [],
-   "source": [
-    "\n",
-    "\n"
-   ]
+   "source": []
   },
   {
    "cell_type": "markdown",
-   "id": "524e93cf",
+   "id": "ae460f3d",
    "metadata": {},
    "source": [
     "## Data loader"
@@ -643,29 +329,153 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 77,
-   "id": "880f0ab1",
+   "execution_count": null,
+   "id": "3accfb03",
    "metadata": {},
    "outputs": [],
    "source": [
     "from torch.utils.data import Dataset\n",
+    "from torch.utils.data import DataLoader\n",
+    "\n",
+    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large', truncation=True, do_lower_case=True)\n",
+    "\n",
     "class SiameseDataloader(Dataset):\n",
     "    \n",
-    "    def __init__(self, X_numerical_train, y_train, X_text_train):\n",
-    "        \n",
-    "        pass\n",
+    "    def __init__(self, X_numerical_train, y_train, X_text_train, MAX_LEN, tokenizer):\n",
+    "        self.X_numerical_train = X_numerical_train\n",
+    "        self.X_text_train = X_text_train\n",
+    "        self.MAX_LEN = MAX_LEN\n",
+    "        self.tokenizer = tokenizer\n",
     "        \n",
     "    def __getitem__(self, index):\n",
     "\n",
-    "        return (X_train[index], text_train[index]), y_train[index]\n",
+    "        \n",
+    "        input_ids = []\n",
+    "        attention_masks = []\n",
+    "        token_type_ids = []\n",
+    "        input_seq = []\n",
+    "\n",
+    "        for sent in X_text_train[index]:\n",
+    "            encoded_sent = self.tokenizer.encode_plus(\n",
+    "                text=sent,\n",
+    "                add_special_tokens=True,        # Add `[CLS]` and `[SEP]` special tokens\n",
+    "                max_length=self.MAX_LEN,             # Choose max length to truncate/pad\n",
+    "                pad_to_max_length=True,         # Pad sentence to max length \n",
+    "                #return_attention_mask=True      # Return attention mask\n",
+    "                return_token_type_ids=True\n",
+    "                )\n",
+    "            input_ids.append(encoded_sent.get('input_ids'))\n",
+    "            attention_masks.append(encoded_sent.get('attention_mask'))\n",
+    "            token_type_ids.append(encoded_sent.get('token_type_ids'))\n",
+    "\n",
+    "        # Convert lists to tensors\n",
+    "        input_ids = torch.tensor(input_ids)\n",
+    "        attention_masks = torch.tensor(attention_masks)\n",
+    "        token_type_ids = torch.tensor(token_type_ids)\n",
+    "\n",
+    "\n",
+    "        return {\n",
+    "            'x_numerical': X_train[index],\n",
+    "            'ids': torch.tensor(input_ids, dtype=torch.long),\n",
+    "            'mask': torch.tensor(attention_masks, dtype=torch.long),\n",
+    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
+    "            'targets': torch.tensor(y_train[index], dtype=torch.float)\n",
+    "        }\n",
+    "    \n",
+    "    \n",
+    "    \n",
+    "    \n",
     "\n",
     "    def __len__(self):\n",
     "        return len(self.X_numerical_train)"
    ]
   },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "c213c4d7",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "train_set = SiameseDataloader(X_numerical_train, y_train, X_text_train, MAX_LEN, tokenizer)\n",
+    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "85256bf2",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "d4d32175",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "7b3c85db",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "4aaa2d36",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "7aae82f2",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "59aa0976",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "83883653",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "f4c1d854",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "e36a1798",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
   {
    "cell_type": "markdown",
-   "id": "4c3a0034",
+   "id": "23af8526",
    "metadata": {},
    "source": [
     "## Build model\n"
@@ -673,24 +483,10 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 74,
-   "id": "651047c6",
+   "execution_count": null,
+   "id": "84540ad9",
    "metadata": {},
-   "outputs": [
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "Downloading (…)olve/main/vocab.json: 100%|███| 899k/899k [00:00<00:00, 1.06MB/s]\n",
-      "Downloading (…)olve/main/merges.txt: 100%|████| 456k/456k [00:00<00:00, 621kB/s]\n",
-      "Downloading (…)lve/main/config.json: 100%|██████| 482/482 [00:00<00:00, 193kB/s]\n",
-      "Downloading pytorch_model.bin: 100%|███████| 1.43G/1.43G [06:27<00:00, 3.68MB/s]\n",
-      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
-      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
-      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
-     ]
-    }
-   ],
+   "outputs": [],
    "source": [
     "\n",
     "\n",
@@ -698,11 +494,23 @@
     "\n",
     "class SiameseModel(nn.Module):\n",
     "    def __init__(self, input_dim1, input_dim2, \n",
-    "                 hidden_dim1, hidden_dim2, hidden_dim3, \n",
+    "                 hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4,\n",
     "                 num_layers1, num_layers2, output_dim1, output_dim2):\n",
     "        super(SiameseModel, self).__init__()\n",
-    "        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
-    "        self.text_encoder = RobertaModel.from_pretrained('roberta-large')\n",
+    "        self.input_dim1 = input_dim1\n",
+    "        self.input_dim2 = input_dim2\n",
+    "        self.hidden_dim1 = hidden_dim1\n",
+    "        self.hidden_dim2 = hidden_dim2\n",
+    "        self.hidden_dim3 = hidden_dim3\n",
+    "        self.hidden_dim4 = hidden_dim4\n",
+    "        self.num_layers1 = num_layers1\n",
+    "        self.num_layers2 = num_layers2\n",
+    "        self.output_dim1 = output_dim1\n",
+    "        self.output_dim2 = output_dim2\n",
+    "        \n",
+    "        \n",
+    "\n",
+    "        self.roberta = RobertaModel.from_pretrained(\"roberta-large\")\n",
     "        \n",
     "        \n",
     "        self.lstm1 = nn.LSTM(input_dim1, hidden_dim1, num_layers1, batch_first=True)\n",
@@ -712,40 +520,57 @@
     "        self.fc1 = nn.Linear(hidden_dim1, output_dim1)\n",
     "        self.fc2 = nn.Linear(hidden_dim2, output_dim2)\n",
     "        self.fc3 = nn.Linear(output_dim1+output_dim2, hidden_dim3)\n",
-    "        self.fc4 = nn.Linear(hidden_dim3, 3)\n",
+    "        self.fc4 = nn.Linear(hidden_dim3, hidden_dim4)\n",
+    "        self.fc5 = nn.Linear(hidden_dim4, 3)\n",
     "        \n",
     "        \n",
     "        \n",
-    "    def forward(self, x1, x2):\n",
+    "    def forward(self, x1, ids, masks, token_type_ids):\n",
     "        #left tower with numerical features\n",
-    "        h10 = torch.zeros(self.num_layers1, x1.size(0), self.hidden_dim1).requires_grad_()\n",
-    "        c10 = torch.zeros(self.num_layers1, x1.size(0), self.hidden_dim1).requires_grad_()\n",
-    "        out1, (h1n, c1n) = self.lstm1(x1, (h10.detach(), c10.detach()))\n",
-    "        out1 = self.fc1(out1[:, -1, :]) \n",
     "        \n",
+    "        h_10 = Variable(torch.zeros(self.num_layers1, x1.size(0), self.hidden_dim1))\n",
+    "        c_10 = Variable(torch.zeros(self.num_layers1, x1.size(0), self.hidden_dim1))\n",
+    "        ula1, (h_out1, _) = self.lstm1(x1, (h_10, c_10))\n",
+    "        h_out1 = h_out1.view(-1, self.hidden_dim1)\n",
+    "        out1 = self.fc1(h_out1)\n",
     "        \n",
-    "        # right tower with roberta on textual features\n",
-    "        encoded_input = self.tokenizer(x2, return_tensors='pt')\n",
-    "        e2 = self.text_encoder(**encoded_input)\n",
     "        \n",
-    "        h20 = torch.zeros(self.num_layers2, e2.size(0), self.hidden_dim2).requires_grad_()\n",
-    "        c20 = torch.zeros(self.num_layers2, e2.size(0), self.hidden_dim2).requires_grad_()\n",
-    "        out2, (hn, cn) = self.lstm2(x2, (h20.detach(), c20.detach()))\n",
-    "        out2 = self.fc2(out2[:, -1, :]) \n",
+    "        \n",
+    "\n",
+    "        e2 = torch.zeros()\n",
+    "        # right tower with roberta on textual features  \n",
+    "        for k in range(ids.shape[1]):\n",
+    "            seq_ids = ids[:,k,:]\n",
+    "            seq_masks = masks[:,k,:]\n",
+    "            seq_token_type_ids = token_type_ids[:,k,:]\n",
+    "            \n",
+    "            \n",
+    "            e2k = self.roberta(input_ids= seq_ids, attention_mask=seq_masks, token_type_ids=seq_token_type_ids)\n",
+    "            e2k = e2k[0][:, 0, :]\n",
+    "    \n",
+    "        \n",
+    "        \n",
+    "        h_20 = Variable(torch.zeros(self.num_layers2, e2.size(0), self.hidden_dim2))\n",
+    "        c_20 = Variable(torch.zeros(self.num_layers2, e2.size(0), self.hidden_dim2))\n",
+    "        ula2, (h_out2, _) = self.lstm2(x2, (h_20, c_20))\n",
+    "        h_out2 = h_out2.view(-1, self.hidden_dim2)\n",
+    "        out2 = self.fc2(h_out2)\n",
     "        \n",
     "        \n",
     "        \n",
-    "\n",
     "        \n",
-    "        output = torch.cat((out1, out2),1)\n",
-    "        output = F.relu(self.fc3(output))\n",
-    "        output = self.fc4(output)\n",
-    "        return output\n",
+    "        # siamese merging layers\n",
+    "        \n",
+    "#         output = torch.cat((out1, out2),1)\n",
+    "#         output = F.relu(self.fc3(output))\n",
+    "#         output = F.relu(self.fc4(output))\n",
+    "#         output = self.fc5(output)\n",
+    "#         return output\n",
     "    \n",
     "#TODO : correct these values\n",
     "model = SiameseModel(input_dim1 = 8, input_dim2 = 1024, \n",
-    "                 hidden_dim1 = 10, hidden_dim2 = 800, hidden_dim3 = 500, \n",
-    "                 num_layers1 = 1, num_layers2 = 1, output_dim1 = 50, output_dim2 = 50)\n",
+    "                 hidden_dim1 = 20, hidden_dim2 = 768, hidden_dim3 = 128, hidden_dim4 = 64,\n",
+    "                 num_layers1 = 1, num_layers2 = 1, output_dim1 = 10, output_dim2 = 256)\n",
     "\n",
     "\n",
     "    \n",
@@ -754,1044 +579,20 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 75,
-   "id": "79b6a8b9",
-   "metadata": {
-    "collapsed": true
-   },
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "SiameseModel(\n",
-      "  (text_encoder): RobertaModel(\n",
-      "    (embeddings): RobertaEmbeddings(\n",
-      "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
-      "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
-      "      (token_type_embeddings): Embedding(1, 1024)\n",
-      "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "      (dropout): Dropout(p=0.1, inplace=False)\n",
-      "    )\n",
-      "    (encoder): RobertaEncoder(\n",
-      "      (layer): ModuleList(\n",
-      "        (0): RobertaLayer(\n",
-      "          (attention): RobertaAttention(\n",
-      "            (self): RobertaSelfAttention(\n",
-      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "            (output): RobertaSelfOutput(\n",
-      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "          )\n",
-      "          (intermediate): RobertaIntermediate(\n",
-      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
-      "            (intermediate_act_fn): GELUActivation()\n",
-      "          )\n",
-      "          (output): RobertaOutput(\n",
-      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
-      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "            (dropout): Dropout(p=0.1, inplace=False)\n",
-      "          )\n",
-      "        )\n",
-      "        (1): RobertaLayer(\n",
-      "          (attention): RobertaAttention(\n",
-      "            (self): RobertaSelfAttention(\n",
-      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "            (output): RobertaSelfOutput(\n",
-      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "          )\n",
-      "          (intermediate): RobertaIntermediate(\n",
-      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
-      "            (intermediate_act_fn): GELUActivation()\n",
-      "          )\n",
-      "          (output): RobertaOutput(\n",
-      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
-      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "            (dropout): Dropout(p=0.1, inplace=False)\n",
-      "          )\n",
-      "        )\n",
-      "        (2): RobertaLayer(\n",
-      "          (attention): RobertaAttention(\n",
-      "            (self): RobertaSelfAttention(\n",
-      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "            (output): RobertaSelfOutput(\n",
-      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "          )\n",
-      "          (intermediate): RobertaIntermediate(\n",
-      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
-      "            (intermediate_act_fn): GELUActivation()\n",
-      "          )\n",
-      "          (output): RobertaOutput(\n",
-      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
-      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "            (dropout): Dropout(p=0.1, inplace=False)\n",
-      "          )\n",
-      "        )\n",
-      "        (3): RobertaLayer(\n",
-      "          (attention): RobertaAttention(\n",
-      "            (self): RobertaSelfAttention(\n",
-      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "            (output): RobertaSelfOutput(\n",
-      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "          )\n",
-      "          (intermediate): RobertaIntermediate(\n",
-      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
-      "            (intermediate_act_fn): GELUActivation()\n",
-      "          )\n",
-      "          (output): RobertaOutput(\n",
-      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
-      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "            (dropout): Dropout(p=0.1, inplace=False)\n",
-      "          )\n",
-      "        )\n",
-      "        (4): RobertaLayer(\n",
-      "          (attention): RobertaAttention(\n",
-      "            (self): RobertaSelfAttention(\n",
-      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "            (output): RobertaSelfOutput(\n",
-      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "          )\n",
-      "          (intermediate): RobertaIntermediate(\n",
-      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
-      "            (intermediate_act_fn): GELUActivation()\n",
-      "          )\n",
-      "          (output): RobertaOutput(\n",
-      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
-      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "            (dropout): Dropout(p=0.1, inplace=False)\n",
-      "          )\n",
-      "        )\n",
-      "        (5): RobertaLayer(\n",
-      "          (attention): RobertaAttention(\n",
-      "            (self): RobertaSelfAttention(\n",
-      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "            (output): RobertaSelfOutput(\n",
-      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "          )\n",
-      "          (intermediate): RobertaIntermediate(\n",
-      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
-      "            (intermediate_act_fn): GELUActivation()\n",
-      "          )\n",
-      "          (output): RobertaOutput(\n",
-      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
-      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "            (dropout): Dropout(p=0.1, inplace=False)\n",
-      "          )\n",
-      "        )\n",
-      "        (6): RobertaLayer(\n",
-      "          (attention): RobertaAttention(\n",
-      "            (self): RobertaSelfAttention(\n",
-      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "            (output): RobertaSelfOutput(\n",
-      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "          )\n",
-      "          (intermediate): RobertaIntermediate(\n",
-      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
-      "            (intermediate_act_fn): GELUActivation()\n",
-      "          )\n",
-      "          (output): RobertaOutput(\n",
-      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
-      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "            (dropout): Dropout(p=0.1, inplace=False)\n",
-      "          )\n",
-      "        )\n",
-      "        (7): RobertaLayer(\n",
-      "          (attention): RobertaAttention(\n",
-      "            (self): RobertaSelfAttention(\n",
-      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "            (output): RobertaSelfOutput(\n",
-      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "          )\n",
-      "          (intermediate): RobertaIntermediate(\n",
-      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
-      "            (intermediate_act_fn): GELUActivation()\n",
-      "          )\n",
-      "          (output): RobertaOutput(\n",
-      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
-      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "            (dropout): Dropout(p=0.1, inplace=False)\n",
-      "          )\n",
-      "        )\n",
-      "        (8): RobertaLayer(\n",
-      "          (attention): RobertaAttention(\n",
-      "            (self): RobertaSelfAttention(\n",
-      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "            (output): RobertaSelfOutput(\n",
-      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "          )\n",
-      "          (intermediate): RobertaIntermediate(\n",
-      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
-      "            (intermediate_act_fn): GELUActivation()\n",
-      "          )\n",
-      "          (output): RobertaOutput(\n",
-      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
-      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "            (dropout): Dropout(p=0.1, inplace=False)\n",
-      "          )\n",
-      "        )\n",
-      "        (9): RobertaLayer(\n",
-      "          (attention): RobertaAttention(\n",
-      "            (self): RobertaSelfAttention(\n",
-      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "            (output): RobertaSelfOutput(\n",
-      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "          )\n",
-      "          (intermediate): RobertaIntermediate(\n",
-      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
-      "            (intermediate_act_fn): GELUActivation()\n",
-      "          )\n",
-      "          (output): RobertaOutput(\n",
-      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
-      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "            (dropout): Dropout(p=0.1, inplace=False)\n",
-      "          )\n",
-      "        )\n",
-      "        (10): RobertaLayer(\n",
-      "          (attention): RobertaAttention(\n",
-      "            (self): RobertaSelfAttention(\n",
-      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "            (output): RobertaSelfOutput(\n",
-      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "          )\n",
-      "          (intermediate): RobertaIntermediate(\n",
-      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
-      "            (intermediate_act_fn): GELUActivation()\n",
-      "          )\n",
-      "          (output): RobertaOutput(\n",
-      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
-      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "            (dropout): Dropout(p=0.1, inplace=False)\n",
-      "          )\n",
-      "        )\n",
-      "        (11): RobertaLayer(\n",
-      "          (attention): RobertaAttention(\n",
-      "            (self): RobertaSelfAttention(\n",
-      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "            (output): RobertaSelfOutput(\n",
-      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "          )\n",
-      "          (intermediate): RobertaIntermediate(\n",
-      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
-      "            (intermediate_act_fn): GELUActivation()\n",
-      "          )\n",
-      "          (output): RobertaOutput(\n",
-      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
-      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "            (dropout): Dropout(p=0.1, inplace=False)\n",
-      "          )\n",
-      "        )\n",
-      "        (12): RobertaLayer(\n",
-      "          (attention): RobertaAttention(\n",
-      "            (self): RobertaSelfAttention(\n",
-      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "            (output): RobertaSelfOutput(\n",
-      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "          )\n",
-      "          (intermediate): RobertaIntermediate(\n",
-      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
-      "            (intermediate_act_fn): GELUActivation()\n",
-      "          )\n",
-      "          (output): RobertaOutput(\n",
-      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
-      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "            (dropout): Dropout(p=0.1, inplace=False)\n",
-      "          )\n",
-      "        )\n",
-      "        (13): RobertaLayer(\n",
-      "          (attention): RobertaAttention(\n",
-      "            (self): RobertaSelfAttention(\n",
-      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "            (output): RobertaSelfOutput(\n",
-      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "          )\n",
-      "          (intermediate): RobertaIntermediate(\n",
-      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
-      "            (intermediate_act_fn): GELUActivation()\n",
-      "          )\n",
-      "          (output): RobertaOutput(\n",
-      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
-      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "            (dropout): Dropout(p=0.1, inplace=False)\n",
-      "          )\n",
-      "        )\n",
-      "        (14): RobertaLayer(\n",
-      "          (attention): RobertaAttention(\n",
-      "            (self): RobertaSelfAttention(\n",
-      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "            (output): RobertaSelfOutput(\n",
-      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "          )\n",
-      "          (intermediate): RobertaIntermediate(\n",
-      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
-      "            (intermediate_act_fn): GELUActivation()\n",
-      "          )\n",
-      "          (output): RobertaOutput(\n",
-      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
-      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "            (dropout): Dropout(p=0.1, inplace=False)\n",
-      "          )\n",
-      "        )\n",
-      "        (15): RobertaLayer(\n",
-      "          (attention): RobertaAttention(\n",
-      "            (self): RobertaSelfAttention(\n",
-      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "            (output): RobertaSelfOutput(\n",
-      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "          )\n",
-      "          (intermediate): RobertaIntermediate(\n",
-      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
-      "            (intermediate_act_fn): GELUActivation()\n",
-      "          )\n",
-      "          (output): RobertaOutput(\n",
-      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
-      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "            (dropout): Dropout(p=0.1, inplace=False)\n",
-      "          )\n",
-      "        )\n",
-      "        (16): RobertaLayer(\n",
-      "          (attention): RobertaAttention(\n",
-      "            (self): RobertaSelfAttention(\n",
-      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "            (output): RobertaSelfOutput(\n",
-      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "          )\n",
-      "          (intermediate): RobertaIntermediate(\n",
-      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
-      "            (intermediate_act_fn): GELUActivation()\n",
-      "          )\n",
-      "          (output): RobertaOutput(\n",
-      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
-      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "            (dropout): Dropout(p=0.1, inplace=False)\n",
-      "          )\n",
-      "        )\n",
-      "        (17): RobertaLayer(\n",
-      "          (attention): RobertaAttention(\n",
-      "            (self): RobertaSelfAttention(\n",
-      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "            (output): RobertaSelfOutput(\n",
-      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "          )\n",
-      "          (intermediate): RobertaIntermediate(\n",
-      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
-      "            (intermediate_act_fn): GELUActivation()\n",
-      "          )\n",
-      "          (output): RobertaOutput(\n",
-      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
-      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "            (dropout): Dropout(p=0.1, inplace=False)\n",
-      "          )\n",
-      "        )\n",
-      "        (18): RobertaLayer(\n",
-      "          (attention): RobertaAttention(\n",
-      "            (self): RobertaSelfAttention(\n",
-      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "            (output): RobertaSelfOutput(\n",
-      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "          )\n",
-      "          (intermediate): RobertaIntermediate(\n",
-      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
-      "            (intermediate_act_fn): GELUActivation()\n",
-      "          )\n",
-      "          (output): RobertaOutput(\n",
-      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
-      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "            (dropout): Dropout(p=0.1, inplace=False)\n",
-      "          )\n",
-      "        )\n",
-      "        (19): RobertaLayer(\n",
-      "          (attention): RobertaAttention(\n",
-      "            (self): RobertaSelfAttention(\n",
-      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "            (output): RobertaSelfOutput(\n",
-      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "          )\n",
-      "          (intermediate): RobertaIntermediate(\n",
-      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
-      "            (intermediate_act_fn): GELUActivation()\n",
-      "          )\n",
-      "          (output): RobertaOutput(\n",
-      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
-      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "            (dropout): Dropout(p=0.1, inplace=False)\n",
-      "          )\n",
-      "        )\n",
-      "        (20): RobertaLayer(\n",
-      "          (attention): RobertaAttention(\n",
-      "            (self): RobertaSelfAttention(\n",
-      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "            (output): RobertaSelfOutput(\n",
-      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "          )\n",
-      "          (intermediate): RobertaIntermediate(\n",
-      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
-      "            (intermediate_act_fn): GELUActivation()\n",
-      "          )\n",
-      "          (output): RobertaOutput(\n",
-      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
-      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "            (dropout): Dropout(p=0.1, inplace=False)\n",
-      "          )\n",
-      "        )\n",
-      "        (21): RobertaLayer(\n",
-      "          (attention): RobertaAttention(\n",
-      "            (self): RobertaSelfAttention(\n",
-      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "            (output): RobertaSelfOutput(\n",
-      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "          )\n",
-      "          (intermediate): RobertaIntermediate(\n",
-      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
-      "            (intermediate_act_fn): GELUActivation()\n",
-      "          )\n",
-      "          (output): RobertaOutput(\n",
-      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
-      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "            (dropout): Dropout(p=0.1, inplace=False)\n",
-      "          )\n",
-      "        )\n",
-      "        (22): RobertaLayer(\n",
-      "          (attention): RobertaAttention(\n",
-      "            (self): RobertaSelfAttention(\n",
-      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "            (output): RobertaSelfOutput(\n",
-      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "          )\n",
-      "          (intermediate): RobertaIntermediate(\n",
-      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
-      "            (intermediate_act_fn): GELUActivation()\n",
-      "          )\n",
-      "          (output): RobertaOutput(\n",
-      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
-      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "            (dropout): Dropout(p=0.1, inplace=False)\n",
-      "          )\n",
-      "        )\n",
-      "        (23): RobertaLayer(\n",
-      "          (attention): RobertaAttention(\n",
-      "            (self): RobertaSelfAttention(\n",
-      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "            (output): RobertaSelfOutput(\n",
-      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "              (dropout): Dropout(p=0.1, inplace=False)\n",
-      "            )\n",
-      "          )\n",
-      "          (intermediate): RobertaIntermediate(\n",
-      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
-      "            (intermediate_act_fn): GELUActivation()\n",
-      "          )\n",
-      "          (output): RobertaOutput(\n",
-      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
-      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
-      "            (dropout): Dropout(p=0.1, inplace=False)\n",
-      "          )\n",
-      "        )\n",
-      "      )\n",
-      "    )\n",
-      "    (pooler): RobertaPooler(\n",
-      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
-      "      (activation): Tanh()\n",
-      "    )\n",
-      "  )\n",
-      "  (lstm1): LSTM(8, 10, batch_first=True)\n",
-      "  (lstm2): LSTM(768, 800, batch_first=True)\n",
-      "  (fc1): Linear(in_features=10, out_features=50, bias=True)\n",
-      "  (fc2): Linear(in_features=800, out_features=50, bias=True)\n",
-      "  (fc3): Linear(in_features=100, out_features=500, bias=True)\n",
-      "  (fc4): Linear(in_features=500, out_features=3, bias=True)\n",
-      ")\n",
-      "407\n",
-      "torch.Size([50265, 1024])\n",
-      "torch.Size([514, 1024])\n",
-      "torch.Size([1, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([4096, 1024])\n",
-      "torch.Size([4096])\n",
-      "torch.Size([1024, 4096])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([4096, 1024])\n",
-      "torch.Size([4096])\n",
-      "torch.Size([1024, 4096])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([4096, 1024])\n",
-      "torch.Size([4096])\n",
-      "torch.Size([1024, 4096])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([4096, 1024])\n",
-      "torch.Size([4096])\n",
-      "torch.Size([1024, 4096])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([4096, 1024])\n",
-      "torch.Size([4096])\n",
-      "torch.Size([1024, 4096])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([4096, 1024])\n",
-      "torch.Size([4096])\n",
-      "torch.Size([1024, 4096])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([4096, 1024])\n",
-      "torch.Size([4096])\n",
-      "torch.Size([1024, 4096])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([4096, 1024])\n",
-      "torch.Size([4096])\n",
-      "torch.Size([1024, 4096])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([4096, 1024])\n",
-      "torch.Size([4096])\n",
-      "torch.Size([1024, 4096])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([4096, 1024])\n",
-      "torch.Size([4096])\n",
-      "torch.Size([1024, 4096])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([4096, 1024])\n",
-      "torch.Size([4096])\n",
-      "torch.Size([1024, 4096])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([4096, 1024])\n",
-      "torch.Size([4096])\n",
-      "torch.Size([1024, 4096])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([4096, 1024])\n",
-      "torch.Size([4096])\n",
-      "torch.Size([1024, 4096])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([4096, 1024])\n",
-      "torch.Size([4096])\n",
-      "torch.Size([1024, 4096])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([4096, 1024])\n",
-      "torch.Size([4096])\n",
-      "torch.Size([1024, 4096])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([4096, 1024])\n",
-      "torch.Size([4096])\n",
-      "torch.Size([1024, 4096])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([4096, 1024])\n",
-      "torch.Size([4096])\n",
-      "torch.Size([1024, 4096])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([4096, 1024])\n",
-      "torch.Size([4096])\n",
-      "torch.Size([1024, 4096])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([4096, 1024])\n",
-      "torch.Size([4096])\n",
-      "torch.Size([1024, 4096])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([4096, 1024])\n",
-      "torch.Size([4096])\n",
-      "torch.Size([1024, 4096])\n",
-      "torch.Size([1024])\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([4096, 1024])\n",
-      "torch.Size([4096])\n",
-      "torch.Size([1024, 4096])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([4096, 1024])\n",
-      "torch.Size([4096])\n",
-      "torch.Size([1024, 4096])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([4096, 1024])\n",
-      "torch.Size([4096])\n",
-      "torch.Size([1024, 4096])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([4096, 1024])\n",
-      "torch.Size([4096])\n",
-      "torch.Size([1024, 4096])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([1024, 1024])\n",
-      "torch.Size([1024])\n",
-      "torch.Size([40, 8])\n",
-      "torch.Size([40, 10])\n",
-      "torch.Size([40])\n",
-      "torch.Size([40])\n",
-      "torch.Size([3200, 768])\n",
-      "torch.Size([3200, 800])\n",
-      "torch.Size([3200])\n",
-      "torch.Size([3200])\n",
-      "torch.Size([50, 10])\n",
-      "torch.Size([50])\n",
-      "torch.Size([50, 800])\n",
-      "torch.Size([50])\n",
-      "torch.Size([500, 100])\n",
-      "torch.Size([500])\n",
-      "torch.Size([3, 500])\n",
-      "torch.Size([3])\n"
-     ]
-    }
-   ],
+   "execution_count": null,
+   "id": "7170f16c",
+   "metadata": {},
+   "outputs": [],
    "source": [
     "print(model)\n",
     "print(len(list(model.parameters())))\n",
-    "for i in range(len(list(model.parameters()))):\n",
-    "    print(list(model.parameters())[i].size())\n"
+    "# for i in range(len(list(model.parameters()))):\n",
+    "#     print(list(model.parameters())[i].size())\n"
    ]
   },
   {
    "cell_type": "markdown",
-   "id": "bd167a89",
+   "id": "4c05bbad",
    "metadata": {},
    "source": [
     "## Train model"
@@ -1799,139 +600,83 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 69,
-   "id": "6f9cc650",
+   "execution_count": 1,
+   "id": "0474d1ea",
    "metadata": {
     "collapsed": true
    },
    "outputs": [
     {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Epoch  10 MSE:  0.8407482504844666\n",
-      "Epoch  20 MSE:  0.8068487048149109\n",
-      "Epoch  30 MSE:  0.7411683797836304\n",
-      "Epoch  40 MSE:  0.7207826375961304\n",
-      "Epoch  50 MSE:  0.47958889603614807\n",
-      "Epoch  60 MSE:  0.29257601499557495\n",
-      "Epoch  70 MSE:  0.14471979439258575\n",
-      "Epoch  80 MSE:  0.045132964849472046\n",
-      "Epoch  90 MSE:  0.01147476676851511\n",
-      "Epoch  100 MSE:  0.0039354609325528145\n",
-      "Epoch  110 MSE:  0.001934159197844565\n",
-      "Epoch  120 MSE:  0.0012813452631235123\n",
-      "Epoch  130 MSE:  0.0009837034158408642\n",
-      "Epoch  140 MSE:  0.0008080229163169861\n",
-      "Epoch  150 MSE:  0.0006962246261537075\n",
-      "Epoch  160 MSE:  0.0006137943710200489\n",
-      "Epoch  170 MSE:  0.0005495513323694468\n",
-      "Epoch  180 MSE:  0.0004968825378455222\n",
-      "Epoch  190 MSE:  0.00045257911551743746\n",
-      "Epoch  200 MSE:  0.0004146856372244656\n",
-      "Epoch  210 MSE:  0.00038184275035746396\n",
-      "Epoch  220 MSE:  0.0003531062975525856\n",
-      "Epoch  230 MSE:  0.0003277502255514264\n",
-      "Epoch  240 MSE:  0.0003052429819945246\n",
-      "Epoch  250 MSE:  0.0002851351164281368\n",
-      "Epoch  260 MSE:  0.000267079594777897\n",
-      "Epoch  270 MSE:  0.00025078782346099615\n",
-      "Epoch  280 MSE:  0.00023602470173500478\n",
-      "Epoch  290 MSE:  0.00022259485558606684\n",
-      "Epoch  300 MSE:  0.0002103321603499353\n",
-      "Epoch  310 MSE:  0.00019910575065296143\n",
-      "Epoch  320 MSE:  0.00018879210983868688\n",
-      "Epoch  330 MSE:  0.00017928759916685522\n",
-      "Epoch  340 MSE:  0.0001705099130049348\n",
-      "Epoch  350 MSE:  0.00016238645184785128\n",
-      "Epoch  360 MSE:  0.0001548424334032461\n",
-      "Epoch  370 MSE:  0.0001478299527661875\n",
-      "Epoch  380 MSE:  0.00014129634655546397\n",
-      "Epoch  390 MSE:  0.0001352005056105554\n",
-      "Epoch  400 MSE:  0.00012949401570949703\n",
-      "Epoch  410 MSE:  0.0001241543359356001\n",
-      "Epoch  420 MSE:  0.00011914050992345437\n",
-      "Epoch  430 MSE:  0.00011443185940152034\n",
-      "Epoch  440 MSE:  0.00010999880032613873\n",
-      "Epoch  450 MSE:  0.00010582833056105301\n",
-      "Epoch  460 MSE:  0.00010189037857344374\n",
-      "Epoch  470 MSE:  9.817193495109677e-05\n",
-      "Epoch  480 MSE:  9.465834591537714e-05\n",
-      "Epoch  490 MSE:  9.132863488048315e-05\n",
-      "Epoch  500 MSE:  8.817756315693259e-05\n",
-      "Epoch  510 MSE:  8.518871618434787e-05\n",
-      "Epoch  520 MSE:  8.234671986429021e-05\n",
-      "Epoch  530 MSE:  7.964819815242663e-05\n",
-      "Epoch  540 MSE:  7.708567136432976e-05\n",
-      "Epoch  550 MSE:  7.464136433554813e-05\n",
-      "Epoch  560 MSE:  7.231043127831072e-05\n",
-      "Epoch  570 MSE:  7.009095861576498e-05\n",
-      "Epoch  580 MSE:  6.797187961637974e-05\n",
-      "Epoch  590 MSE:  6.594765727641061e-05\n",
-      "Epoch  600 MSE:  6.401153950719163e-05\n",
-      "Epoch  610 MSE:  6.216402107384056e-05\n",
-      "Epoch  620 MSE:  6.039403160684742e-05\n",
-      "Epoch  630 MSE:  5.8696503401733935e-05\n",
-      "Epoch  640 MSE:  5.707578020519577e-05\n",
-      "Epoch  650 MSE:  5.5515476560685784e-05\n",
-      "Epoch  660 MSE:  5.4020911193219945e-05\n",
-      "Epoch  670 MSE:  5.2586528909159824e-05\n",
-      "Epoch  680 MSE:  5.1207523938501254e-05\n",
-      "Epoch  690 MSE:  4.988486034562811e-05\n",
-      "Epoch  700 MSE:  4.8607224016450346e-05\n",
-      "Epoch  710 MSE:  4.7382065531564876e-05\n",
-      "Epoch  720 MSE:  4.619977335096337e-05\n",
-      "Epoch  730 MSE:  4.506563345785253e-05\n",
-      "Epoch  740 MSE:  4.397338489070535e-05\n",
-      "Epoch  750 MSE:  4.2914838559227064e-05\n",
-      "Epoch  760 MSE:  4.189506944385357e-05\n",
-      "Epoch  770 MSE:  4.091285518370569e-05\n",
-      "Epoch  780 MSE:  3.996507075498812e-05\n",
-      "Epoch  790 MSE:  3.9044738514348865e-05\n",
-      "Epoch  800 MSE:  3.8160764233907685e-05\n",
-      "Epoch  810 MSE:  3.730784737854265e-05\n",
-      "Epoch  820 MSE:  3.647780977189541e-05\n",
-      "Epoch  830 MSE:  3.567449311958626e-05\n",
-      "Epoch  840 MSE:  3.489935625111684e-05\n",
-      "Epoch  850 MSE:  3.4149736166000366e-05\n",
-      "Epoch  860 MSE:  3.34188953274861e-05\n",
-      "Epoch  870 MSE:  3.271406239946373e-05\n",
-      "Epoch  880 MSE:  3.20357212331146e-05\n",
-      "Epoch  890 MSE:  3.137230305583216e-05\n",
-      "Epoch  900 MSE:  3.072886829613708e-05\n",
-      "Epoch  910 MSE:  3.0105424229986966e-05\n",
-      "Epoch  920 MSE:  2.950196540041361e-05\n",
-      "Epoch  930 MSE:  2.891439726226963e-05\n",
-      "Epoch  940 MSE:  2.8345370083115995e-05\n",
-      "Epoch  950 MSE:  2.7794885681942105e-05\n",
-      "Epoch  960 MSE:  2.725427293626126e-05\n",
-      "Epoch  970 MSE:  2.6731717298389412e-05\n",
-      "Epoch  980 MSE:  2.62262619799003e-05\n",
-      "Epoch  990 MSE:  2.572947778389789e-05\n"
+     "ename": "NameError",
+     "evalue": "name 'nn' is not defined",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
+      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m criterion \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      2\u001b[0m optimiser \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m      3\u001b[0m loss_arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(num_epochs)\n",
+      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
      ]
     }
    ],
    "source": [
-    "\n",
     "criterion = nn.CrossEntropyLoss()\n",
-    "\n",
     "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
-    "\n",
-    "num_epochs = 20\n",
-    "loss_arr = np.zeros(num_epochs)\n",
-    "\n",
-    "for t in range(num_epochs):\n",
+    "loss_arr = np.zeros(num_epochs)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "ac7c399e",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "roberta = RobertaModel.from_pretrained(\"roberta-large\")\n",
+    "for epoch in range(num_epochs):\n",
+    "\n",
+    "    for idx, data in tqdm(enumerate(train_loader, 0)):\n",
+    "        x_numerical = data['x_numerical'].to(device, dtype = torch.float)\n",
+    "        ids = data['ids'].to(device, dtype = torch.long)\n",
+    "        masks = data['mask'].to(device, dtype = torch.long)\n",
+    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
+    "        targets = data['targets'].to(device, dtype = torch.long)\n",
+    "\n",
+    "        # debugging roberta encoder\n",
+    "        '''\n",
+    "        debug start here\n",
+    "        '''\n",
+    "        for k in range(ids.shape[1]):\n",
+    "            seq_ids = ids[:,k,:]\n",
+    "            seq_masks = masks[:,k,:]\n",
+    "            seq_token_type_ids = token_type_ids[:,k,:]\n",
+    "\n",
+    "\n",
+    "            e2k = roberta(input_ids= seq_ids, attention_mask=seq_masks, token_type_ids=seq_token_type_ids)\n",
+    "            print(type(e2k))\n",
+    "            e2k1 = e2k[0][:, 0, :]\n",
+    "            print(type(e2k1))\n",
+    "            print((e2k1.shape))\n",
+    "\n",
+    "            print(type(e2k1))\n",
+    "        \n",
+    "        '''\n",
+    "        debug end here\n",
+    "        '''\n",
+    "        \n",
+    "    #     print(ids.shape)\n",
+    "    #     print(masks.shape)\n",
+    "    #     print(token_type_ids.shape)\n",
     "    \n",
-    "    for batch_idx, (x1, x2, y_train) in enumerate(train_loader):\n",
+    "        y_pred = model(x_numerical, ids, masks, token_type_ids)\n",
     "        \n",
-    "        # Forward pass\n",
-    "        y_train_pred = model(x1, x2)\n",
-    "\n",
-    "\n",
-    "        loss = criterion(y_train_pred, y_train)\n",
-    "        if t % 10 == 0 and t !=0:\n",
+    "        if idx > 1:\n",
+    "            break\n",
+    "    \n",
+    "        loss = criterion(y_pred, y_train)\n",
+    "        if epoch % 10 == 0 and epoch !=0:\n",
     "            print(\"Epoch \", t, \"CELoss: \", loss.item())\n",
-    "        loss_arr[t] = loss.item()\n",
+    "        loss_arr[t=epoch] = loss.item()\n",
+    "        wandb.log({'celoss': loss.item().avg, 'epoch': epoch, 'batch_id': batch_id})\n",
     "\n",
     "        # Zero out gradient, else they will accumulate between epochs\n",
     "        optimiser.zero_grad()\n",
@@ -1942,23 +687,39 @@
     "        # Update parameters\n",
     "        optimiser.step()\n",
     "    \n",
-    "    \n",
-    "\n",
-    "    \n",
-    "\n",
-    "\n",
-    "\n",
-    "   \n",
-    "        \n",
     "        \n",
     "        \n",
-    "\n"
+    "        "
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
-   "id": "65603dd5",
+   "id": "aa770bdf",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "3477c9f7",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "20177fcc",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "de6ac270",
    "metadata": {},
    "outputs": [],
    "source": [
@@ -1970,7 +731,7 @@
   {
    "cell_type": "code",
    "execution_count": null,
-   "id": "17854251",
+   "id": "c9231c9f",
    "metadata": {},
    "outputs": [],
    "source": [
