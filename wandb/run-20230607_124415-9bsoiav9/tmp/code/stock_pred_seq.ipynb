{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab479ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yfinance\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34241ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import mpl, plt\n",
    "import math, time\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from operator import itemgetter\n",
    "from tqdm import tqdm\n",
    "from math import sqrt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from transformers import RobertaTokenizer, RobertaModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdc57aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9c4dbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvisriv\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/visriv/Documents/Git/xai-seq/wandb/run-20230607_124415-9bsoiav9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/visriv/stock_prediction/runs/9bsoiav9' target=\"_blank\">rose-violet-2</a></strong> to <a href='https://wandb.ai/visriv/stock_prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/visriv/stock_prediction' target=\"_blank\">https://wandb.ai/visriv/stock_prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/visriv/stock_prediction/runs/9bsoiav9' target=\"_blank\">https://wandb.ai/visriv/stock_prediction/runs/9bsoiav9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/visriv/stock_prediction/runs/9bsoiav9?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1221e0d60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "wandb.init(project=\"stock_prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195a4f49",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26a1ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_days_to_lookforward = 1\n",
    "no_of_days_to_lookback = 5\n",
    "up_threshold = 0.015\n",
    "down_threshold = -0.015\n",
    "max_text_per_iter = 100\n",
    "batch_size = 1\n",
    "MAX_LEN = 1000\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0670a31",
   "metadata": {},
   "source": [
    "### Get stocks data for last N days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f866e929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64983989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "stock_symbols = [ 'XOM']\n",
    "no_of_days = 4*365\n",
    "\n",
    "EXPORT_DATA_FOLDER = './data/'\n",
    "\n",
    "# Set the start and end dates for the data \n",
    "# here matching it with dates of news text available\n",
    "start = datetime.strptime('2019/01/04', '%Y/%m/%d')\n",
    "end = datetime.strptime('2023/01/04', '%Y/%m/%d')\n",
    "\n",
    "\n",
    "# start = datetime.datetime.now() - datetime.timedelta(days=no_of_days)\n",
    "# end = datetime.datetime.now()\n",
    "\n",
    "for symbol in stock_symbols:\n",
    "    # Download the historical price and volume data using yfinance\n",
    "    data_raw = yf.download(symbol, start=start, end=end)\n",
    "\n",
    "    # Normalize features by percent of changes between today and yesterday\n",
    "    pct_change_open = data_raw['Open'].pct_change().fillna(0)\n",
    "    pct_change_high = data_raw['High'].pct_change().fillna(0)\n",
    "    pct_change_high_over_open = (data_raw['High']-data_raw['Open'])/data_raw['Open']\n",
    "    pct_change_low = data_raw['Low'].pct_change().fillna(0)\n",
    "    pct_change_low_over_open = (data_raw['Low']-data_raw['Open'])/data_raw['Open']\n",
    "    pct_change_close = data_raw['Close'].pct_change().fillna(0)\n",
    "    pct_change_close_over_open = (data_raw['Close']-data_raw['Open'])/data_raw['Open']\n",
    "    pct_change_adjclose = data_raw['Adj Close'].pct_change().fillna(0)\n",
    "    pct_change_adjclose_over_open = (data_raw['Adj Close']-data_raw['Open'])/data_raw['Open']\n",
    "    pct_change_volume = data_raw['Volume'].pct_change().fillna(0)\n",
    "\n",
    "    # Prepare labels: 2 means the close price of tomorow is higher than today's close price; 1 is down; 0 means the movement is between up_threshold and down_threshold\n",
    "    label = np.where(pct_change_close > up_threshold, 2, np.where(pct_change_close < down_threshold, 1, 0))[1:]\n",
    "    label = np.append(label, 0)\n",
    "\n",
    "    # Construct a data_norm data frame\n",
    "    data_norm = pd.DataFrame({'Open_norm':pct_change_open,\n",
    "                              'High_norm':pct_change_high,\n",
    "                              'Low_norm': pct_change_low,\n",
    "                              'Close_norm':pct_change_close,\n",
    "                              'Volume_norm':pct_change_volume,\n",
    "                              'High-Open_norm':pct_change_high_over_open,\n",
    "                              'Low-Open_norm':pct_change_low_over_open,\n",
    "                              'Close-Open_norm':pct_change_close_over_open,\n",
    "                              'Label_2up1down':label})\n",
    "\n",
    "    # Normalize by min-max normalization after the pct normalization\n",
    "    data_norm['Open_norm'] = data_norm['Open_norm'].apply(lambda x: (x - data_norm['Open_norm'].min()) / (data_norm['Open_norm'].max() - data_norm['Open_norm'].min()))\n",
    "    data_norm['High_norm'] = data_norm['High_norm'].apply(lambda x: (x - data_norm['High_norm'].min()) / (data_norm['High_norm'].max() - data_norm['High_norm'].min()))\n",
    "    data_norm['Low_norm'] = data_norm['Low_norm'].apply(lambda x: (x - data_norm['Low_norm'].min()) / (data_norm['Low_norm'].max() - data_norm['Low_norm'].min()))\n",
    "    data_norm['Close_norm'] = data_norm['Close_norm'].apply(lambda x: (x - data_norm['Close_norm'].min()) / (data_norm['Close_norm'].max() - data_norm['Close_norm'].min()))\n",
    "    data_norm['Volume_norm'] = data_norm['Volume_norm'].apply(lambda x: (x - data_norm['Volume_norm'].min()) / (data_norm['Volume_norm'].max() - data_norm['Volume_norm'].min()))\n",
    "    data_norm['High-Open_norm'] = data_norm['High-Open_norm'].apply(lambda x: (x - data_norm['High-Open_norm'].min()) / (data_norm['High-Open_norm'].max() - data_norm['High-Open_norm'].min()))\n",
    "    data_norm['Low-Open_norm'] = data_norm['Low-Open_norm'].apply(lambda x: (x - data_norm['Low-Open_norm'].min()) / (data_norm['Low-Open_norm'].max() - data_norm['Low-Open_norm'].min()))\n",
    "    data_norm['Close-Open_norm'] = data_norm['Close-Open_norm'].apply(lambda x: (x - data_norm['Close-Open_norm'].min()) / (data_norm['Close-Open_norm'].max() - data_norm['Close-Open_norm'].min()))\n",
    "\n",
    "    # Remove the first and the last row, becuase of NAN values\n",
    "    data_raw = data_raw.iloc[1:-1]\n",
    "    data_norm = data_norm.iloc[1:-1]\n",
    "\n",
    "    data_raw.to_csv(EXPORT_DATA_FOLDER+symbol+'_raw_data.csv', index=True)\n",
    "    data_norm.to_csv(EXPORT_DATA_FOLDER+symbol+'_norm_data.csv', index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7781d5f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2622d382",
   "metadata": {},
   "source": [
    "## TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b508e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "(2023-06-05)\n",
    "cuda support check\n",
    "//read textual data into correct shape\n",
    "hyperparam tuning: number of neurons: tune to right number of neurons in FC in model\n",
    "//max_text_per_iter -> code in dataloader to maintain the size \n",
    "\n",
    "(2023-06-05)\n",
    "cuda check\n",
    "roberta encoder fix\n",
    "multi label - how to create target label?\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47afaba",
   "metadata": {},
   "source": [
    "## Prep textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31a73958",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data_df = pd.read_csv('./data/XOM_20200401_20230401_medium.csv', sep= ',', header= 0)\n",
    "text_data_df = text_data_df[['Date', 'News']]\n",
    "\n",
    "\n",
    "text_data_df = text_data_df.groupby('Date')['News'].apply('$$$###'.join)\n",
    "\n",
    "text_data_df.index = pd.to_datetime(text_data_df.index, dayfirst=True)\n",
    "# text_data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cf882d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_norm</th>\n",
       "      <th>High_norm</th>\n",
       "      <th>Low_norm</th>\n",
       "      <th>Close_norm</th>\n",
       "      <th>Volume_norm</th>\n",
       "      <th>High-Open_norm</th>\n",
       "      <th>Low-Open_norm</th>\n",
       "      <th>Close-Open_norm</th>\n",
       "      <th>Label_2up1down</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-04-01</th>\n",
       "      <td>0.424588</td>\n",
       "      <td>0.325630</td>\n",
       "      <td>0.445426</td>\n",
       "      <td>0.444210</td>\n",
       "      <td>0.187485</td>\n",
       "      <td>0.493448</td>\n",
       "      <td>0.887338</td>\n",
       "      <td>0.568367</td>\n",
       "      <td>2</td>\n",
       "      <td>Global Polymers Market, By Type (Thermoplastic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-02</th>\n",
       "      <td>0.778879</td>\n",
       "      <td>0.795313</td>\n",
       "      <td>0.736065</td>\n",
       "      <td>0.797702</td>\n",
       "      <td>0.445139</td>\n",
       "      <td>0.821628</td>\n",
       "      <td>0.826839</td>\n",
       "      <td>0.730773</td>\n",
       "      <td>1</td>\n",
       "      <td>European Morning Briefing: U.S. Jobs Report Ey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-03</th>\n",
       "      <td>0.853804</td>\n",
       "      <td>0.435964</td>\n",
       "      <td>0.626517</td>\n",
       "      <td>0.372487</td>\n",
       "      <td>0.160943</td>\n",
       "      <td>0.174091</td>\n",
       "      <td>0.435487</td>\n",
       "      <td>0.093004</td>\n",
       "      <td>2</td>\n",
       "      <td>Nordic Morning Briefing: Services PMI Data in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-06</th>\n",
       "      <td>0.427455</td>\n",
       "      <td>0.266519</td>\n",
       "      <td>0.635065</td>\n",
       "      <td>0.619722</td>\n",
       "      <td>0.182685</td>\n",
       "      <td>0.235477</td>\n",
       "      <td>0.869836</td>\n",
       "      <td>0.546103</td>\n",
       "      <td>2</td>\n",
       "      <td>圖表 Texas Takes Two Punches -- Oil Shock and O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-07</th>\n",
       "      <td>0.876632</td>\n",
       "      <td>0.689534</td>\n",
       "      <td>0.761913</td>\n",
       "      <td>0.567103</td>\n",
       "      <td>0.283049</td>\n",
       "      <td>0.096776</td>\n",
       "      <td>0.676091</td>\n",
       "      <td>0.194020</td>\n",
       "      <td>2</td>\n",
       "      <td>Exxon Cuts Capital Spending by 30% in Response...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-23</th>\n",
       "      <td>0.542767</td>\n",
       "      <td>0.435239</td>\n",
       "      <td>0.669969</td>\n",
       "      <td>0.596883</td>\n",
       "      <td>0.188873</td>\n",
       "      <td>0.165444</td>\n",
       "      <td>0.996267</td>\n",
       "      <td>0.555196</td>\n",
       "      <td>0</td>\n",
       "      <td>Energy Transfer's Gulf Run gas pipeline gets U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-27</th>\n",
       "      <td>0.662985</td>\n",
       "      <td>0.484486</td>\n",
       "      <td>0.633508</td>\n",
       "      <td>0.546500</td>\n",
       "      <td>0.238297</td>\n",
       "      <td>0.108558</td>\n",
       "      <td>0.967843</td>\n",
       "      <td>0.504141</td>\n",
       "      <td>1</td>\n",
       "      <td>Butadiene Market, By Application (Polybutadien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-28</th>\n",
       "      <td>0.600374</td>\n",
       "      <td>0.392759</td>\n",
       "      <td>0.523834</td>\n",
       "      <td>0.424789</td>\n",
       "      <td>0.195329</td>\n",
       "      <td>0.008094</td>\n",
       "      <td>0.853954</td>\n",
       "      <td>0.357091</td>\n",
       "      <td>0</td>\n",
       "      <td>Exxon sues EU in move to block new windfall ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-29</th>\n",
       "      <td>0.508146</td>\n",
       "      <td>0.400534</td>\n",
       "      <td>0.562507</td>\n",
       "      <td>0.521098</td>\n",
       "      <td>0.222456</td>\n",
       "      <td>0.158359</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.522474</td>\n",
       "      <td>0</td>\n",
       "      <td>Global Car Care Products Market 2023-2027 Publ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-30</th>\n",
       "      <td>0.599916</td>\n",
       "      <td>0.444452</td>\n",
       "      <td>0.582740</td>\n",
       "      <td>0.531163</td>\n",
       "      <td>0.263573</td>\n",
       "      <td>0.172017</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.551144</td>\n",
       "      <td>1</td>\n",
       "      <td>Global Polypropylene Nonwoven Fabric Market 20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>694 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Open_norm  High_norm  Low_norm  Close_norm  Volume_norm  \\\n",
       "Date                                                                  \n",
       "2020-04-01   0.424588   0.325630  0.445426    0.444210     0.187485   \n",
       "2020-04-02   0.778879   0.795313  0.736065    0.797702     0.445139   \n",
       "2020-04-03   0.853804   0.435964  0.626517    0.372487     0.160943   \n",
       "2020-04-06   0.427455   0.266519  0.635065    0.619722     0.182685   \n",
       "2020-04-07   0.876632   0.689534  0.761913    0.567103     0.283049   \n",
       "...               ...        ...       ...         ...          ...   \n",
       "2022-12-23   0.542767   0.435239  0.669969    0.596883     0.188873   \n",
       "2022-12-27   0.662985   0.484486  0.633508    0.546500     0.238297   \n",
       "2022-12-28   0.600374   0.392759  0.523834    0.424789     0.195329   \n",
       "2022-12-29   0.508146   0.400534  0.562507    0.521098     0.222456   \n",
       "2022-12-30   0.599916   0.444452  0.582740    0.531163     0.263573   \n",
       "\n",
       "            High-Open_norm  Low-Open_norm  Close-Open_norm  Label_2up1down  \\\n",
       "Date                                                                         \n",
       "2020-04-01        0.493448       0.887338         0.568367               2   \n",
       "2020-04-02        0.821628       0.826839         0.730773               1   \n",
       "2020-04-03        0.174091       0.435487         0.093004               2   \n",
       "2020-04-06        0.235477       0.869836         0.546103               2   \n",
       "2020-04-07        0.096776       0.676091         0.194020               2   \n",
       "...                    ...            ...              ...             ...   \n",
       "2022-12-23        0.165444       0.996267         0.555196               0   \n",
       "2022-12-27        0.108558       0.967843         0.504141               1   \n",
       "2022-12-28        0.008094       0.853954         0.357091               0   \n",
       "2022-12-29        0.158359       1.000000         0.522474               0   \n",
       "2022-12-30        0.172017       1.000000         0.551144               1   \n",
       "\n",
       "                                                         News  \n",
       "Date                                                           \n",
       "2020-04-01  Global Polymers Market, By Type (Thermoplastic...  \n",
       "2020-04-02  European Morning Briefing: U.S. Jobs Report Ey...  \n",
       "2020-04-03  Nordic Morning Briefing: Services PMI Data in ...  \n",
       "2020-04-06   圖表 Texas Takes Two Punches -- Oil Shock and O...  \n",
       "2020-04-07  Exxon Cuts Capital Spending by 30% in Response...  \n",
       "...                                                       ...  \n",
       "2022-12-23  Energy Transfer's Gulf Run gas pipeline gets U...  \n",
       "2022-12-27  Butadiene Market, By Application (Polybutadien...  \n",
       "2022-12-28  Exxon sues EU in move to block new windfall ta...  \n",
       "2022-12-29  Global Car Care Products Market 2023-2027 Publ...  \n",
       "2022-12-30  Global Polypropylene Nonwoven Fabric Market 20...  \n",
       "\n",
       "[694 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_df = data_norm.join(text_data_df, how = 'inner')\n",
    "all_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56e91800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "all_train = all_train_df.values\n",
    "\n",
    "window_size = no_of_days_to_lookback\n",
    "\n",
    "X_numerical_train = []\n",
    "y_train = []\n",
    "X_text_train = []\n",
    "X_text_train_curr = []\n",
    "\n",
    "\n",
    "for i in range(window_size, len(all_train) - no_of_days_to_lookforward + 1):\n",
    "    X_numerical_train.append(all_train[i-window_size: i, :-2])\n",
    "    \n",
    "    # split and append sequence of text\n",
    "    curr_seq = all_train[i-window_size: i, -1]\n",
    "    for j in range(window_size):\n",
    "        split_curr_seq = curr_seq[window_size - 1 -j].split('$$$###')\n",
    "        X_text_train_curr = X_text_train_curr + split_curr_seq\n",
    "    \n",
    "    if len(X_text_train_curr) > max_text_per_iter:\n",
    "        X_text_train_curr = X_text_train_curr[:100]\n",
    "    \n",
    "    X_text_train.append(X_text_train_curr)\n",
    "        \n",
    "    # target labels\n",
    "    y_train.append(all_train[i:i+no_of_days_to_lookforward, -2])\n",
    "\n",
    "X_numerical_train, y_train = np.array(X_numerical_train).astype(np.float16), np.array(y_train).astype(np.int32)\n",
    "print(type(X_numerical_train))\n",
    "print(type(y_train))\n",
    "\n",
    "X_numerical_train = torch.from_numpy(X_numerical_train).type(torch.Tensor)\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8d18dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "689\n",
      "689\n",
      "689\n"
     ]
    }
   ],
   "source": [
    "print(len(X_numerical_train))\n",
    "print(len(X_text_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428c9aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_text_train))\n",
    "print(len(X_text_train[2]))\n",
    "print(X_text_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fffea64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8d057c9",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6be2e373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large', truncation=True, do_lower_case=True)\n",
    "\n",
    "class SiameseDataloader(Dataset):\n",
    "    \n",
    "    def __init__(self, X_numerical_train, y_train, X_text_train, MAX_LEN, tokenizer):\n",
    "        self.X_numerical_train = X_numerical_train\n",
    "        self.X_text_train = X_text_train\n",
    "        self.MAX_LEN = MAX_LEN\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        \n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        token_type_ids = []\n",
    "        input_seq = []\n",
    "\n",
    "        for sent in X_text_train[index]:\n",
    "            encoded_sent = self.tokenizer.encode_plus(\n",
    "                text=sent,\n",
    "                add_special_tokens=True,        # Add `[CLS]` and `[SEP]` special tokens\n",
    "                max_length=self.MAX_LEN,             # Choose max length to truncate/pad\n",
    "                pad_to_max_length=True,         # Pad sentence to max length \n",
    "                #return_attention_mask=True      # Return attention mask\n",
    "                return_token_type_ids=True\n",
    "                )\n",
    "            input_ids.append(encoded_sent.get('input_ids'))\n",
    "            attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "            token_type_ids.append(encoded_sent.get('token_type_ids'))\n",
    "\n",
    "        # Convert lists to tensors\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        attention_masks = torch.tensor(attention_masks)\n",
    "        token_type_ids = torch.tensor(token_type_ids)\n",
    "\n",
    "\n",
    "        return {\n",
    "            'x_numerical': X_numerical_train[index],\n",
    "            'ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(attention_masks, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(y_train[index], dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_numerical_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9e887d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = SiameseDataloader(X_numerical_train, y_train, X_text_train, MAX_LEN, tokenizer)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b73c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fb10c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08507a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0e6325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383bec74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a3afef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5ec5ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc2c3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b2aad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "404aa501",
   "metadata": {},
   "source": [
    "## Build model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72c11e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class SiameseModel(nn.Module):\n",
    "    def __init__(self, input_dim1, input_dim2, \n",
    "                 hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4,\n",
    "                 num_layers1, num_layers2, output_dim1, output_dim2):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.input_dim1 = input_dim1\n",
    "        self.input_dim2 = input_dim2\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.hidden_dim3 = hidden_dim3\n",
    "        self.hidden_dim4 = hidden_dim4\n",
    "        self.num_layers1 = num_layers1\n",
    "        self.num_layers2 = num_layers2\n",
    "        self.output_dim1 = output_dim1\n",
    "        self.output_dim2 = output_dim2\n",
    "        \n",
    "        \n",
    "\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta-large\")\n",
    "        \n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_dim1, hidden_dim1, num_layers1, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_dim2, hidden_dim2, num_layers2, batch_first=True)\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim1, output_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim2, output_dim2)\n",
    "        self.fc3 = nn.Linear(output_dim1+output_dim2, hidden_dim3)\n",
    "        self.fc4 = nn.Linear(hidden_dim3, hidden_dim4)\n",
    "        self.fc5 = nn.Linear(hidden_dim4, 3)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x1, ids, masks, token_type_ids):\n",
    "        #left tower with numerical features\n",
    "        \n",
    "        h_10 = Variable(torch.zeros(self.num_layers1, x1.size(0), self.hidden_dim1))\n",
    "        c_10 = Variable(torch.zeros(self.num_layers1, x1.size(0), self.hidden_dim1))\n",
    "        ula1, (h_out1, _) = self.lstm1(x1, (h_10, c_10))\n",
    "        h_out1 = h_out1.view(-1, self.hidden_dim1)\n",
    "        out1 = self.fc1(h_out1)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        e2 = torch.zeros()\n",
    "        # right tower with roberta on textual features  \n",
    "        #TODO\n",
    "        for k in range(ids.shape[1]):\n",
    "            seq_ids = ids[:,k,:]\n",
    "            seq_masks = masks[:,k,:]\n",
    "            seq_token_type_ids = token_type_ids[:,k,:]\n",
    "            \n",
    "            \n",
    "            e2k = self.roberta(input_ids= seq_ids, attention_mask=seq_masks, token_type_ids=seq_token_type_ids)\n",
    "            e2k = e2k[0][:, 0, :]\n",
    "    \n",
    "        \n",
    "        \n",
    "        h_20 = Variable(torch.zeros(self.num_layers2, e2.size(0), self.hidden_dim2))\n",
    "        c_20 = Variable(torch.zeros(self.num_layers2, e2.size(0), self.hidden_dim2))\n",
    "        ula2, (h_out2, _) = self.lstm2(x2, (h_20, c_20))\n",
    "        h_out2 = h_out2.view(-1, self.hidden_dim2)\n",
    "        out2 = self.fc2(h_out2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # siamese merging layers\n",
    "        \n",
    "#         output = torch.cat((out1, out2),1)\n",
    "#         output = F.relu(self.fc3(output))\n",
    "#         output = F.relu(self.fc4(output))\n",
    "#         output = self.fc5(output)\n",
    "#         return output\n",
    "    \n",
    "#TODO : correct these values\n",
    "model = SiameseModel(input_dim1 = 8, input_dim2 = 1024, \n",
    "                 hidden_dim1 = 20, hidden_dim2 = 768, hidden_dim3 = 128, hidden_dim4 = 64,\n",
    "                 num_layers1 = 1, num_layers2 = 1, output_dim1 = 10, output_dim2 = 256)\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe858ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "print(len(list(model.parameters())))\n",
    "# for i in range(len(list(model.parameters()))):\n",
    "#     print(list(model.parameters())[i].size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbddfbcd",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3fbf0f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_arr = np.zeros(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ba21eac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "0it [00:00, ?it/s]/var/folders/tw/t5br5nkd2k3f065pr50zxvn80000gn/T/ipykernel_28802/3665585654.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/var/folders/tw/t5br5nkd2k3f065pr50zxvn80000gn/T/ipykernel_28802/3665585654.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'mask': torch.tensor(attention_masks, dtype=torch.long),\n",
      "/var/folders/tw/t5br5nkd2k3f065pr50zxvn80000gn/T/ipykernel_28802/3665585654.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
      "/var/folders/tw/t5br5nkd2k3f065pr50zxvn80000gn/T/ipykernel_28802/3665585654.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'targets': torch.tensor(y_train[index], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 1024])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:09, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 1024])\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "zeros() received an invalid combination of arguments - got (), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of SymInts size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 37\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    debug end here\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#     print(ids.shape)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#     print(masks.shape)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#     print(token_type_ids.shape)\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_numerical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[40], line 46\u001b[0m, in \u001b[0;36mSiameseModel.forward\u001b[0;34m(self, x1, ids, masks, token_type_ids)\u001b[0m\n\u001b[1;32m     40\u001b[0m h_out1 \u001b[38;5;241m=\u001b[39m h_out1\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim1)\n\u001b[1;32m     41\u001b[0m out1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(h_out1)\n\u001b[0;32m---> 46\u001b[0m e2 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# right tower with roberta on textual features  \u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#TODO\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n",
      "\u001b[0;31mTypeError\u001b[0m: zeros() received an invalid combination of arguments - got (), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of SymInts size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "roberta = RobertaModel.from_pretrained(\"roberta-large\")\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for idx, data in tqdm(enumerate(train_loader, 0)):\n",
    "        x_numerical = data['x_numerical'].to(device, dtype = torch.float)\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        masks = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        # debugging roberta encoder\n",
    "        '''\n",
    "        debug start here\n",
    "        '''\n",
    "        for k in range(2):#ids.shape[1]):\n",
    "            seq_ids = ids[:,k,:]\n",
    "            seq_masks = masks[:,k,:]\n",
    "            seq_token_type_ids = token_type_ids[:,k,:]\n",
    "\n",
    "\n",
    "            e2k = roberta(input_ids= seq_ids, attention_mask=seq_masks, token_type_ids=seq_token_type_ids)\n",
    "            print(type(e2k))\n",
    "            e2k1 = e2k[0][:, 0, :]\n",
    "            print(type(e2k1))\n",
    "            print((e2k1.shape))\n",
    "\n",
    "            print(type(e2k1))\n",
    "        \n",
    "        '''\n",
    "        debug end here\n",
    "        '''\n",
    "        \n",
    "    #     print(ids.shape)\n",
    "    #     print(masks.shape)\n",
    "    #     print(token_type_ids.shape)\n",
    "    \n",
    "        y_pred = model(x_numerical, ids, masks, token_type_ids)\n",
    "        \n",
    "        if idx > 1:\n",
    "            break\n",
    "    \n",
    "        loss = criterion(y_pred, y_train)\n",
    "        if epoch % 10 == 0 and epoch !=0:\n",
    "            print(\"Epoch \", t, \"CELoss: \", loss.item())\n",
    "        loss_arr[epoch] = loss.item()\n",
    "        wandb.log({'celoss': loss.item().avg, 'epoch': epoch, 'batch_id': batch_id})\n",
    "\n",
    "        # Zero out gradient, else they will accumulate between epochs\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimiser.step()\n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a80db11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3562edd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecedb68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f10c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_arr, label=\"Training loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51b5bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
