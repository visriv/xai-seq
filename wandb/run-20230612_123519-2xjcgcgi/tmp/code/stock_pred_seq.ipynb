{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ac4d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yfinance\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af22d194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import mpl, plt\n",
    "import math, time\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from operator import itemgetter\n",
    "from tqdm import tqdm\n",
    "from math import sqrt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38b3868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1ef75b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvisriv\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/visriv/Documents/Git/xai-seq/wandb/run-20230612_123519-2xjcgcgi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/visriv/stock_prediction/runs/2xjcgcgi' target=\"_blank\">drawn-microwave-3</a></strong> to <a href='https://wandb.ai/visriv/stock_prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/visriv/stock_prediction' target=\"_blank\">https://wandb.ai/visriv/stock_prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/visriv/stock_prediction/runs/2xjcgcgi' target=\"_blank\">https://wandb.ai/visriv/stock_prediction/runs/2xjcgcgi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/visriv/stock_prediction/runs/2xjcgcgi?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x12d3f0910>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "wandb.init(project=\"stock_prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f78002",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b1ca44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_days_to_lookforward = 1\n",
    "no_of_days_to_lookback = 5\n",
    "up_threshold = 0.015\n",
    "down_threshold = -0.015\n",
    "max_text_per_iter = 20\n",
    "batch_size = 1\n",
    "MAX_LEN = 10\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3558d6",
   "metadata": {},
   "source": [
    "### Get stocks data for last N days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e89f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64983989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "stock_symbols = [ 'XOM']\n",
    "no_of_days = 4*365\n",
    "\n",
    "EXPORT_DATA_FOLDER = './data/'\n",
    "\n",
    "# Set the start and end dates for the data \n",
    "# here matching it with dates of news text available\n",
    "start = datetime.strptime('2019/01/04', '%Y/%m/%d')\n",
    "end = datetime.strptime('2023/01/04', '%Y/%m/%d')\n",
    "\n",
    "\n",
    "# start = datetime.datetime.now() - datetime.timedelta(days=no_of_days)\n",
    "# end = datetime.datetime.now()\n",
    "\n",
    "for symbol in stock_symbols:\n",
    "    # Download the historical price and volume data using yfinance\n",
    "    data_raw = yf.download(symbol, start=start, end=end)\n",
    "\n",
    "    # Normalize features by percent of changes between today and yesterday\n",
    "    pct_change_open = data_raw['Open'].pct_change().fillna(0)\n",
    "    pct_change_high = data_raw['High'].pct_change().fillna(0)\n",
    "    pct_change_high_over_open = (data_raw['High']-data_raw['Open'])/data_raw['Open']\n",
    "    pct_change_low = data_raw['Low'].pct_change().fillna(0)\n",
    "    pct_change_low_over_open = (data_raw['Low']-data_raw['Open'])/data_raw['Open']\n",
    "    pct_change_close = data_raw['Close'].pct_change().fillna(0)\n",
    "    pct_change_close_over_open = (data_raw['Close']-data_raw['Open'])/data_raw['Open']\n",
    "    pct_change_adjclose = data_raw['Adj Close'].pct_change().fillna(0)\n",
    "    pct_change_adjclose_over_open = (data_raw['Adj Close']-data_raw['Open'])/data_raw['Open']\n",
    "    pct_change_volume = data_raw['Volume'].pct_change().fillna(0)\n",
    "\n",
    "    # Prepare labels: 2 means the close price of tomorow is higher than today's close price; 1 is down; 0 means the movement is between up_threshold and down_threshold\n",
    "    label = np.where(pct_change_close > up_threshold, 2, np.where(pct_change_close < down_threshold, 1, 0))[1:]\n",
    "    label = np.append(label, 0)\n",
    "\n",
    "    # Construct a data_norm data frame\n",
    "    data_norm = pd.DataFrame({'Open_norm':pct_change_open,\n",
    "                              'High_norm':pct_change_high,\n",
    "                              'Low_norm': pct_change_low,\n",
    "                              'Close_norm':pct_change_close,\n",
    "                              'Volume_norm':pct_change_volume,\n",
    "                              'High-Open_norm':pct_change_high_over_open,\n",
    "                              'Low-Open_norm':pct_change_low_over_open,\n",
    "                              'Close-Open_norm':pct_change_close_over_open,\n",
    "                              'Label_2up1down':label})\n",
    "\n",
    "    # Normalize by min-max normalization after the pct normalization\n",
    "    data_norm['Open_norm'] = data_norm['Open_norm'].apply(lambda x: (x - data_norm['Open_norm'].min()) / (data_norm['Open_norm'].max() - data_norm['Open_norm'].min()))\n",
    "    data_norm['High_norm'] = data_norm['High_norm'].apply(lambda x: (x - data_norm['High_norm'].min()) / (data_norm['High_norm'].max() - data_norm['High_norm'].min()))\n",
    "    data_norm['Low_norm'] = data_norm['Low_norm'].apply(lambda x: (x - data_norm['Low_norm'].min()) / (data_norm['Low_norm'].max() - data_norm['Low_norm'].min()))\n",
    "    data_norm['Close_norm'] = data_norm['Close_norm'].apply(lambda x: (x - data_norm['Close_norm'].min()) / (data_norm['Close_norm'].max() - data_norm['Close_norm'].min()))\n",
    "    data_norm['Volume_norm'] = data_norm['Volume_norm'].apply(lambda x: (x - data_norm['Volume_norm'].min()) / (data_norm['Volume_norm'].max() - data_norm['Volume_norm'].min()))\n",
    "    data_norm['High-Open_norm'] = data_norm['High-Open_norm'].apply(lambda x: (x - data_norm['High-Open_norm'].min()) / (data_norm['High-Open_norm'].max() - data_norm['High-Open_norm'].min()))\n",
    "    data_norm['Low-Open_norm'] = data_norm['Low-Open_norm'].apply(lambda x: (x - data_norm['Low-Open_norm'].min()) / (data_norm['Low-Open_norm'].max() - data_norm['Low-Open_norm'].min()))\n",
    "    data_norm['Close-Open_norm'] = data_norm['Close-Open_norm'].apply(lambda x: (x - data_norm['Close-Open_norm'].min()) / (data_norm['Close-Open_norm'].max() - data_norm['Close-Open_norm'].min()))\n",
    "\n",
    "    # Remove the first and the last row, becuase of NAN values\n",
    "    data_raw = data_raw.iloc[1:-1]\n",
    "    data_norm = data_norm.iloc[1:-1]\n",
    "\n",
    "    data_raw.to_csv(EXPORT_DATA_FOLDER+symbol+'_raw_data.csv', index=True)\n",
    "    data_norm.to_csv(EXPORT_DATA_FOLDER+symbol+'_norm_data.csv', index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deefc52d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a867cee",
   "metadata": {},
   "source": [
    "## TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f091efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "(2023-06-05)\n",
    "cuda support check\n",
    "//read textual data into correct shape\n",
    "hyperparam tuning: number of neurons: tune to right number of neurons in FC in model\n",
    "//max_text_per_iter -> code in dataloader to maintain the size \n",
    "\n",
    "(2023-06-07)\n",
    "cuda check\n",
    "roberta encoder fix\n",
    "multi label - how to create target label?\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0143d5",
   "metadata": {},
   "source": [
    "## Prep textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d848c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data_df = pd.read_csv('./data/XOM_20200401_20230401_medium.csv', sep= ',', header= 0)\n",
    "text_data_df = text_data_df[['Date', 'News']]\n",
    "\n",
    "\n",
    "text_data_df = text_data_df.groupby('Date')['News'].apply('$$$###'.join)\n",
    "\n",
    "text_data_df.index = pd.to_datetime(text_data_df.index, dayfirst=True)\n",
    "# text_data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fceff01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_norm</th>\n",
       "      <th>High_norm</th>\n",
       "      <th>Low_norm</th>\n",
       "      <th>Close_norm</th>\n",
       "      <th>Volume_norm</th>\n",
       "      <th>High-Open_norm</th>\n",
       "      <th>Low-Open_norm</th>\n",
       "      <th>Close-Open_norm</th>\n",
       "      <th>Label_2up1down</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-04-01</th>\n",
       "      <td>0.424588</td>\n",
       "      <td>0.325630</td>\n",
       "      <td>0.445426</td>\n",
       "      <td>0.444210</td>\n",
       "      <td>0.187485</td>\n",
       "      <td>0.493448</td>\n",
       "      <td>0.887338</td>\n",
       "      <td>0.568367</td>\n",
       "      <td>2</td>\n",
       "      <td>Global Polymers Market, By Type (Thermoplastic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-02</th>\n",
       "      <td>0.778879</td>\n",
       "      <td>0.795313</td>\n",
       "      <td>0.736065</td>\n",
       "      <td>0.797702</td>\n",
       "      <td>0.445139</td>\n",
       "      <td>0.821628</td>\n",
       "      <td>0.826839</td>\n",
       "      <td>0.730773</td>\n",
       "      <td>1</td>\n",
       "      <td>European Morning Briefing: U.S. Jobs Report Ey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-03</th>\n",
       "      <td>0.853804</td>\n",
       "      <td>0.435964</td>\n",
       "      <td>0.626517</td>\n",
       "      <td>0.372487</td>\n",
       "      <td>0.160943</td>\n",
       "      <td>0.174091</td>\n",
       "      <td>0.435487</td>\n",
       "      <td>0.093004</td>\n",
       "      <td>2</td>\n",
       "      <td>Nordic Morning Briefing: Services PMI Data in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-06</th>\n",
       "      <td>0.427455</td>\n",
       "      <td>0.266519</td>\n",
       "      <td>0.635065</td>\n",
       "      <td>0.619722</td>\n",
       "      <td>0.182685</td>\n",
       "      <td>0.235477</td>\n",
       "      <td>0.869836</td>\n",
       "      <td>0.546103</td>\n",
       "      <td>2</td>\n",
       "      <td>圖表 Texas Takes Two Punches -- Oil Shock and O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-07</th>\n",
       "      <td>0.876632</td>\n",
       "      <td>0.689534</td>\n",
       "      <td>0.761913</td>\n",
       "      <td>0.567103</td>\n",
       "      <td>0.283049</td>\n",
       "      <td>0.096776</td>\n",
       "      <td>0.676091</td>\n",
       "      <td>0.194020</td>\n",
       "      <td>2</td>\n",
       "      <td>Exxon Cuts Capital Spending by 30% in Response...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-23</th>\n",
       "      <td>0.542767</td>\n",
       "      <td>0.435239</td>\n",
       "      <td>0.669969</td>\n",
       "      <td>0.596883</td>\n",
       "      <td>0.188873</td>\n",
       "      <td>0.165444</td>\n",
       "      <td>0.996267</td>\n",
       "      <td>0.555196</td>\n",
       "      <td>0</td>\n",
       "      <td>Energy Transfer's Gulf Run gas pipeline gets U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-27</th>\n",
       "      <td>0.662985</td>\n",
       "      <td>0.484486</td>\n",
       "      <td>0.633508</td>\n",
       "      <td>0.546500</td>\n",
       "      <td>0.238297</td>\n",
       "      <td>0.108558</td>\n",
       "      <td>0.967843</td>\n",
       "      <td>0.504141</td>\n",
       "      <td>1</td>\n",
       "      <td>Butadiene Market, By Application (Polybutadien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-28</th>\n",
       "      <td>0.600374</td>\n",
       "      <td>0.392759</td>\n",
       "      <td>0.523834</td>\n",
       "      <td>0.424789</td>\n",
       "      <td>0.195329</td>\n",
       "      <td>0.008094</td>\n",
       "      <td>0.853954</td>\n",
       "      <td>0.357091</td>\n",
       "      <td>0</td>\n",
       "      <td>Exxon sues EU in move to block new windfall ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-29</th>\n",
       "      <td>0.508146</td>\n",
       "      <td>0.400534</td>\n",
       "      <td>0.562507</td>\n",
       "      <td>0.521098</td>\n",
       "      <td>0.222456</td>\n",
       "      <td>0.158359</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.522474</td>\n",
       "      <td>0</td>\n",
       "      <td>Global Car Care Products Market 2023-2027 Publ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-30</th>\n",
       "      <td>0.599916</td>\n",
       "      <td>0.444452</td>\n",
       "      <td>0.582740</td>\n",
       "      <td>0.531163</td>\n",
       "      <td>0.263573</td>\n",
       "      <td>0.172017</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.551144</td>\n",
       "      <td>1</td>\n",
       "      <td>Global Polypropylene Nonwoven Fabric Market 20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>694 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Open_norm  High_norm  Low_norm  Close_norm  Volume_norm  \\\n",
       "Date                                                                  \n",
       "2020-04-01   0.424588   0.325630  0.445426    0.444210     0.187485   \n",
       "2020-04-02   0.778879   0.795313  0.736065    0.797702     0.445139   \n",
       "2020-04-03   0.853804   0.435964  0.626517    0.372487     0.160943   \n",
       "2020-04-06   0.427455   0.266519  0.635065    0.619722     0.182685   \n",
       "2020-04-07   0.876632   0.689534  0.761913    0.567103     0.283049   \n",
       "...               ...        ...       ...         ...          ...   \n",
       "2022-12-23   0.542767   0.435239  0.669969    0.596883     0.188873   \n",
       "2022-12-27   0.662985   0.484486  0.633508    0.546500     0.238297   \n",
       "2022-12-28   0.600374   0.392759  0.523834    0.424789     0.195329   \n",
       "2022-12-29   0.508146   0.400534  0.562507    0.521098     0.222456   \n",
       "2022-12-30   0.599916   0.444452  0.582740    0.531163     0.263573   \n",
       "\n",
       "            High-Open_norm  Low-Open_norm  Close-Open_norm  Label_2up1down  \\\n",
       "Date                                                                         \n",
       "2020-04-01        0.493448       0.887338         0.568367               2   \n",
       "2020-04-02        0.821628       0.826839         0.730773               1   \n",
       "2020-04-03        0.174091       0.435487         0.093004               2   \n",
       "2020-04-06        0.235477       0.869836         0.546103               2   \n",
       "2020-04-07        0.096776       0.676091         0.194020               2   \n",
       "...                    ...            ...              ...             ...   \n",
       "2022-12-23        0.165444       0.996267         0.555196               0   \n",
       "2022-12-27        0.108558       0.967843         0.504141               1   \n",
       "2022-12-28        0.008094       0.853954         0.357091               0   \n",
       "2022-12-29        0.158359       1.000000         0.522474               0   \n",
       "2022-12-30        0.172017       1.000000         0.551144               1   \n",
       "\n",
       "                                                         News  \n",
       "Date                                                           \n",
       "2020-04-01  Global Polymers Market, By Type (Thermoplastic...  \n",
       "2020-04-02  European Morning Briefing: U.S. Jobs Report Ey...  \n",
       "2020-04-03  Nordic Morning Briefing: Services PMI Data in ...  \n",
       "2020-04-06   圖表 Texas Takes Two Punches -- Oil Shock and O...  \n",
       "2020-04-07  Exxon Cuts Capital Spending by 30% in Response...  \n",
       "...                                                       ...  \n",
       "2022-12-23  Energy Transfer's Gulf Run gas pipeline gets U...  \n",
       "2022-12-27  Butadiene Market, By Application (Polybutadien...  \n",
       "2022-12-28  Exxon sues EU in move to block new windfall ta...  \n",
       "2022-12-29  Global Car Care Products Market 2023-2027 Publ...  \n",
       "2022-12-30  Global Polypropylene Nonwoven Fabric Market 20...  \n",
       "\n",
       "[694 rows x 10 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_df = data_norm.join(text_data_df, how = 'inner')\n",
    "all_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56e91800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "all_train = all_train_df.values\n",
    "\n",
    "window_size = no_of_days_to_lookback\n",
    "\n",
    "X_numerical_train = []\n",
    "y_train = []\n",
    "X_text_train = []\n",
    "X_text_train_curr = []\n",
    "\n",
    "\n",
    "for i in range(window_size, len(all_train) - no_of_days_to_lookforward + 1):\n",
    "    X_numerical_train.append(all_train[i-window_size: i, :-2])\n",
    "    \n",
    "    # split and append sequence of text\n",
    "    curr_seq = all_train[i-window_size: i, -1]\n",
    "    for j in range(window_size):\n",
    "        split_curr_seq = curr_seq[window_size - 1 -j].split('$$$###')\n",
    "        X_text_train_curr = X_text_train_curr + split_curr_seq\n",
    "    \n",
    "    if len(X_text_train_curr) > max_text_per_iter:\n",
    "        X_text_train_curr = X_text_train_curr[:max_text_per_iter]\n",
    "    \n",
    "    X_text_train.append(X_text_train_curr)\n",
    "        \n",
    "    # target labels\n",
    "    y_train.append(all_train[i:i+no_of_days_to_lookforward, -2])\n",
    "\n",
    "X_numerical_train, y_train = np.array(X_numerical_train).astype(np.float16), np.array(y_train).astype(np.int32)\n",
    "print(type(X_numerical_train))\n",
    "print(type(y_train))\n",
    "\n",
    "X_numerical_train = torch.from_numpy(X_numerical_train).type(torch.Tensor)\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69c8dbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "689\n",
      "689\n",
      "689\n",
      "torch.Size([689, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "print(len(X_numerical_train))\n",
    "print(len(X_text_train))\n",
    "print(len(y_train))\n",
    "print(X_numerical_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5bc27d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "689\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(X_text_train))\n",
    "print(len(X_text_train[2]))\n",
    "# print(X_text_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068c0403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca774d67",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2dfa40d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large', truncation=True, do_lower_case=True)\n",
    "\n",
    "class SiameseDataloader(Dataset):\n",
    "    \n",
    "    def __init__(self, X_numerical_train, y_train, X_text_train, MAX_LEN, tokenizer):\n",
    "        self.X_numerical_train = X_numerical_train\n",
    "        self.X_text_train = X_text_train\n",
    "        self.MAX_LEN = MAX_LEN\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        \n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        token_type_ids = []\n",
    "        input_seq = []\n",
    "\n",
    "        for sent in X_text_train[index]:\n",
    "            encoded_sent = self.tokenizer.encode_plus(\n",
    "                text=sent,\n",
    "                add_special_tokens=True,        # Add `[CLS]` and `[SEP]` special tokens\n",
    "                max_length=self.MAX_LEN,             # Choose max length to truncate/pad\n",
    "                pad_to_max_length=True,         # Pad sentence to max length \n",
    "                #return_attention_mask=True      # Return attention mask\n",
    "                return_token_type_ids=True\n",
    "                )\n",
    "            input_ids.append(encoded_sent.get('input_ids'))\n",
    "            attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "            token_type_ids.append(encoded_sent.get('token_type_ids'))\n",
    "\n",
    "        # Convert lists to tensors\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        attention_masks = torch.tensor(attention_masks)\n",
    "        token_type_ids = torch.tensor(token_type_ids)\n",
    "\n",
    "\n",
    "        return {\n",
    "            'x_numerical': X_numerical_train[index],\n",
    "            'ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(attention_masks, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(y_train[index], dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_numerical_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4937603",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = SiameseDataloader(X_numerical_train, y_train, X_text_train, MAX_LEN, tokenizer)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa473658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b1a421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6644f72c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55a39e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802dd89a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa8e30d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94acc026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c6349f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd799c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db91967b",
   "metadata": {},
   "source": [
    "## Build model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d59795b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class SiameseModel(nn.Module):\n",
    "    def __init__(self, input_dim1, input_dim2, \n",
    "                 hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4,\n",
    "                 num_layers1, num_layers2, output_dim1, output_dim2):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.input_dim1 = input_dim1\n",
    "        self.input_dim2 = input_dim2\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.hidden_dim3 = hidden_dim3\n",
    "        self.hidden_dim4 = hidden_dim4\n",
    "        self.num_layers1 = num_layers1\n",
    "        self.num_layers2 = num_layers2\n",
    "        self.output_dim1 = output_dim1\n",
    "        self.output_dim2 = output_dim2\n",
    "        \n",
    "        \n",
    "\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta-large\")\n",
    "        \n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_dim1, hidden_dim1, num_layers1, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_dim2, hidden_dim2, num_layers2, batch_first=True)\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim1, output_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim2, output_dim2)\n",
    "        self.fc3 = nn.Linear(output_dim1+output_dim2, hidden_dim3)\n",
    "        self.fc4 = nn.Linear(hidden_dim3, hidden_dim4)\n",
    "        self.fc5 = nn.Linear(hidden_dim4, 3)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x1, ids, masks, token_type_ids):\n",
    "        #left tower with numerical features\n",
    "        \n",
    "        h_10 = Variable(torch.zeros(self.num_layers1, x1.size(0), self.hidden_dim1))\n",
    "        c_10 = Variable(torch.zeros(self.num_layers1, x1.size(0), self.hidden_dim1))\n",
    "        ula1, (h_out1, _) = self.lstm1(x1, (h_10, c_10))\n",
    "        h_out1 = h_out1.view(-1, self.hidden_dim1)\n",
    "        out1 = self.fc1(h_out1)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # right tower with roberta on textual features  \n",
    "        #TODO\n",
    "        e2 = torch.zeros(1,max_text_per_iter,1024)\n",
    "        \n",
    "        for k in range(ids.shape[1]):\n",
    "            seq_ids = ids[:,k,:]\n",
    "            seq_masks = masks[:,k,:]\n",
    "            seq_token_type_ids = token_type_ids[:,k,:]\n",
    "\n",
    "\n",
    "            e2k = roberta(input_ids= seq_ids, attention_mask=seq_masks, token_type_ids=seq_token_type_ids)\n",
    "\n",
    "            e2k1 = e2k[0][:, 0, :]  #first 0 is for 'CLS' token\n",
    "            e2[:,k,:] = e2k1\n",
    "    \n",
    "    \n",
    "        print(e2.shape)        \n",
    "        h_20 = Variable(torch.zeros(self.num_layers2, e2.size(0), self.hidden_dim2))\n",
    "        c_20 = Variable(torch.zeros(self.num_layers2, e2.size(0), self.hidden_dim2))\n",
    "        ula2, (h_out2, _) = self.lstm2(e2, (h_20, c_20))\n",
    "        h_out2 = h_out2.view(-1, self.hidden_dim2)\n",
    "        out2 = self.fc2(h_out2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # siamese merging layers\n",
    "        \n",
    "        output = torch.cat((out1, out2),1)\n",
    "        output = F.relu(self.fc3(output))\n",
    "        output = F.relu(self.fc4(output))\n",
    "        output = self.fc5(output)\n",
    "        return output\n",
    "    \n",
    "#TODO : correct these values\n",
    "model = SiameseModel(input_dim1 = 8, input_dim2 = 1024, \n",
    "                 hidden_dim1 = 20, hidden_dim2 = 768, hidden_dim3 = 128, hidden_dim4 = 64,\n",
    "                 num_layers1 = 1, num_layers2 = 1, output_dim1 = 10, output_dim2 = 256)\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8288a4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SiameseModel(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (12): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (13): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (14): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (15): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (16): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (17): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (18): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (19): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (20): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (21): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (22): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (23): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (lstm1): LSTM(8, 20, batch_first=True)\n",
      "  (lstm2): LSTM(1024, 768, batch_first=True)\n",
      "  (fc1): Linear(in_features=20, out_features=10, bias=True)\n",
      "  (fc2): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=266, out_features=128, bias=True)\n",
      "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n",
      "409\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(len(list(model.parameters())))\n",
    "# for i in range(len(list(model.parameters()))):\n",
    "#     print(list(model.parameters())[i].size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36cadbe",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40203923",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_arr = np.zeros(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "42a6ae54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "0it [00:00, ?it/s]/var/folders/tw/t5br5nkd2k3f065pr50zxvn80000gn/T/ipykernel_33319/3665585654.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/var/folders/tw/t5br5nkd2k3f065pr50zxvn80000gn/T/ipykernel_33319/3665585654.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'mask': torch.tensor(attention_masks, dtype=torch.long),\n",
      "/var/folders/tw/t5br5nkd2k3f065pr50zxvn80000gn/T/ipykernel_33319/3665585654.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
      "/var/folders/tw/t5br5nkd2k3f065pr50zxvn80000gn/T/ipykernel_33319/3665585654.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'targets': torch.tensor(y_train[index], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0462, -0.1470]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:14, 14.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0462, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:30, 15.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0462, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [00:45, 15.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0461, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [01:01, 15.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0462, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [01:16, 15.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0462, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [01:30, 15.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0461, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [01:45, 14.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0462, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [01:59, 14.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0461, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [02:13, 14.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0461, -0.1470]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10it [02:28, 14.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0461, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "11it [02:43, 14.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0461, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "12it [02:58, 14.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0461, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [03:14, 15.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0461, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "14it [03:31, 15.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0462, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "15it [03:47, 15.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0461, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "16it [04:02, 15.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1368,  0.0463, -0.1472]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "17it [04:17, 15.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0462, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "18it [04:32, 15.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0462, -0.1470]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "19it [04:47, 15.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0461, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "20it [05:02, 15.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0462, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "21it [05:17, 15.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0461, -0.1470]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "22it [05:31, 14.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0461, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "23it [05:44, 14.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0462, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "24it [05:59, 14.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0461, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "25it [06:14, 14.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1369,  0.0463, -0.1470]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "26it [06:28, 14.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0461, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "27it [06:42, 14.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0462, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "28it [06:56, 14.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0461, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "29it [07:10, 14.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0462, -0.1470]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "30it [07:24, 14.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0462, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "31it [07:38, 14.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0462, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "32it [07:52, 14.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0461, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "33it [08:07, 14.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1369,  0.0464, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "34it [08:21, 14.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0462, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "35it [08:35, 14.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0462, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "36it [08:49, 14.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0462, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "37it [09:04, 14.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0462, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "38it [09:19, 14.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0462, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "39it [09:36, 15.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0461, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "40it [09:55, 16.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0461, -0.1472]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "41it [10:11, 16.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0463, -0.1470]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "42it [10:28, 16.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0461, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "43it [10:42, 15.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0463, -0.1470]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "44it [10:58, 15.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0462, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "45it [11:13, 15.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0461, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "46it [11:27, 14.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0461, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "47it [11:40, 14.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0461, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "48it [11:54, 14.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0461, -0.1471]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "49it [12:07, 14.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 1024])\n",
      "y_pred: tensor([[ 0.1370,  0.0462, -0.1470]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49it [12:21, 15.12s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_log_string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 83\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCELoss: \u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m ((idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 83\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[43mtrain_log_string\u001b[49m \u001b[38;5;241m%\u001b[39m (datetime\u001b[38;5;241m.\u001b[39mnow(), epoch, idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train), np\u001b[38;5;241m.\u001b[39mmean(train_loss)))\n\u001b[1;32m     84\u001b[0m             train_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m#         loss_arr[epoch] = loss.item()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_log_string' is not defined"
     ]
    }
   ],
   "source": [
    "roberta = RobertaModel.from_pretrained(\"roberta-large\").to(device)\n",
    "train_loss_record = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    train_loss_sum = []\n",
    "    \n",
    "    for idx, data in tqdm(enumerate(train_loader, 0)):\n",
    "        x_numerical = data['x_numerical'].to(device, dtype = torch.float)\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        masks = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        # debugging roberta encoder and second lstm\n",
    "        '''\n",
    "        debug starts here\n",
    "        '''\n",
    "#         if idx > 1:\n",
    "#             break\n",
    "#         e2 = torch.zeros(1,max_text_per_iter,1024)\n",
    "#         print('ids shape:', ids.shape)\n",
    "#         for k in range(ids.shape[1]):  #number of sentences in sequence\n",
    "#             print('k:', k)\n",
    "#             seq_ids = ids[:,k,:].to(device)\n",
    "#             seq_masks = masks[:,k,:].to(device)\n",
    "#             seq_token_type_ids = token_type_ids[:,k,:].to(device)\n",
    "\n",
    "\n",
    "#             e2k = roberta(input_ids= seq_ids, attention_mask=seq_masks, token_type_ids=seq_token_type_ids)\n",
    "\n",
    "#             e2k1 = e2k[0][:, 0, :]  #first 0 is for 'CLS' token\n",
    "#             e2[:,k,:] = e2k1\n",
    "    \n",
    "    \n",
    "#         print(e2.shape) \n",
    "#         lstm2 = nn.LSTM(1024, 768, 1, batch_first=True)\n",
    "#         fc2 = nn.Linear(768, 256)\n",
    "\n",
    "#         h_20 = Variable(torch.zeros(1, e2.size(0), 768))\n",
    "#         c_20 = Variable(torch.zeros(1, e2.size(0), 768))\n",
    "#         ula2, (h_out2, _) = lstm2(e2, (h_20, c_20))\n",
    "#         h_out2 = h_out2.view(-1, 768)\n",
    "#         out2 = fc2(h_out2)\n",
    "        \n",
    "\n",
    "#     #     print(ids.shape)\n",
    "#     #     print(masks.shape)\n",
    "#     #     print(token_type_ids.shape)\n",
    "    \n",
    "#         print(out2)\n",
    "\n",
    "\n",
    "        \n",
    "        '''\n",
    "        debug ends here\n",
    "        '''\n",
    "        \n",
    "\n",
    "    \n",
    "        y_pred = model(x_numerical, ids, masks, token_type_ids)\n",
    "        print('y_pred:', y_pred)\n",
    "        loss = criterion(y_pred, targets.reshape(-1))\n",
    "        \n",
    "         # Zero out gradient, else they will accumulate between epochs\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimiser.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        train_loss.append(loss.data.cpu())\n",
    "        train_loss_sum.append(loss.data.cpu())\n",
    "        \n",
    "        if epoch % 10 == 0 and epoch !=0:\n",
    "            print(\"Epoch \", epoch, \"CELoss: \", loss.item())\n",
    "            \n",
    "            \n",
    "#         if ((idx + 1) % 50) == 0:\n",
    "#             print(train_log_string % (datetime.now(), epoch, idx + 1, len(train), np.mean(train_loss)))\n",
    "#             train_loss = []\n",
    "                \n",
    "                \n",
    "#         loss_arr[epoch] = loss.item()\n",
    "        wandb.log({'avg loss in this batch': loss.item(), 'epoch': epoch, 'batch_id': idx})\n",
    "\n",
    "       \n",
    "    \n",
    "    # Record at every epoch\n",
    "    print('Train Loss at epoch {}: {}\\n'.format(epoch, np.mean(train_loss_sum)))\n",
    "    train_loss_record.append(np.mean(train_loss_sum))\n",
    "    wandb.log({'avg loss in this epoch': np.mean(train_loss_sum), 'epoch': epoch})\n",
    "\n",
    "            \n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d0b46ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.07136285, 1.07140326, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5534c296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1370,  0.0461, -0.1471]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b91d66dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad568e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e12c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_arr, label=\"Training loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5494666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
